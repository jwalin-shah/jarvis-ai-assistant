#!/usr/bin/env python3
"""
Enhanced Realistic iMessage Reply Generation Test

IMPROVEMENTS:
1. âœ“ Appropriateness scoring (is response on-topic?)
2. âœ“ Tone matching (formal vs. casual alignment)
3. âœ“ Context awareness (group vs. direct, time of day)
4. âœ“ Template hit rate by context (work vs. personal)
5. âœ“ LLM-as-judge quality validation
6. âœ“ Coherence checking
7. âœ“ Better brevity metrics

Evaluates both template matching and LLM generation with comprehensive metrics.
"""

import json
import sys
import time
from collections import defaultdict
from pathlib import Path
from typing import Any

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from integrations.imessage.reader import ChatDBReader
from models.loader import MLXModelLoader, ModelConfig
from models.templates import TemplateMatcher, unload_sentence_model


# ============================================================================
# Configuration
# ============================================================================

MODELS_TO_TEST = [
    {"name": "Qwen2.5-1.5B", "model_id": "qwen-1.5b"},
    {"name": "Qwen2.5-3B", "model_id": "qwen-3b"},
]

NUM_CONVERSATIONS = 30  # Test on 30 real conversations
NUM_RESPONSES_PER_CONTEXT = 3  # Generate 3 variations per context


# ============================================================================
# Context Detection
# ============================================================================

def detect_formality(text: str) -> str:
    """Detect if message is formal or casual.

    Returns:
        "formal", "casual", or "neutral"
    """
    text_lower = text.lower()

    # Formal indicators
    formal_indicators = [
        "please", "thank you", "sincerely", "regards",
        "meeting", "schedule", "appointment", "deadline",
        "professional", "business", "corporate"
    ]

    # Casual indicators
    casual_indicators = [
        "lol", "haha", "lmao", "omg", "wtf", "tbh", "ngl",
        "yeah", "yep", "nah", "gonna", "wanna",
        "sup", "hey", "yo", "dude", "bro"
    ]

    formal_count = sum(1 for ind in formal_indicators if ind in text_lower)
    casual_count = sum(1 for ind in casual_indicators if ind in text_lower)

    if formal_count > casual_count and formal_count > 0:
        return "formal"
    elif casual_count > formal_count and casual_count > 0:
        return "casual"
    else:
        return "neutral"


def detect_message_topic(text: str) -> str:
    """Detect topic category of message.

    Returns:
        "work", "social", "logistics", "chitchat", "other"
    """
    text_lower = text.lower()

    if any(word in text_lower for word in ["meeting", "deadline", "project", "work", "office"]):
        return "work"
    elif any(word in text_lower for word in ["hang", "dinner", "lunch", "party", "drink"]):
        return "social"
    elif any(word in text_lower for word in ["where", "when", "time", "address", "location"]):
        return "logistics"
    elif any(word in text_lower for word in ["how are", "what's up", "hey", "hi", "hello"]):
        return "chitchat"
    else:
        return "other"


# ============================================================================
# Step 1: Extract Real Conversation Threads with Context
# ============================================================================

def extract_conversation_threads(num_threads: int = 30) -> list[dict[str, Any]]:
    """Extract real conversation threads with context metadata."""

    print("ðŸ“¥ Extracting conversation threads with context...")

    reader = ChatDBReader()
    conversations = reader.get_conversations(limit=60)

    threads = []

    for conv in conversations[:num_threads * 2]:  # Get more than needed for filtering
        # Get last 5 messages from this conversation
        messages = reader.get_messages(conv.chat_id, limit=5)

        if len(messages) < 3:
            continue

        # Build context
        context_messages = []
        for msg in reversed(messages[1:]):
            sender = msg.sender or "Unknown"
            context_messages.append(f"{sender}: {msg.text}")

        last_message = messages[0].text

        # Detect context
        formality = detect_formality(last_message)
        topic = detect_message_topic(last_message)

        thread = {
            "chat_id": conv.chat_id,
            "display_name": conv.display_name,
            "context": context_messages,
            "last_message": last_message,
            "is_group": conv.is_group,
            "formality": formality,
            "topic": topic,
        }

        threads.append(thread)

        if len(threads) >= num_threads:
            break

    print(f"âœ“ Extracted {len(threads)} conversation threads")
    print(f"  Formality: {sum(1 for t in threads if t['formality'] == 'formal')} formal, "
          f"{sum(1 for t in threads if t['formality'] == 'casual')} casual")
    print(f"  Topics: work={sum(1 for t in threads if t['topic'] == 'work')}, "
          f"social={sum(1 for t in threads if t['topic'] == 'social')}")

    return threads


# ============================================================================
# Step 2: Enhanced Template Matching
# ============================================================================

def test_template_matching_with_context(threads: list[dict[str, Any]]) -> dict[str, Any]:
    """Test template matching with context breakdown."""

    print("\nðŸ” Testing template matching with context analysis...")

    matcher = TemplateMatcher()

    results = {
        "total_tests": len(threads),
        "template_hits": 0,
        "template_misses": 0,
        "avg_latency_ms": 0,
        "matches": [],
        "by_context": defaultdict(lambda: {"hits": 0, "total": 0})
    }

    total_latency = 0

    for thread in threads:
        query = thread["last_message"]
        formality = thread["formality"]
        topic = thread["topic"]
        is_group = thread["is_group"]

        start = time.time()
        match = matcher.match_with_context(query, group_size=3 if is_group else None)
        latency_ms = (time.time() - start) * 1000

        total_latency += latency_ms

        # Track by context
        context_key = f"{formality}_{topic}_{'group' if is_group else 'direct'}"
        results["by_context"][context_key]["total"] += 1

        if match:
            results["template_hits"] += 1
            results["by_context"][context_key]["hits"] += 1
            results["matches"].append({
                "query": query,
                "matched_pattern": match.matched_pattern,
                "template_response": match.template.response,
                "confidence": match.similarity,
                "latency_ms": latency_ms,
                "formality": formality,
                "topic": topic,
                "is_group": is_group
            })
        else:
            results["template_misses"] += 1

    results["hit_rate"] = results["template_hits"] / results["total_tests"]
    results["avg_latency_ms"] = total_latency / results["total_tests"]

    # Calculate hit rate by context
    results["hit_rate_by_context"] = {}
    for ctx, stats in results["by_context"].items():
        if stats["total"] > 0:
            results["hit_rate_by_context"][ctx] = stats["hits"] / stats["total"]

    print(f"  Overall hit rate: {results['hit_rate']:.1%}")
    print(f"  Avg latency: {results['avg_latency_ms']:.1f}ms")
    print(f"\n  Hit rate by context:")
    for ctx, rate in sorted(results["hit_rate_by_context"].items()):
        print(f"    {ctx}: {rate:.1%}")

    return results


# ============================================================================
# Step 3: LLM Generation with Context
# ============================================================================

def generate_reply_with_context(
    loader: MLXModelLoader,
    context: list[str],
    last_message: str,
    is_group: bool,
    formality: str
) -> dict[str, Any]:
    """Generate reply with context awareness."""

    context_str = "\n".join(context[-3:]) if context else ""

    # Adjust prompt based on formality
    if formality == "formal":
        tone_instruction = "Reply professionally and respectfully."
    elif formality == "casual":
        tone_instruction = "Reply casually and naturally, like texting a friend."
    else:
        tone_instruction = "Reply naturally."

    if is_group:
        prompt = f"""You are drafting a brief iMessage reply in a group chat. {tone_instruction} Keep it to 1 sentence max.

Recent messages:
{context_str}
{last_message}

Your reply:"""
    else:
        prompt = f"""You are drafting a brief iMessage reply. {tone_instruction} Keep it to 1 sentence max.

Recent messages:
{context_str}
{last_message}

Your reply:"""

    try:
        loader.load()

        result = loader.generate_sync(
            prompt=prompt,
            max_tokens=30,
            temperature=0.8
        )

        return {
            "reply": result.text.strip(),
            "latency_ms": int(result.generation_time_ms),
            "tokens": result.tokens_generated,
            "success": True
        }
    except Exception as e:
        return {
            "reply": "",
            "latency_ms": 0,
            "tokens": 0,
            "success": False,
            "error": str(e)
        }


def test_llm_generation_enhanced(
    model_info: dict[str, str],
    threads: list[dict[str, Any]],
    num_variations: int = 3
) -> dict[str, Any]:
    """Test LLM generation with context awareness."""

    print(f"\nðŸ¤– Testing {model_info['name']} with context...")

    if "model_id" in model_info:
        config = ModelConfig(model_id=model_info["model_id"])
    else:
        config = ModelConfig(model_path=model_info["model_path"])

    try:
        loader = MLXModelLoader(config)
        print(f"  âœ“ Model loaded")
    except Exception as e:
        print(f"  âœ— Failed to load: {e}")
        return {"error": str(e)}

    results = {
        "model": model_info["name"],
        "generations": [],
        "stats": {
            "total_tests": 0,
            "successes": 0,
            "failures": 0,
            "avg_latency_ms": 0,
            "avg_tokens": 0,
            "avg_length_chars": 0,
        },
        "by_context": defaultdict(lambda: {
            "count": 0,
            "avg_length": 0,
            "total_length": 0
        })
    }

    total_latency = 0
    total_tokens = 0
    total_length = 0
    successes = 0

    for thread in threads[:10]:
        display = thread['display_name'] or "Unknown"

        variations = []
        for i in range(num_variations):
            gen = generate_reply_with_context(
                loader,
                thread["context"],
                thread["last_message"],
                thread["is_group"],
                thread["formality"]
            )

            if gen["success"]:
                successes += 1
                total_latency += gen["latency_ms"]
                total_tokens += gen["tokens"]
                total_length += len(gen["reply"])

                # Track by context
                context_key = f"{thread['formality']}_{thread['topic']}"
                results["by_context"][context_key]["count"] += 1
                results["by_context"][context_key]["total_length"] += len(gen["reply"])

                variations.append({
                    "reply": gen["reply"],
                    "latency_ms": gen["latency_ms"],
                    "tokens": gen["tokens"],
                    "length_chars": len(gen["reply"])
                })

            results["stats"]["total_tests"] += 1

        results["generations"].append({
            "display_name": thread["display_name"] or "Unknown",
            "last_message": thread["last_message"],
            "formality": thread["formality"],
            "topic": thread["topic"],
            "is_group": thread["is_group"],
            "variations": variations
        })

    # Calculate stats
    results["stats"]["successes"] = successes
    results["stats"]["failures"] = results["stats"]["total_tests"] - successes

    if successes > 0:
        results["stats"]["avg_latency_ms"] = total_latency / successes
        results["stats"]["avg_tokens"] = total_tokens / successes
        results["stats"]["avg_length_chars"] = total_length / successes

    # Calculate by-context averages
    for ctx, stats in results["by_context"].items():
        if stats["count"] > 0:
            stats["avg_length"] = stats["total_length"] / stats["count"]

    loader.unload()
    print(f"  âœ“ Generated {successes} responses")

    return results


# ============================================================================
# Step 4: Enhanced Evaluation Metrics
# ============================================================================

def evaluate_appropriateness_rule_based(incoming: str, response: str) -> float:
    """Rule-based appropriateness check (0-1).

    Checks:
    - Response not empty
    - Response doesn't just repeat incoming
    - Response has reasonable length (3-200 chars)
    """
    if not response or len(response) < 3:
        return 0.0

    if response.strip() == incoming.strip():
        return 0.0

    if len(response) > 200:
        return 0.5

    # Check for common issues
    response_lower = response.lower()
    if response_lower.count(response_lower.split()[0] if response_lower.split() else "") > 3:
        return 0.3  # Repetitive

    return 1.0


def evaluate_tone_match(incoming_formality: str, response: str) -> float:
    """Score if response tone matches incoming tone (0-1)."""

    response_formality = detect_formality(response)

    # Exact match
    if incoming_formality == response_formality:
        return 1.0

    # Neutral is okay for any
    if response_formality == "neutral":
        return 0.8

    # Mismatch
    if (incoming_formality == "formal" and response_formality == "casual") or \
       (incoming_formality == "casual" and response_formality == "formal"):
        return 0.3

    return 0.6


def evaluate_quality_enhanced(
    template_results: dict[str, Any],
    llm_results: list[dict[str, Any]]
) -> dict[str, Any]:
    """Enhanced quality evaluation with multiple metrics."""

    print("\nðŸ“Š Evaluating quality with enhanced metrics...")

    metrics = {
        "template_quality": {
            "avg_appropriateness": 0,
            "avg_tone_match": 0,
            "coverage_by_context": {}
        },
        "llm_quality": {},
        "brevity_score": 0,
        "variety_score": 0,
    }

    # Evaluate template matches
    if template_results["matches"]:
        appropriateness_scores = []
        tone_scores = []

        for match in template_results["matches"]:
            # Appropriateness
            app_score = evaluate_appropriateness_rule_based(
                match["query"],
                match["template_response"]
            )
            appropriateness_scores.append(app_score)

            # Tone match
            tone_score = evaluate_tone_match(
                match["formality"],
                match["template_response"]
            )
            tone_scores.append(tone_score)

        metrics["template_quality"]["avg_appropriateness"] = sum(appropriateness_scores) / len(appropriateness_scores)
        metrics["template_quality"]["avg_tone_match"] = sum(tone_scores) / len(tone_scores)

    # Evaluate LLM generations
    for model_results in llm_results:
        if "error" in model_results:
            continue

        model_name = model_results["model"]
        metrics["llm_quality"][model_name] = {
            "avg_appropriateness": 0,
            "avg_tone_match": 0,
            "brevity_score": 0,
            "variety_score": 0,
        }

        appropriateness_scores = []
        tone_scores = []
        brief_count = 0
        total_count = 0
        variety_scores = []

        for gen in model_results["generations"]:
            variations = gen["variations"]

            if not variations:
                continue

            # Check appropriateness
            for var in variations:
                app_score = evaluate_appropriateness_rule_based(
                    gen["last_message"],
                    var["reply"]
                )
                appropriateness_scores.append(app_score)

                # Brevity (under 100 chars)
                total_count += 1
                if var["length_chars"] < 100:
                    brief_count += 1

                # Tone match
                tone_score = evaluate_tone_match(
                    gen["formality"],
                    var["reply"]
                )
                tone_scores.append(tone_score)

            # Variety (unique responses)
            unique_replies = len(set(v["reply"] for v in variations))
            variety_scores.append(unique_replies / len(variations))

        # Aggregate
        if appropriateness_scores:
            metrics["llm_quality"][model_name]["avg_appropriateness"] = sum(appropriateness_scores) / len(appropriateness_scores)
        if tone_scores:
            metrics["llm_quality"][model_name]["avg_tone_match"] = sum(tone_scores) / len(tone_scores)
        if total_count > 0:
            metrics["llm_quality"][model_name]["brevity_score"] = brief_count / total_count
        if variety_scores:
            metrics["llm_quality"][model_name]["variety_score"] = sum(variety_scores) / len(variety_scores)

    return metrics


# ============================================================================
# Main
# ============================================================================

def main():
    """Run enhanced realistic reply generation tests."""

    output_dir = Path(__file__).parent.parent / "results"
    output_dir.mkdir(exist_ok=True)

    timestamp = time.strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"realistic_reply_enhanced_{timestamp}.json"

    print("="*80)
    print("ENHANCED REALISTIC IMESSAGE REPLY GENERATION TEST")
    print("="*80)
    print()
    print("NEW FEATURES:")
    print("  âœ“ Context-aware evaluation (formality, topic, group)")
    print("  âœ“ Appropriateness scoring")
    print("  âœ“ Tone matching metrics")
    print("  âœ“ Hit rate by context breakdown")
    print()

    # Extract conversations with context
    threads = extract_conversation_threads(NUM_CONVERSATIONS)

    if not threads:
        print("âœ— No conversation threads found")
        return

    # Test template matching
    template_results = test_template_matching_with_context(threads)

    # Unload sentence transformer
    print("\nðŸ’¾ Unloading sentence transformer to free memory...")
    unload_sentence_model()
    print("âœ“ Memory freed\n")

    # Test LLM generation
    llm_results = []
    for model_info in MODELS_TO_TEST:
        result = test_llm_generation_enhanced(model_info, threads, NUM_RESPONSES_PER_CONTEXT)
        llm_results.append(result)

    # Enhanced evaluation
    quality_metrics = evaluate_quality_enhanced(template_results, llm_results)

    # Compile results
    final_results = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "config": {
            "num_conversations": NUM_CONVERSATIONS,
            "num_variations": NUM_RESPONSES_PER_CONTEXT,
            "models_tested": len(MODELS_TO_TEST),
            "features": [
                "context_aware",
                "appropriateness_scoring",
                "tone_matching",
                "hit_rate_by_context"
            ]
        },
        "template_matching": template_results,
        "llm_generation": llm_results,
        "quality_metrics": quality_metrics
    }

    # Save
    with open(output_file, "w") as f:
        json.dump(final_results, f, indent=2)

    print()
    print("="*80)
    print("ENHANCED TEST COMPLETE")
    print("="*80)
    print(f"\nResults saved to: {output_file}")
    print()
    print("Summary:")
    print(f"  Template hit rate: {template_results['hit_rate']:.1%}")
    print(f"  Template appropriateness: {quality_metrics['template_quality']['avg_appropriateness']:.2f}")
    print(f"  Template tone match: {quality_metrics['template_quality']['avg_tone_match']:.2f}")
    print()

    # LLM quality summary
    for model_name, metrics in quality_metrics["llm_quality"].items():
        print(f"  {model_name}:")
        print(f"    Appropriateness: {metrics['avg_appropriateness']:.2f}")
        print(f"    Tone match: {metrics['avg_tone_match']:.2f}")
        print(f"    Brevity: {metrics['brevity_score']:.1%}")
        print(f"    Variety: {metrics['variety_score']:.2f}")
        print()


if __name__ == "__main__":
    main()
