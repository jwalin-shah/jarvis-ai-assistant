#!/usr/bin/env python3
"""End-to-end training script with validation for JARVIS fine-tuning.

This script performs the full training loop with validation at each step:
1. Validates data exists and is properly formatted
2. Tokenizes samples to verify format matches tokenizer expectations
3. Trains with early stopping (best checkpoint tracking)
4. Evaluates the trained adapter

Usage:
    # Train with config file
    uv run python scripts/training/train_end_to_end.py --config ft_configs/experiment_1.2b.yaml
    
    # Train with explicit parameters
    uv run python scripts/training/train_end_to_end.py \
        --model mlx-community/LFM2-1.2B-4bit \
        --data data/personal/raw_style_variable \
        --adapter-path adapters/test_run \
        --iters 100

    # Validate data only (don't train)
    uv run python scripts/training/train_end_to_end.py --config ft_configs/experiment_1.2b.yaml --validate-only
"""

from __future__ import annotations

import argparse
import json
import shutil
import signal
import sys
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

import mlx.core as mx
import yaml

PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


class TrainingError(Exception):
    """Training validation or execution error."""
    pass


@dataclass
class TrainingState:
    """Track training progress for early stopping."""
    iteration: int = 0
    best_val_loss: float = float('inf')
    best_iteration: int = 0
    val_loss_history: list[tuple[int, float]] = field(default_factory=list)
    patience: int = 3  # Stop after N evals without improvement
    
    def should_stop(self) -> bool:
        """Check if training should stop (no improvement for patience evals)."""
        if len(self.val_loss_history) < self.patience + 1:
            return False
        
        # Check last N evals
        recent_losses = [loss for _, loss in self.val_loss_history[-self.patience:]]
        best_recent = min(recent_losses)
        
        # Stop if best recent is not better than best overall
        return best_recent >= self.best_val_loss
    
    def update(self, iteration: int, val_loss: float) -> bool:
        """Update state with new validation loss. Returns True if new best."""
        self.iteration = iteration
        self.val_loss_history.append((iteration, val_loss))
        
        if val_loss < self.best_val_loss:
            self.best_val_loss = val_loss
            self.best_iteration = iteration
            return True
        return False


def validate_data_format(data_path: Path, tokenizer, max_checks: int = 3) -> dict:
    """Validate training data format and tokenization.
    
    Args:
        data_path: Path to data directory containing train.jsonl, test.jsonl
        tokenizer: The tokenizer to use for validation
        max_checks: Number of samples to validate
        
    Returns:
        Dict with validation results
        
    Raises:
        TrainingError: If data is invalid
    """
    from mlx_lm.tuner.datasets import ChatDataset
    
    results = {
        "train_path": None,
        "test_path": None,
        "train_samples": 0,
        "test_samples": 0,
        "train_tokens_avg": 0,
        "test_tokens_avg": 0,
        "errors": [],
    }
    
    # Check files exist
    train_path = data_path / "train.jsonl"
    test_path = data_path / "test.jsonl"
    
    if not train_path.exists():
        raise TrainingError(f"Training data not found: {train_path}")
    if not test_path.exists():
        raise TrainingError(f"Test data not found: {test_path}")
    
    results["train_path"] = str(train_path)
    results["test_path"] = str(test_path)
    
    # Load and validate samples
    for split, path in [("train", train_path), ("test", test_path)]:
        try:
            with open(path) as f:
                lines = f.readlines()
            
            count = len(lines)
            results[f"{split}_samples"] = count
            
            if count == 0:
                raise TrainingError(f"{split}.jsonl is empty!")
            
            # Validate first N samples
            total_tokens = 0
            for i, line in enumerate(lines[:max_checks]):
                try:
                    data = json.loads(line)
                    
                    # Check structure
                    if "messages" not in data:
                        raise TrainingError(f"Sample {i} missing 'messages' key")
                    
                    messages = data["messages"]
                    if not isinstance(messages, list) or len(messages) < 2:
                        raise TrainingError(f"Sample {i} has invalid messages format")
                    
                    # Check required roles
                    roles = [m.get("role") for m in messages]
                    if "assistant" not in roles:
                        raise TrainingError(f"Sample {i} missing assistant message")
                    
                    # Try tokenizing
                    sample_data = [data]
                    dataset = ChatDataset(data=sample_data, tokenizer=tokenizer)
                    
                    if len(dataset) == 0:
                        raise TrainingError(f"Sample {i} tokenized to empty dataset!")
                    
                    # ChatDataset returns dict with messages; we just validate structure
                    # The actual tokenization happens internally in the dataset
                    # Count tokens in the text as a rough estimate
                    for msg in messages:
                        text = msg.get("content", "")
                        # Rough token estimate: ~4 chars per token
                        total_tokens += max(1, len(text) // 4)
                    
                except json.JSONDecodeError as e:
                    raise TrainingError(f"Sample {i} is invalid JSON: {e}")
                except Exception as e:
                    raise TrainingError(f"Sample {i} validation failed: {e}")
            
            results[f"{split}_tokens_avg"] = total_tokens / max_checks if max_checks > 0 else 0
            
        except TrainingError:
            raise
        except Exception as e:
            raise TrainingError(f"Failed to validate {split} data: {e}")
    
    return results


def print_validation_results(results: dict) -> None:
    """Print validation results in a readable format."""
    print(f"\n{'='*70}")
    print("DATA VALIDATION RESULTS")
    print(f"{'='*70}")
    print(f"Train samples: {results['train_samples']:,}")
    print(f"Test samples:  {results['test_samples']:,}")
    print(f"Train tokens (avg): {results['train_tokens_avg']:.0f}")
    print(f"Test tokens (avg):  {results['test_tokens_avg']:.0f}")
    
    if results['errors']:
        print(f"\n‚ö†Ô∏è  Errors found:")
        for err in results['errors']:
            print(f"  - {err}")
    else:
        print(f"\n‚úÖ All validation checks passed!")
    
    print(f"{'='*70}\n")


def load_config(config_path: Path) -> dict:
    """Load YAML config file."""
    with open(config_path) as f:
        return yaml.safe_load(f)


def train_with_validation(
    config_path: Path | None = None,
    model: str | None = None,
    data: str | None = None,
    adapter_path: str | None = None,
    patience: int = 3,
    max_iters: int | None = None,
    cleanup_every: int = 25,
    validate_only: bool = False,
    lora_layers: int = 8,
    lora_rank: int = 8,
    lora_alpha: int = 16,
    lora_dropout: float = 0.1,
    learning_rate: float = 5e-5,
    batch_size: int = 1,
    grad_checkpoint: bool = True,
    val_batches: int = 25,
    steps_per_eval: int = 25,
    steps_per_save: int = 50,
    steps_per_report: int = 10,
    max_seq_length: int = 512,
) -> dict[str, Any]:
    """Train with full validation pipeline.
    
    Args:
        config_path: Path to YAML config (optional)
        model: Model name/path (if no config)
        data: Data directory path (if no config)
        adapter_path: Output directory for adapter (if no config)
        ... (other training params)
        validate_only: If True, only validate data, don't train
        
    Returns:
        Dict with training results
    """
    from mlx_lm import load
    from mlx_lm.tuner import train, TrainingArgs
    from mlx_lm.tuner.datasets import load_dataset, ChatDataset, CacheDataset
    from mlx_lm.tuner.callbacks import TrainingCallback
    import mlx.optimizers as optim
    
    # Load config or build from args
    if config_path:
        config = load_config(config_path)
        model_name = config["model"]
        data_path = PROJECT_ROOT / config["data"]
        adapter_path = PROJECT_ROOT / config["adapter_path"]
        
        # Override with explicit args if provided
        lora_layers = config.get("lora_layers", lora_layers)
        lora_rank = config.get("lora_rank", lora_rank)
        lora_alpha = config.get("lora_alpha", lora_alpha)
        lora_dropout = config.get("lora_dropout", lora_dropout)
        learning_rate = config.get("learning_rate", learning_rate)
        batch_size = config.get("batch_size", batch_size)
        grad_checkpoint = config.get("grad_checkpoint", grad_checkpoint)
        val_batches = config.get("val_batches", val_batches)
        steps_per_eval = config.get("steps_per_eval", steps_per_eval)
        steps_per_save = config.get("save_every", steps_per_save)
        steps_per_report = config.get("steps_per_report", steps_per_report)
        max_seq_length = config.get("max_seq_length", max_seq_length)
        iters = max_iters or config.get("iters", 500)
    else:
        if not all([model, data, adapter_path]):
            raise TrainingError("Must provide --config or all of --model, --data, --adapter-path")
        model_name = model
        data_path = PROJECT_ROOT / data
        adapter_path = PROJECT_ROOT / adapter_path
        iters = max_iters or 500
    
    # Create adapter directory
    adapter_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\n{'='*70}")
    print("JARVIS END-TO-END TRAINING")
    print(f"{'='*70}")
    print(f"Model: {model_name}")
    print(f"Data: {data_path}")
    print(f"Adapter: {adapter_path}")
    print(f"Max iterations: {iters}")
    print(f"{'='*70}\n")
    
    # Step 1: Load model and tokenizer
    print("Step 1: Loading model and tokenizer...")
    try:
        model_obj, tokenizer = load(model_name)
        print(f"  ‚úÖ Model loaded: {model_name}")
    except Exception as e:
        raise TrainingError(f"Failed to load model: {e}")
    
    # Step 2: Validate data format
    print("\nStep 2: Validating data format...")
    try:
        validation_results = validate_data_format(data_path, tokenizer)
        print_validation_results(validation_results)
    except TrainingError:
        raise
    except Exception as e:
        raise TrainingError(f"Data validation failed: {e}")
    
    if validate_only:
        print("‚úÖ Validation complete. Exiting without training.")
        return {"status": "validated", "validation": validation_results}
    
    # Step 3: Load datasets
    print("Step 3: Loading datasets...")
    try:
        train_path = data_path / "train.jsonl"
        test_path = data_path / "test.jsonl"
        
        train_raw = []
        with open(train_path) as f:
            for line in f:
                train_raw.append(json.loads(line))
        
        test_raw = []
        with open(test_path) as f:
            for line in f:
                test_raw.append(json.loads(line))
        
        train_set = ChatDataset(data=train_raw, tokenizer=tokenizer)
        test_set = ChatDataset(data=test_raw, tokenizer=tokenizer)
        
        print(f"  ‚úÖ Train: {len(train_set)} examples")
        print(f"  ‚úÖ Test:  {len(test_set)} examples")
    except Exception as e:
        raise TrainingError(f"Failed to load datasets: {e}")
    
    # Step 4: Setup training args
    adapter_file = adapter_path / "adapters.safetensors"
    training_args = TrainingArgs(
        batch_size=batch_size,
        iters=iters,
        val_batches=val_batches,
        steps_per_report=steps_per_report,
        steps_per_eval=steps_per_eval,
        steps_per_save=steps_per_save,
        grad_checkpoint=grad_checkpoint,
        adapter_file=str(adapter_file),
        max_seq_length=max_seq_length,
    )
    
    # Create optimizer
    optimizer = optim.Adam(learning_rate=learning_rate)
    print(f"\nStep 4: Training setup")
    print(f"  Learning rate: {learning_rate}")
    print(f"  Batch size: {batch_size}")
    print(f"  LoRA: layers={lora_layers}, rank={lora_rank}, alpha={lora_alpha}")
    
    # State tracking
    state = TrainingState(patience=patience)
    best_adapter_path: Path | None = None
    start_time = time.perf_counter()
    interrupted = False
    
    # Training callback
    class EarlyStoppingCallback(TrainingCallback):
        def __init__(self, state, adapter_path, cleanup_every):
            self.state = state
            self.adapter_path = adapter_path
            self.cleanup_every = cleanup_every
            self.best_adapter_path = None
            
        def on_train_loss_report(self, train_info: dict):
            iteration = train_info.get("iteration", 0)
            if iteration > 0 and iteration % self.cleanup_every == 0:
                mx.clear_cache()
        
        def on_val_loss_report(self, val_info: dict):
            nonlocal best_adapter_path
            iteration = val_info.get("iteration", 0)
            val_loss = val_info.get("val_loss", float('inf'))
            
            is_best = self.state.update(iteration, val_loss)
            status = "üåü NEW BEST" if is_best else ""
            print(f"\n[Eval @ {iteration}] Val loss: {val_loss:.4f} {status}")
            
            # Save best checkpoint
            if is_best:
                best_path = self.adapter_path / "best_adapters.safetensors"
                current_path = self.adapter_path / "adapters.safetensors"
                if current_path.exists():
                    shutil.copy2(current_path, best_path)
                    print(f"  üíæ Saved best checkpoint")
                    best_adapter_path = best_path
    
    callback = EarlyStoppingCallback(state, adapter_path, cleanup_every)
    
    # Handle interrupt
    def signal_handler(sig, frame):
        nonlocal interrupted
        interrupted = True
        print("\n\n‚ö†Ô∏è  Interrupt received, finishing current iteration...")
    
    signal.signal(signal.SIGINT, signal_handler)
    
    # Step 5: Run training
    print(f"\n{'='*70}")
    print("STARTING TRAINING")
    print(f"{'='*70}\n")
    
    try:
        train(
            model=model_obj,
            args=training_args,
            optimizer=optimizer,
            train_dataset=CacheDataset(train_set),
            val_dataset=CacheDataset(test_set),
            training_callback=callback,
        )
    except Exception as e:
        print(f"\n‚ùå Training error: {e}")
        import traceback
        traceback.print_exc()
        raise TrainingError(f"Training failed: {e}")
    
    elapsed = time.perf_counter() - start_time
    mx.clear_cache()
    
    # Step 6: Save results
    result = {
        "config": str(config_path) if config_path else None,
        "adapter_path": str(adapter_path),
        "final_iteration": state.iteration,
        "best_iteration": state.best_iteration,
        "best_val_loss": state.best_val_loss,
        "val_loss_history": state.val_loss_history,
        "stopped_early": state.should_stop(),
        "interrupted": interrupted,
        "elapsed_seconds": elapsed,
        "best_adapter": str(best_adapter_path) if best_adapter_path else None,
        "validation": validation_results,
    }
    
    results_file = adapter_path / "training_results.json"
    with open(results_file, 'w') as f:
        json.dump(result, f, indent=2)
    
    # Step 7: Create adapter_config.json for loading
    adapter_config = {
        "adapter_path": str(adapter_path),
        "model": model_name,
        "data": str(data_path.relative_to(PROJECT_ROOT)),
        "fine_tune_type": "lora",
        "lora_layers": lora_layers,
        "lora_rank": lora_rank,
        "lora_alpha": lora_alpha,
        "lora_dropout": lora_dropout,
        "lora_parameters": {
            "rank": lora_rank,
            "alpha": lora_alpha,
            "dropout": lora_dropout,
            "scale": lora_alpha / lora_rank,
        },
        "num_layers": lora_layers,
        "learning_rate": learning_rate,
        "batch_size": batch_size,
        "iters": iters,
        "grad_checkpoint": grad_checkpoint,
        "max_seq_length": max_seq_length,
        "steps_per_eval": steps_per_eval,
        "steps_per_save": steps_per_save,
        "val_batches": val_batches,
    }
    
    config_file = adapter_path / "adapter_config.json"
    with open(config_file, 'w') as f:
        json.dump(adapter_config, f, indent=2)
    
    # Copy best to adapters.safetensors if best exists
    if best_adapter_path and best_adapter_path.exists():
        final_adapter = adapter_path / "adapters.safetensors"
        shutil.copy2(best_adapter_path, final_adapter)
        print(f"\n  Copied best checkpoint to adapters.safetensors")
    
    # Print summary
    print(f"\n{'='*70}")
    print("TRAINING COMPLETE")
    print(f"{'='*70}")
    print(f"Best iteration: {state.best_iteration}")
    print(f"Best val loss: {state.best_val_loss:.4f}")
    print(f"Total iterations: {state.iteration}")
    print(f"Time: {elapsed/60:.1f} minutes")
    print(f"Results saved to: {results_file}")
    print(f"Adapter config: {config_file}")
    
    return result


def main() -> int:
    parser = argparse.ArgumentParser(
        description="End-to-end training with validation",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Train with config
  %(prog)s --config ft_configs/experiment_1.2b.yaml
  
  # Validate data only
  %(prog)s --config ft_configs/experiment_1.2b.yaml --validate-only
  
  # Quick test run
  %(prog)s --config ft_configs/experiment_1.2b.yaml --max-iters 50
        """
    )
    
    # Config or manual params
    parser.add_argument("--config", type=Path, help="Path to YAML config file")
    parser.add_argument("--model", help="Model name/path (e.g., mlx-community/LFM2-1.2B-4bit)")
    parser.add_argument("--data", help="Data directory path (e.g., data/personal/raw_style_variable)")
    parser.add_argument("--adapter-path", help="Output directory for adapter")
    
    # Training params
    parser.add_argument("--max-iters", type=int, help="Max iterations (overrides config)")
    parser.add_argument("--patience", type=int, default=3, help="Early stopping patience")
    parser.add_argument("--cleanup-every", type=int, default=25, help="Clear cache every N iterations")
    parser.add_argument("--learning-rate", type=float, default=5e-5, help="Learning rate")
    parser.add_argument("--batch-size", type=int, default=1, help="Batch size")
    parser.add_argument("--lora-rank", type=int, default=8, help="LoRA rank")
    parser.add_argument("--lora-alpha", type=int, default=16, help="LoRA alpha")
    parser.add_argument("--lora-layers", type=int, default=8, help="Number of LoRA layers")
    parser.add_argument("--lora-dropout", type=float, default=0.1, help="LoRA dropout")
    parser.add_argument("--grad-checkpoint", action="store_true", default=True, help="Use gradient checkpointing")
    parser.add_argument("--val-batches", type=int, default=25, help="Validation batches")
    parser.add_argument("--steps-per-eval", type=int, default=25, help="Steps between evaluations")
    
    # Control
    parser.add_argument("--validate-only", action="store_true", help="Only validate data, don't train")
    
    args = parser.parse_args()
    
    if not args.config and not all([args.model, args.data, args.adapter_path]):
        parser.error("Must provide --config or all of --model, --data, --adapter-path")
    
    try:
        result = train_with_validation(
            config_path=args.config,
            model=args.model,
            data=args.data,
            adapter_path=args.adapter_path,
            patience=args.patience,
            max_iters=args.max_iters,
            cleanup_every=args.cleanup_every,
            validate_only=args.validate_only,
            lora_layers=args.lora_layers,
            lora_rank=args.lora_rank,
            lora_alpha=args.lora_alpha,
            lora_dropout=args.lora_dropout,
            learning_rate=args.learning_rate,
            batch_size=args.batch_size,
            grad_checkpoint=args.grad_checkpoint,
            val_batches=args.val_batches,
            steps_per_eval=args.steps_per_eval,
        )
        return 0
    except TrainingError as e:
        print(f"\n‚ùå {e}")
        return 1
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
