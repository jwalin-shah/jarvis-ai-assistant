Makefile:467: warning: overriding commands for target `train-category-svm'
Makefile:385: warning: ignoring old commands for target `train-category-svm'
uv run ruff check .
I001 [*] Import block is un-sorted or un-formatted
  --> evals/batch_eval.py:38:1
   |
36 | # ---------------------------------------------------------------------------
37 |
38 | from evals.judge_config import JUDGE_MODEL, get_judge_client as _get_judge_client  # noqa: E402
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
39 |
40 | ANTI_AI_PHRASES = [
   |
help: Organize imports

F821 Undefined name `benchmark_v2_classifier`
   --> evals/benchmarks/classifier/classifier_benchmark.py:648:24
    |
646 |     else:
647 |         print(f"\nRunning {args.benchmark} benchmark...")
648 |         bench_result = benchmark_v2_classifier(messages, args.batch_size)
    |                        ^^^^^^^^^^^^^^^^^^^^^^^
649 |         final_results = bench_result.to_dict()
    |

I001 [*] Import block is un-sorted or un-formatted
  --> evals/dspy_reply.py:12:1
   |
10 |   """
11 |
12 | / from __future__ import annotations
13 | |
14 | | import json
15 | | import re
16 | | from pathlib import Path
17 | |
18 | | import dspy
19 | |
20 | | from evals.judge_config import JUDGE_MODEL, get_judge_client as _get_judge_client
   | |_________________________________________________________________________________^
   |
help: Organize imports

F401 [*] `pathlib.Path` imported but unused
  --> evals/dspy_reply.py:16:21
   |
14 | import json
15 | import re
16 | from pathlib import Path
   |                     ^^^^
17 |
18 | import dspy
   |
help: Remove unused import: `pathlib.Path`

I001 [*] Import block is un-sorted or un-formatted
  --> evals/rag_eval.py:39:1
   |
37 |             os.environ.setdefault(key.strip(), val.strip())
38 |
39 | from evals.judge_config import JUDGE_MODEL, get_judge_client  # noqa: E402
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |
help: Organize imports

F541 [*] f-string without any placeholders
   --> experiments/scripts/analyze_dailydialog_results.py:230:11
    |
228 |         label_map_flag = "--label-map 4class"
229 |
230 |     print(f"  uv run python scripts/train_category_svm.py \\")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
231 |     print(f"    --data-dir data/dailydialog_native \\")
232 |     print(f"    {label_map_flag}")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> experiments/scripts/analyze_dailydialog_results.py:231:11
    |
230 |     print(f"  uv run python scripts/train_category_svm.py \\")
231 |     print(f"    --data-dir data/dailydialog_native \\")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
232 |     print(f"    {label_map_flag}")
233 |     print()
    |
help: Remove extraneous `f` prefix

F821 Undefined name `X_test`
   --> experiments/scripts/dailydialog_sweep.py:294:35
    |
292 |         # Add dataset info
293 |         result["train_size"] = len(X_train_balanced)
294 |         result["test_size"] = len(X_test)
    |                                   ^^^^^^
295 |         result["train_distribution"] = dict(Counter(y_train_balanced))
    |

I001 [*] Import block is un-sorted or un-formatted
  --> experiments/scripts/prepare_data.py:83:5
   |
81 |       """
82 |       MINORITY_CLASSES = {"AGREE", "DECLINE", "DEFER"}
83 | /     from jarvis.classifiers.response_classifier import get_response_classifier
84 | |     from jarvis.db import get_db
85 | |     from jarvis.embedding_adapter import get_embedder
   | |_____________________________________________________^
86 |
87 |       logger.info("Loading pairs from database...")
   |
help: Organize imports

E501 Line too long (104 > 100)
   --> jarvis/_cli_main.py:274:101
    |
272 |         console.print("[red]Error: Please specify a db subcommand[/red]")
273 |         console.print(
274 |             "Available: init, sync-contacts, add-contact, list-contacts, extract, stats, build-profiles"
    |                                                                                                     ^^^^
275 |         )
276 |         return 1
    |

E501 Line too long (125 > 100)
   --> jarvis/classifiers/category_classifier.py:158:101
    |
156 |     # 2. you_modal: "can you", "could you", "would you", "will you"
157 |     text_lower = text.lower()
158 |     you_modal = 1.0 if any(p in text_lower for p in ["can you", "could you", "would you", "will you", "should you"]) else 0.0
    |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^
159 |     features.append(you_modal)
    |

E501 Line too long (105 > 100)
   --> jarvis/classifiers/category_classifier.py:177:101
    |
176 |     # 6. i_will: "I'll", "I will", "I'm gonna"
177 |     i_will = 1.0 if any(p in text_lower for p in ["i'll", "i will", "i'm gonna", "ima", "imma"]) else 0.0
    |                                                                                                     ^^^^^
178 |     features.append(i_will)
    |

E501 Line too long (103 > 100)
   --> jarvis/classifiers/category_classifier.py:186:101
    |
185 |     # 8. first_person_count
186 |     first_person = sum(1 for token in doc if token.text.lower() in ("i", "me", "my", "mine", "myself"))
    |                                                                                                     ^^^
187 |     features.append(float(first_person))
    |

E501 Line too long (101 > 100)
   --> jarvis/classifiers/category_classifier.py:190:101
    |
189 |     # 9. agreement: Agreement words
190 |     agreement_words = {"sure", "okay", "ok", "yes", "yeah", "yep", "yup", "sounds good", "bet", "fs"}
    |                                                                                                     ^
191 |     has_agreement = 1.0 if any(word in text_lower for word in agreement_words) else 0.0
192 |     features.append(has_agreement)
    |

E501 Line too long (105 > 100)
   --> jarvis/classifiers/category_classifier.py:203:101
    |
202 |     # 12. second_person_count
203 |     second_person = sum(1 for token in doc if token.text.lower() in ("you", "your", "yours", "yourself"))
    |                                                                                                     ^^^^^
204 |     features.append(float(second_person))
    |

E501 Line too long (112 > 100)
   --> jarvis/classifiers/category_classifier.py:211:101
    |
210 |     # 14. is_interrogative: Question indicators
211 |     is_question = 1.0 if "?" in text or any(token.tag_ in ("WDT", "WP", "WP$", "WRB") for token in doc) else 0.0
    |                                                                                                     ^^^^^^^^^^^^
212 |     features.append(is_question)
    |

W291 Trailing whitespace
 --> jarvis/memory_layer.py:3:68
  |
1 | """Adaptive Memory Layer using Mem0.
2 |
3 | Provides long-term, self-improving user memory that persists across 
  |                                                                    ^
4 | conversations and sessions.
5 | """
  |
help: Remove trailing whitespace

W293 Blank line contains whitespace
  --> jarvis/memory_layer.py:13:1
   |
11 | class JARVISMemory:
12 |     """Memory layer wrapper for JARVIS using Mem0."""
13 |     
   | ^^^^
14 |     def __init__(self, user_id: str = "default_user"):
15 |         self.user_id = user_id
   |
help: Remove whitespace from blank line

invalid-syntax: f-string: unterminated string
  --> jarvis/memory_layer.py:23:43
   |
21 |         """Record an interaction to learn from it."""
22 |         # Mem0 will extract facts automatically
23 |         self.memory.add(f"User: {user_msg}
   |                                           ^
24 | Assistant: {assistant_msg}", user_id=self.user_id)
   |

invalid-syntax: Expected `)`, found dedent
  --> jarvis/memory_layer.py:24:1
   |
22 |         # Mem0 will extract facts automatically
23 |         self.memory.add(f"User: {user_msg}
24 | Assistant: {assistant_msg}", user_id=self.user_id)
   | ^
25 |
26 |     def get_relevant_facts(self, query: str):
   |

invalid-syntax: missing closing quote in string literal
  --> jarvis/memory_layer.py:24:27
   |
22 |         # Mem0 will extract facts automatically
23 |         self.memory.add(f"User: {user_msg}
24 | Assistant: {assistant_msg}", user_id=self.user_id)
   |                           ^^^^^^^^^^^^^^^^^^^^^^^^
25 |
26 |     def get_relevant_facts(self, query: str):
   |

invalid-syntax: Unexpected indentation
  --> jarvis/memory_layer.py:26:1
   |
24 | Assistant: {assistant_msg}", user_id=self.user_id)
25 |
26 |     def get_relevant_facts(self, query: str):
   | ^^^^
27 |         """Retrieve learned facts relevant to the current query."""
28 |         results = self.memory.search(query, user_id=self.user_id)
   |

invalid-syntax: Expected a statement
  --> jarvis/memory_layer.py:38:1
   |
37 | # Global memory instance
38 | _memory = None
   | ^
39 |
40 | def get_memory():
   |

E501 Line too long (102 > 100)
   --> jarvis/nlp/ner_client.py:266:101
    |
265 |     Returns 14 features:
266 |     - Directive indicators (5): imperative, you+modal, request verbs, starts_modal, directive_question
    |                                                                                                     ^^
267 |     - Commissive indicators (4): i_will, promise_verb, first_person_count, agreement
268 |     - General syntactic (5): modal_count, verb_count, second_person_count, has_negation, is_interrogative
    |

E501 Line too long (105 > 100)
   --> jarvis/nlp/ner_client.py:268:101
    |
266 |     - Directive indicators (5): imperative, you+modal, request verbs, starts_modal, directive_question
267 |     - Commissive indicators (4): i_will, promise_verb, first_person_count, agreement
268 |     - General syntactic (5): modal_count, verb_count, second_person_count, has_negation, is_interrogative
    |                                                                                                     ^^^^^
269 |
270 |     Args:
    |

F601 Dictionary key literal `"professional"` repeated
    --> jarvis/prompts.py:1814:5
     |
1812 |     "mixed": "statement",
1813 |     "celebration": "social",
1814 |     "professional": "brief",
     |     ^^^^^^^^^^^^^^
1815 |     # Ambiguous / low context
1816 |     "clarify": "clarify",
     |
help: Remove repeated key literal `"professional"`

F601 Dictionary key literal `"clarify"` repeated
    --> jarvis/prompts.py:1816:5
     |
1814 |     "professional": "brief",
1815 |     # Ambiguous / low context
1816 |     "clarify": "clarify",
     |     ^^^^^^^^^
1817 |     "edge_case": "clarify",
1818 |     "unknown": "clarify",
     |
help: Remove repeated key literal `"clarify"`

E501 Line too long (105 > 100)
   --> jarvis/reply_service.py:200:101
    |
198 |             base_confidence *= 0.9
199 |
200 |         confidence = "high" if base_confidence >= 0.7 else "medium" if base_confidence >= 0.45 else "low"
    |                                                                                                     ^^^^^
201 |
202 |         metadata = {
    |

I001 [*] Import block is un-sorted or un-formatted
   --> jarvis/reply_service.py:247:9
    |
246 |           # 1b. Category classification and routing
247 | /         from jarvis.classifiers.category_classifier import classify_category
248 | |         from jarvis.prompts import ACKNOWLEDGE_TEMPLATES, CLOSING_TEMPLATES, get_category_config
249 | |         import random
    | |_____________________^
250 |
251 |           category_result = classify_category(incoming, context=thread or [], mobilization=mobilization)
    |
help: Organize imports

E501 Line too long (102 > 100)
   --> jarvis/reply_service.py:251:101
    |
249 |         import random
250 |
251 |         category_result = classify_category(incoming, context=thread or [], mobilization=mobilization)
    |                                                                                                     ^^
252 |         category_config = get_category_config(category_result.category)
    |

E501 Line too long (108 > 100)
   --> jarvis/reply_service.py:359:101
    |
357 |             context_messages = thread[-context_depth:] if context_depth > 0 else []
358 |         elif chat_id and context_depth > 0:
359 |             context_messages = self.context_service.fetch_conversation_context(chat_id, limit=context_depth)
    |                                                                                                     ^^^^^^^^
360 |
361 |         context = "\n".join(context_messages) + f"\n[Incoming]: {incoming}"
    |

F401 [*] `os` imported but unused
  --> jarvis/utils/memory.py:16:8
   |
15 | import logging
16 | import os
   |        ^^
17 | import platform
18 | import re
   |
help: Remove unused import: `os`

UP035 [*] Import from `collections.abc` instead: `Iterator`
  --> jarvis/utils/memory.py:23:1
   |
21 | from contextlib import contextmanager
22 | from dataclasses import dataclass
23 | from typing import Iterator
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^
24 |
25 | import psutil
   |
help: Import from `collections.abc`

N818 Exception name `SwapThresholdExceeded` should be named with an Error suffix
  --> jarvis/utils/memory.py:32:7
   |
32 | class SwapThresholdExceeded(Exception):
   |       ^^^^^^^^^^^^^^^^^^^^^
33 |     """Raised when swap usage exceeds configured threshold."""
   |

E501 Line too long (113 > 100)
   --> models/bert_embedder.py:218:101
    |
217 |     mem_before = proc.memory_info()
218 |     print(f"[BERT] BEFORE mx.load: RSS={mem_before.rss/1024/1024:.1f} MB, VMS={mem_before.vms/1024/1024:.1f} MB")
    |                                                                                                     ^^^^^^^^^^^^^
219 |
220 |     hf_weights = mx.load(str(weights_path))
    |

E501 Line too long (210 > 100)
   --> models/bert_embedder.py:223:101
    |
222 | â€¦
223 | â€¦mem_before.rss)/1024/1024:.1f}), VMS={mem_after.vms/1024/1024:.1f} MB (+{(mem_after.vms-mem_before.vms)/1024/1024:.1f})")
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
224 | â€¦
225 | â€¦
    |

E501 Line too long (134 > 100)
   --> models/bert_embedder.py:254:101
    |
253 |     mem_before_load = proc.memory_info()
254 |     print(f"[BERT] BEFORE model.load_weights: RSS={mem_before_load.rss/1024/1024:.1f} MB, VMS={mem_before_load.vms/1024/1024:.1f} MB")
    |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
255 |
256 |     model.load_weights(list(new_weights.items()))
    |

E501 Line too long (251 > 100)
   --> models/bert_embedder.py:259:101
    |
258 | â€¦
259 | â€¦after_load.rss-mem_before_load.rss)/1024/1024:.1f}), VMS={mem_after_load.vms/1024/1024:.1f} MB (+{(mem_after_load.vms-mem_before_load.vms)/1024/1024:.1f})")
    |       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
260 | â€¦
261 | â€¦
    |

E501 Line too long (132 > 100)
   --> models/bert_embedder.py:265:101
    |
264 |     mem_after_del = proc.memory_info()
265 |     print(f"[BERT] AFTER deleting weight dicts: RSS={mem_after_del.rss/1024/1024:.1f} MB, VMS={mem_after_del.vms/1024/1024:.1f} MB")
    |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

E501 Line too long (131 > 100)
   --> models/bert_embedder.py:372:101
    |
371 |             mem_before_eval = proc.memory_info()
372 |             print(f"[BERT] BEFORE mx.eval: RSS={mem_before_eval.rss/1024/1024:.1f} MB, VMS={mem_before_eval.vms/1024/1024:.1f} MB")
    |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
373 |
374 |             mx.eval(self.model.parameters())
    |

E501 Line too long (248 > 100)
   --> models/bert_embedder.py:377:101
    |
376 | â€¦
377 | â€¦er_eval.rss-mem_before_eval.rss)/1024/1024:.1f}), VMS={mem_after_eval.vms/1024/1024:.1f} MB (+{(mem_after_eval.vms-mem_before_eval.vms)/1024/1024:.1f})")
    |       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
378 | â€¦
379 | â€¦
    |

E501 Line too long (137 > 100)
   --> models/bert_embedder.py:385:101
    |
384 | â€¦
385 | â€¦{mem_after_clear.rss/1024/1024:.1f} MB, VMS={mem_after_clear.vms/1024/1024:.1f} MB")
    |                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
386 | â€¦
387 | â€¦
    |

E501 Line too long (102 > 100)
   --> scripts/batch_eval.py:275:101
    |
273 |         print(f"  Generated: {r['generated'][:100]}")
274 |         print(
275 |             f"  Sim: {r['similarity']:.3f} | Conf: {r['confidence']} | Time: {r['gen_time_ms']:.0f}ms"
    |                                                                                                     ^^
276 |         )
    |

E402 Module level import not at top of file
  --> scripts/batch_review_llm.py:14:1
   |
12 | sys.path.insert(0, str(PROJECT_ROOT))
13 |
14 | from datasets import load_dataset
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
15 | from scripts.labeling_functions import get_registry
16 | from scripts.label_aggregation import aggregate_labels
   |

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/batch_review_llm.py:14:1
   |
12 |   sys.path.insert(0, str(PROJECT_ROOT))
13 |
14 | / from datasets import load_dataset
15 | | from scripts.labeling_functions import get_registry
16 | | from scripts.label_aggregation import aggregate_labels
17 | | from evals.judge_config import get_judge_client
   | |_______________________________________________^
18 |
19 |   CLASSIFICATION_PROMPT_TEMPLATE = """For each message, check the rules IN ORDER and pick the FIRST match:
   |
help: Organize imports

E402 Module level import not at top of file
  --> scripts/batch_review_llm.py:15:1
   |
14 | from datasets import load_dataset
15 | from scripts.labeling_functions import get_registry
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
16 | from scripts.label_aggregation import aggregate_labels
17 | from evals.judge_config import get_judge_client
   |

E402 Module level import not at top of file
  --> scripts/batch_review_llm.py:16:1
   |
14 | from datasets import load_dataset
15 | from scripts.labeling_functions import get_registry
16 | from scripts.label_aggregation import aggregate_labels
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
17 | from evals.judge_config import get_judge_client
   |

E402 Module level import not at top of file
  --> scripts/batch_review_llm.py:17:1
   |
15 | from scripts.labeling_functions import get_registry
16 | from scripts.label_aggregation import aggregate_labels
17 | from evals.judge_config import get_judge_client
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
18 |
19 | CLASSIFICATION_PROMPT_TEMPLATE = """For each message, check the rules IN ORDER and pick the FIRST match:
   |

E501 Line too long (104 > 100)
  --> scripts/batch_review_llm.py:19:101
   |
17 | from evals.judge_config import get_judge_client
18 |
19 | CLASSIFICATION_PROMPT_TEMPLATE = """For each message, check the rules IN ORDER and pick the FIRST match:
   |                                                                                                     ^^^^
20 |
21 | RULE 1: closing
   |

E501 Line too long (203 > 100)
  --> scripts/batch_review_llm.py:26:101
   |
25 | â€¦
26 | â€¦ "yep", "yup", "sure", "thanks", "thank you", "gotcha", "fine", "alright", "cool", "k", "kk", or just emoji like "ðŸ‘"?
   |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
27 | â€¦
   |

E501 Line too long (115 > 100)
  --> scripts/batch_review_llm.py:34:101
   |
33 | RULE 4: question
34 | - Check: Does it contain "?" OR start with "what", "when", "where", "who", "why", "how", "is", "are", "do", "does"?
   |                                                                                                     ^^^^^^^^^^^^^^^
35 | - If YES â†’ category = question
   |

E501 Line too long (173 > 100)
  --> scripts/batch_review_llm.py:38:101
   |
37 | â€¦
38 | â€¦", "frustrated", "love", "hate", "amazing", "terrible" OR "!!" (2+ exclamation marks) OR ALLCAPS words?
   |                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
39 | â€¦
   |

E501 Line too long (106 > 100)
  --> scripts/batch_review_llm.py:59:101
   |
57 |     messages_block = "\n\n".join([
58 |         f"Message {i + 1}:\n"
59 |         f"Previous: \"{ex['last_message'][:100] if ex['last_message'] else '(start of conversation)'}\"\n"
   |                                                                                                     ^^^^^^
60 |         f"Current: \"{ex['text']}\""
61 |         for i, ex in enumerate(examples)
   |

F541 [*] f-string without any placeholders
   --> scripts/batch_review_llm.py:203:19
    |
202 |         if heuristic == llm:
203 |             print(f"  âœ“ AGREE")
    |                   ^^^^^^^^^^^^
204 |         else:
205 |             print(f"  âœ— DISAGREE")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/batch_review_llm.py:205:19
    |
203 |             print(f"  âœ“ AGREE")
204 |         else:
205 |             print(f"  âœ— DISAGREE")
    |                   ^^^^^^^^^^^^^^^
206 |         print()
    |
help: Remove extraneous `f` prefix

E501 Line too long (121 > 100)
  --> scripts/claude_labels.py:36:101
   |
34 |     if "?" in text:
35 |         is_question = True
36 |     elif any(text_lower.startswith(w) for w in ["what", "where", "when", "who", "why", "how", "which", "whose", "whom"]):
   |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^
37 |         is_question = True
38 |     elif text_lower.startswith(("are you", "is it", "is this", "is that", "did you", "do you", "does", "can you", "could you", "would â€¦
   |

E501 Line too long (167 > 100)
  --> scripts/claude_labels.py:38:101
   |
36 | â€¦hen", "who", "why", "how", "which", "whose", "whom"]):
37 | â€¦
38 | â€¦that", "did you", "do you", "does", "can you", "could you", "would you", "should you", "will you")):
   |                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
39 | â€¦
40 | â€¦u", "would you", "will you"]) and "please" not in text_lower:
   |

E501 Line too long (128 > 100)
  --> scripts/claude_labels.py:40:101
   |
38 |     elif text_lower.startswith(("are you", "is it", "is this", "is that", "did you", "do you", "does", "can you", "could you", "would â€¦
39 |         # Check if it's a genuine question or a polite directive
40 |         if any(word in text_lower for word in ["can you", "could you", "would you", "will you"]) and "please" not in text_lower:
   |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
41 |             # Likely a directive phrased as question
42 |             is_question = False
   |

E501 Line too long (125 > 100)
  --> scripts/claude_labels.py:57:101
   |
55 |         is_directive = True
56 |         is_question = False  # Override question if it's a polite request
57 |     elif text_lower.startswith(("pick", "call", "send", "help", "go", "come", "get", "take", "make", "do", "don't", "stop")):
   |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^
58 |         is_directive = True
59 |     elif " gotta " in text_lower or text_lower.startswith("gotta "):
   |

E402 Module level import not at top of file
   --> scripts/claude_labels.py:108:1
    |
107 | # Analysis
108 | from collections import Counter
    | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
109 | from sklearn.metrics import classification_report, confusion_matrix
    |

I001 [*] Import block is un-sorted or un-formatted
   --> scripts/claude_labels.py:108:1
    |
107 |   # Analysis
108 | / from collections import Counter
109 | | from sklearn.metrics import classification_report, confusion_matrix
    | |___________________________________________________________________^
110 |
111 |   # Extract predictions
    |
help: Organize imports

E402 Module level import not at top of file
   --> scripts/claude_labels.py:109:1
    |
107 | # Analysis
108 | from collections import Counter
109 | from sklearn.metrics import classification_report, confusion_matrix
    | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
110 |
111 | # Extract predictions
    |

E501 Line too long (118 > 100)
   --> scripts/claude_labels.py:175:101
    |
173 | print("LightGBM Per-Class (Claude as ground truth)")
174 | print("=" * 70)
175 | print(classification_report(claude_labels, lgbm_preds, labels=labels, target_names=labels, digits=3, zero_division=0))
    |                                                                                                     ^^^^^^^^^^^^^^^^^^
176 |
177 | print("=" * 70)
    |

E501 Line too long (117 > 100)
   --> scripts/claude_labels.py:180:101
    |
178 | print("LLM Per-Class (Claude as ground truth)")
179 | print("=" * 70)
180 | print(classification_report(claude_labels, llm_preds, labels=labels, target_names=labels, digits=3, zero_division=0))
    |                                                                                                     ^^^^^^^^^^^^^^^^^
181 |
182 | # Winner
    |

E501 Line too long (103 > 100)
  --> scripts/claude_manual_labels.py:27:101
   |
26 |     # needs_answer: wh-questions expecting factual info
27 |     if any(text.startswith(w) for w in ["what ", "when ", "where ", "who ", "how ", "why ", "which "]):
   |                                                                                                     ^^^
28 |         label = "needs_answer"
29 |     elif "?" in orig and any(f" {w} " in f" {text} " for w in ["what", "when", "where", "who", "how", "why", "which"]):
   |

E501 Line too long (119 > 100)
  --> scripts/claude_manual_labels.py:29:101
   |
27 |     if any(text.startswith(w) for w in ["what ", "when ", "where ", "who ", "how ", "why ", "which "]):
28 |         label = "needs_answer"
29 |     elif "?" in orig and any(f" {w} " in f" {text} " for w in ["what", "when", "where", "who", "how", "why", "which"]):
   |                                                                                                     ^^^^^^^^^^^^^^^^^^^
30 |         label = "needs_answer"
   |

E501 Line too long (110 > 100)
  --> scripts/claude_manual_labels.py:35:101
   |
33 |     elif "?" in orig and label is None:
34 |         # Yes/no question or request
35 |         if any(p in text for p in ["can you", "could you", "will you", "would you", "shall we", "should we"]):
   |                                                                                                     ^^^^^^^^^^
36 |             label = "needs_confirmation"
37 |         elif "?" in orig and " or " in text:  # either/or question
   |

E501 Line too long (101 > 100)
  --> scripts/claude_manual_labels.py:44:101
   |
43 |     # Explicit requests (no ?)
44 |     elif any(p in text for p in ["can you", "could you", "will you", "would you please", "please "]):
   |                                                                                                     ^
45 |         label = "needs_confirmation"
   |

E501 Line too long (112 > 100)
  --> scripts/claude_manual_labels.py:48:101
   |
47 |     # needs_empathy: emotional content
48 |     elif any(w in text for w in ["sorry about", "i am so sorry", "my condolences", "that sucks", "this sucks"]):
   |                                                                                                     ^^^^^^^^^^^^
49 |         label = "needs_empathy"
50 |     elif any(w in text for w in ["congrat", "i am so excited", "i am so happy", "that is amazing", "so proud", "i got the job"]):
   |

E501 Line too long (129 > 100)
  --> scripts/claude_manual_labels.py:50:101
   |
48 |     elif any(w in text for w in ["sorry about", "i am so sorry", "my condolences", "that sucks", "this sucks"]):
49 |         label = "needs_empathy"
50 |     elif any(w in text for w in ["congrat", "i am so excited", "i am so happy", "that is amazing", "so proud", "i got the job"]):
   |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
51 |         label = "needs_empathy"
52 |     elif any(e in orig for e in ["ðŸ˜­", "ðŸ˜¢", "ðŸŽ‰", "â¤ï¸", "ðŸ’ª", "ðŸ¥³", ":("]):
   |

E501 Line too long (101 > 100)
  --> scripts/claude_manual_labels.py:62:101
   |
61 |     # Special overrides
62 |     if text in ["ok", "okay", "yeah", "yep", "sure", "alright", "thanks", "thank you", "no problem"]:
   |                                                                                                     ^
63 |         label = "conversational"
64 |     if text.startswith(("i see", "i hope", "me too", "same here", "exactly")):
   |

E402 Module level import not at top of file
  --> scripts/claude_manual_labels.py:86:1
   |
85 | # Stats
86 | from collections import Counter
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
87 | label_counts = Counter(m["manual_label"] for m in messages)
88 | confidence_counts = Counter(m["manual_confidence"] for m in messages)
   |

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/claude_manual_labels.py:86:1
   |
85 | # Stats
86 | from collections import Counter
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
87 | label_counts = Counter(m["manual_label"] for m in messages)
88 | confidence_counts = Counter(m["manual_confidence"] for m in messages)
   |
help: Organize imports

F541 [*] f-string without any placeholders
  --> scripts/claude_manual_labels.py:91:7
   |
90 | print(f"\nâœ… Labeled all {len(messages)} messages")
91 | print(f"\nLabel distribution:")
   |       ^^^^^^^^^^^^^^^^^^^^^^^^
92 | for label, count in sorted(label_counts.items(), key=lambda x: -x[1]):
93 |     pct = count / len(messages) * 100
   |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
  --> scripts/claude_manual_labels.py:96:7
   |
94 |     print(f"  {label:20s} {count:3d} ({pct:5.1f}%)")
95 |
96 | print(f"\nConfidence:")
   |       ^^^^^^^^^^^^^^^^
97 | for conf, count in sorted(confidence_counts.items()):
98 |     pct = count / len(messages) * 100
   |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
  --> scripts/compare_c_results.py:72:11
   |
71 |     # Check for gaps
72 |     print(f"\nGap analysis:")
   |           ^^^^^^^^^^^^^^^^^^
73 |     for i in range(len(all_results) - 1):
74 |         curr = all_results[i]
   |
help: Remove extraneous `f` prefix

F401 `numpy` imported but unused; consider using `importlib.util.find_spec` to test for availability
  --> scripts/compare_c_results.py:89:29
   |
87 |         try:
88 |             import matplotlib.pyplot as plt
89 |             import numpy as np
   |                             ^^
90 |
91 |             c_values = [r["C"] for r in all_results]
   |
help: Remove unused import: `numpy`

F841 Local variable `best_idx` is assigned to but never used
   --> scripts/compare_c_results.py:104:13
    |
103 |             # Mark best C
104 |             best_idx = c_values.index(best["C"])
    |             ^^^^^^^^
105 |             plt.axvline(x=best["C"], color='r', linestyle='--', alpha=0.5, label=f'Best C={best["C"]}')
106 |             plt.legend()
    |
help: Remove assignment to unused variable `best_idx`

E501 Line too long (103 > 100)
   --> scripts/compare_c_results.py:105:101
    |
103 |             # Mark best C
104 |             best_idx = c_values.index(best["C"])
105 |             plt.axvline(x=best["C"], color='r', linestyle='--', alpha=0.5, label=f'Best C={best["C"]}')
    |                                                                                                     ^^^
106 |             plt.legend()
    |

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/compare_kernels.py:13:1
   |
11 |   """
12 |
13 | / import argparse
14 | | import json
15 | | import sys
16 | | import time
17 | | from pathlib import Path
18 | |
19 | | import numpy as np
20 | | from sklearn.compose import ColumnTransformer
21 | | from sklearn.metrics import classification_report
22 | | from sklearn.model_selection import StratifiedKFold, cross_val_score
23 | | from sklearn.pipeline import Pipeline
24 | | from sklearn.preprocessing import StandardScaler
25 | | from sklearn.svm import LinearSVC, SVC
   | |______________________________________^
26 |
27 |   PROJECT_ROOT = Path(__file__).parent.parent
   |
help: Organize imports

F401 [*] `sklearn.metrics.classification_report` imported but unused
  --> scripts/compare_kernels.py:21:29
   |
19 | import numpy as np
20 | from sklearn.compose import ColumnTransformer
21 | from sklearn.metrics import classification_report
   |                             ^^^^^^^^^^^^^^^^^^^^^
22 | from sklearn.model_selection import StratifiedKFold, cross_val_score
23 | from sklearn.pipeline import Pipeline
   |
help: Remove unused import: `sklearn.metrics.classification_report`

E402 Module level import not at top of file
  --> scripts/compare_kernels.py:30:1
   |
28 | sys.path.insert(0, str(PROJECT_ROOT))
29 |
30 | from jarvis.utils.memory import get_swap_info, get_memory_info
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
31 |
32 | import psutil
   |

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/compare_kernels.py:30:1
   |
28 |   sys.path.insert(0, str(PROJECT_ROOT))
29 |
30 | / from jarvis.utils.memory import get_swap_info, get_memory_info
31 | |
32 | | import psutil
   | |_____________^
   |
help: Organize imports

E402 Module level import not at top of file
  --> scripts/compare_kernels.py:32:1
   |
30 | from jarvis.utils.memory import get_swap_info, get_memory_info
31 |
32 | import psutil
   | ^^^^^^^^^^^^^
   |

N806 Variable `X_full` in function should be lowercase
  --> scripts/compare_kernels.py:80:5
   |
78 |     metadata = json.loads((args.data_dir / "metadata.json").read_text())
79 |
80 |     X_full, y_full = train_data["X"], train_data["y"]
   |     ^^^^^^
81 |     embedding_dims = metadata["embedding_dims"]
82 |     hand_crafted_dims = metadata["hand_crafted_dims"]
   |

N806 Variable `X_subset` in function should be lowercase
  --> scripts/compare_kernels.py:90:5
   |
88 |     from sklearn.model_selection import train_test_split
89 |
90 |     X_subset, _, y_subset, _ = train_test_split(
   |     ^^^^^^^^
91 |         X_full,
92 |         y_full,
   |

F541 [*] f-string without any placeholders
   --> scripts/compare_kernels.py:99:11
    |
 98 |     # Show class distribution
 99 |     print(f"\nClass distribution:")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^
100 |     for label in sorted(set(y_subset)):
101 |         count = (y_subset == label).sum()
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/compare_kernels.py:237:15
    |
235 |     if improvement > 0.01:  # >1% improvement
236 |         print(f"âœ… RBF wins by {improvement:.4f} ({improvement_pct:.2f}%)")
237 |         print(f"\nRBF can capture non-linear patterns that linear misses!")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
238 |         print(f"â†’ Use RBF kernel for production")
239 |     elif improvement < -0.01:  # Linear wins
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/compare_kernels.py:238:15
    |
236 |         print(f"âœ… RBF wins by {improvement:.4f} ({improvement_pct:.2f}%)")
237 |         print(f"\nRBF can capture non-linear patterns that linear misses!")
238 |         print(f"â†’ Use RBF kernel for production")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
239 |     elif improvement < -0.01:  # Linear wins
240 |         print(f"âœ… Linear wins by {-improvement:.4f} ({-improvement_pct:.2f}%)")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/compare_kernels.py:241:15
    |
239 |     elif improvement < -0.01:  # Linear wins
240 |         print(f"âœ… Linear wins by {-improvement:.4f} ({-improvement_pct:.2f}%)")
241 |         print(f"\nLinear is sufficient - data is already separable!")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
242 |         print(f"â†’ Stick with LinearSVC (faster, simpler)")
243 |     else:  # Negligible difference
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/compare_kernels.py:242:15
    |
240 |         print(f"âœ… Linear wins by {-improvement:.4f} ({-improvement_pct:.2f}%)")
241 |         print(f"\nLinear is sufficient - data is already separable!")
242 |         print(f"â†’ Stick with LinearSVC (faster, simpler)")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
243 |     else:  # Negligible difference
244 |         print(f"âš–ï¸  Tie: difference = {improvement:.4f} ({improvement_pct:.2f}%)")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/compare_kernels.py:245:15
    |
243 |     else:  # Negligible difference
244 |         print(f"âš–ï¸  Tie: difference = {improvement:.4f} ({improvement_pct:.2f}%)")
245 |         print(f"\nBoth kernels perform equally!")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
246 |         print(f"â†’ Use Linear (3-5x faster, lower memory)")
247 |     print(f"{'=' * 60}")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/compare_kernels.py:246:15
    |
244 |         print(f"âš–ï¸  Tie: difference = {improvement:.4f} ({improvement_pct:.2f}%)")
245 |         print(f"\nBoth kernels perform equally!")
246 |         print(f"â†’ Use Linear (3-5x faster, lower memory)")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
247 |     print(f"{'=' * 60}")
    |
help: Remove extraneous `f` prefix

E501 Line too long (103 > 100)
  --> scripts/create_true_gold_standard.py:37:101
   |
36 |     # NEEDS_ANSWER: Asking for factual information with wh-word
37 |     if any(text.startswith(w) for w in ["What ", "When ", "Where ", "Who ", "How ", "Why ", "Which "]):
   |                                                                                                     ^^^
38 |         true_label = "needs_answer"
39 |     elif "?" in text_orig and any(f" {w} " in f" {text_lower} " for w in ["what", "when", "where", "who", "how", "why", "which"]):
   |

E501 Line too long (130 > 100)
  --> scripts/create_true_gold_standard.py:39:101
   |
37 |     if any(text.startswith(w) for w in ["What ", "When ", "Where ", "Who ", "How ", "Why ", "Which "]):
38 |         true_label = "needs_answer"
39 |     elif "?" in text_orig and any(f" {w} " in f" {text_lower} " for w in ["what", "when", "where", "who", "how", "why", "which"]):
   |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
40 |         true_label = "needs_answer"
   |

E501 Line too long (118 > 100)
  --> scripts/create_true_gold_standard.py:51:101
   |
49 |             else:
50 |                 true_label = "needs_confirmation"  # Simple either/or
51 |         elif any(p in text_lower for p in ["can you", "could you", "will you", "would you", "shall we", "should we"]):
   |                                                                                                     ^^^^^^^^^^^^^^^^^^
52 |             true_label = "needs_confirmation"
53 |         elif text.strip() in ["Really?", "Seriously?", "Right?", "You sure?"]:
   |

E501 Line too long (153 > 100)
  --> scripts/create_true_gold_standard.py:66:101
   |
65 | â€¦
66 | â€¦ so excited", "i'm so happy", "i'm so stressed", "i'm so sad", "this sucks", "i hate this"]):
   |                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
67 | â€¦
68 | â€¦, "amazing news", "i'm sorry to hear", "my condolences"]):
   |

E501 Line too long (118 > 100)
  --> scripts/create_true_gold_standard.py:68:101
   |
66 |     elif any(w in text_lower for w in ["i got the job", "i'm so excited", "i'm so happy", "i'm so stressed", "i'm so sad", "this sucksâ€¦
67 |         true_label = "needs_empathy"
68 |     elif any(w in text_lower for w in ["congrat", "so proud", "amazing news", "i'm sorry to hear", "my condolences"]):
   |                                                                                                     ^^^^^^^^^^^^^^^^^^
69 |         true_label = "needs_empathy"
70 |     elif any(e in text_orig for e in ["ðŸ˜­", "ðŸ˜¢", "ðŸŽ‰", "ðŸ¥³", "â¤ï¸", "ðŸ’ª"]):
   |

E501 Line too long (130 > 100)
  --> scripts/create_true_gold_standard.py:92:101
   |
91 |     # Special overrides for common patterns
92 |     if text_lower.strip() in ["ok", "okay", "yeah", "yep", "sure", "alright", "thanks", "thank you", "no problem", "sounds good"]:
   |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
93 |         true_label = "conversational"
94 |     if text_lower.startswith(("i see", "i think", "i believe", "i guess", "me too", "same here")):
   |

E402 Module level import not at top of file
   --> scripts/create_true_gold_standard.py:130:1
    |
128 | accuracy = correct / len(messages)
129 |
130 | from collections import Counter
    | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
131 | true_dist = Counter(m["true_gold_label"] for m in messages)
132 | llm_dist = Counter(m["simple_label"] for m in messages)
    |

I001 [*] Import block is un-sorted or un-formatted
   --> scripts/create_true_gold_standard.py:130:1
    |
128 | accuracy = correct / len(messages)
129 |
130 | from collections import Counter
    | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
131 | true_dist = Counter(m["true_gold_label"] for m in messages)
132 | llm_dist = Counter(m["simple_label"] for m in messages)
    |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
 --> scripts/debug_lf_coverage.py:4:1
  |
2 | """Debug LF coverage - check why abstain rate is so high."""
3 |
4 | from scripts.labeling_functions import get_registry, ABSTAIN
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
5 |
6 | # Test examples
  |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/deploy_linearsvc.py:3:1
   |
 1 |   #!/usr/bin/env python3
 2 |   """Quick script to train and deploy LinearSVC category classifier."""
 3 | / import json
 4 | | import time
 5 | | from pathlib import Path
 6 | |
 7 | | import joblib
 8 | | import numpy as np
 9 | | from sklearn.svm import LinearSVC
10 | | from sklearn.metrics import classification_report
11 | | from sklearn.model_selection import GridSearchCV, train_test_split
12 | |
13 | | from jarvis.embeddings.bert_embedder import BERTEmbedder
14 | | from jarvis.classifiers.feature_extractor import FeatureExtractor
   | |_________________________________________________________________^
   |
help: Organize imports

F401 [*] `numpy` imported but unused
  --> scripts/deploy_linearsvc.py:8:17
   |
 7 | import joblib
 8 | import numpy as np
   |                 ^^
 9 | from sklearn.svm import LinearSVC
10 | from sklearn.metrics import classification_report
   |
help: Remove unused import: `numpy`

N806 Variable `X_train` in function should be lowercase
  --> scripts/deploy_linearsvc.py:56:5
   |
55 |     print(f"\nExtracting features for {len(train_examples)} training examples...", flush=True)
56 |     X_train, y_train = feature_extractor.extract_batch(train_examples)
   |     ^^^^^^^
57 |
58 |     print(f"\nExtracting features for {len(test_examples)} test examples...", flush=True)
   |

N806 Variable `X_test` in function should be lowercase
  --> scripts/deploy_linearsvc.py:59:5
   |
58 |     print(f"\nExtracting features for {len(test_examples)} test examples...", flush=True)
59 |     X_test, y_test = feature_extractor.extract_batch(test_examples)
   |     ^^^^^^
60 |
61 |     print(f"\nFeature matrix: {X_train.shape[0]} samples, {X_train.shape[1]} features", flush=True)
   |

F541 [*] f-string without any placeholders
   --> scripts/deploy_linearsvc.py:108:11
    |
106 |     joblib.dump(grid.best_estimator_, output_path)
107 |
108 |     print(f"\nâœ… DEPLOYMENT COMPLETE!", flush=True)
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
109 |     print(f"   Model: {output_path}", flush=True)
110 |     print(f"   Test F1 (macro): {grid.best_score_:.4f}", flush=True)
    |
help: Remove extraneous `f` prefix

I001 [*] Import block is un-sorted or un-formatted
 --> scripts/gen_ref_pages.py:3:1
  |
1 |   """Generate the code reference pages and navigation."""
2 |
3 | / from pathlib import Path
4 | | import mkdocs_gen_files
  | |_______________________^
5 |
6 |   nav = mkdocs_gen_files.Nav()
  |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/generate_preference_pairs.py:40:1
   |
38 |               os.environ.setdefault(key.strip(), val.strip())
39 |
40 | / from evals.judge_config import (  # noqa: E402
41 | |     JUDGE_BASE_URL,
42 | |     JUDGE_MODEL as GEMINI_MODEL,
43 | |     get_judge_api_key,
44 | | )
   | |_^
   |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/generate_synthetic_examples.py:11:1
   |
 9 |   """
10 |
11 | / from __future__ import annotations
12 | |
13 | | import random
14 | | from pathlib import Path
15 | | import sys
   | |__________^
16 |
17 |   PROJECT_ROOT = Path(__file__).parent.parent
   |
help: Organize imports

F401 [*] `re` imported but unused
 --> scripts/label_and_analyze.py:7:8
  |
6 | import json
7 | import re
  |        ^^
8 | from pathlib import Path
  |
help: Remove unused import: `re`

F841 Local variable `source` is assigned to but never used
   --> scripts/label_and_analyze.py:158:9
    |
156 |         text = msg["text"]
157 |         previous = msg["previous"]
158 |         source = msg["source"]
    |         ^^^^^^
159 |
160 |         category, reason, is_confident = label_message(text, previous)
    |
help: Remove assignment to unused variable `source`

F401 [*] `os` imported but unused
  --> scripts/label_production_messages.py:14:8
   |
12 | import argparse
13 | import json
14 | import os
   |        ^^
15 | import sys
16 | from pathlib import Path
   |
help: Remove unused import: `os`

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/label_production_messages.py:39:5
   |
37 |       # Import the validation script functions
38 |       sys.path.insert(0, str(PROJECT_ROOT / "scripts"))
39 | /     from validate_on_production import (
40 | |         sample_messages,
41 | |         extract_features_batch,
42 | |         get_llm_labels,
43 | |     )
   | |_____^
44 |
45 |       # Load model
   |
help: Organize imports

F541 [*] f-string without any placeholders
   --> scripts/label_production_messages.py:116:11
    |
114 |         print(f"  {i}. {label}{marker}")
115 |     print()
116 |     print(f"  s. Skip this message")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^
117 |     print(f"  q. Quit and save progress")
118 |     print()
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/label_production_messages.py:117:11
    |
115 |     print()
116 |     print(f"  s. Skip this message")
117 |     print(f"  q. Quit and save progress")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
118 |     print()
    |
help: Remove extraneous `f` prefix

E501 Line too long (108 > 100)
   --> scripts/label_production_messages.py:224:101
    |
222 |     print("LightGBM Per-Class Performance (Human as ground truth)")
223 |     print("=" * 70)
224 |     print(classification_report(human, lgbm, labels=labels, target_names=labels, digits=3, zero_division=0))
    |                                                                                                     ^^^^^^^^
225 |
226 |     print("=" * 70)
    |

E501 Line too long (107 > 100)
   --> scripts/label_production_messages.py:229:101
    |
227 |     print("LLM Per-Class Performance (Human as ground truth)")
228 |     print("=" * 70)
229 |     print(classification_report(human, llm, labels=labels, target_names=labels, digits=3, zero_division=0))
    |                                                                                                     ^^^^^^^
230 |
231 |     # Winner
    |

F541 [*] f-string without any placeholders
   --> scripts/label_production_messages.py:240:15
    |
238 |         print(f"âœ… LightGBM WINS ({lgbm_rate:.1%} vs {llm_rate:.1%})")
239 |         print(f"   LightGBM is {lgbm_rate - llm_rate:.1%} more accurate than LLM on your messages")
240 |         print(f"   â†’ Use LightGBM for production")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
241 |     elif llm_rate > lgbm_rate + 0.05:
242 |         print(f"âœ… LLM WINS ({llm_rate:.1%} vs {lgbm_rate:.1%})")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/label_production_messages.py:244:15
    |
242 |         print(f"âœ… LLM WINS ({llm_rate:.1%} vs {lgbm_rate:.1%})")
243 |         print(f"   LLM is {llm_rate - lgbm_rate:.1%} more accurate than LightGBM on your messages")
244 |         print(f"   â†’ Need to retrain LightGBM on iMessage-like data")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
245 |     else:
246 |         print(f"ðŸ¤ TIE ({lgbm_rate:.1%} vs {llm_rate:.1%})")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/label_production_messages.py:247:15
    |
245 |     else:
246 |         print(f"ðŸ¤ TIE ({lgbm_rate:.1%} vs {llm_rate:.1%})")
247 |         print(f"   Both models perform similarly on your messages")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
248 |         print(f"   â†’ Either model is acceptable for production")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/label_production_messages.py:248:15
    |
246 |         print(f"ðŸ¤ TIE ({lgbm_rate:.1%} vs {llm_rate:.1%})")
247 |         print(f"   Both models perform similarly on your messages")
248 |         print(f"   â†’ Either model is acceptable for production")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
249 |
250 |     print()
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/label_production_messages.py:323:23
    |
321 |                     "next_idx": idx,
322 |                 })
323 |                 print(f"âœ“ Progress saved. Resume with: --resume flag")
    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
324 |                 print(f"   Labeled: {sum(1 for h in human_labels if h is not None)}/{len(messages)}")
325 |                 sys.exit(0)
    |
help: Remove extraneous `f` prefix

E501 Line too long (101 > 100)
   --> scripts/label_production_messages.py:324:101
    |
322 |                 })
323 |                 print(f"âœ“ Progress saved. Resume with: --resume flag")
324 |                 print(f"   Labeled: {sum(1 for h in human_labels if h is not None)}/{len(messages)}")
    |                                                                                                     ^
325 |                 sys.exit(0)
326 |             elif user_label == "skip":
    |

F541 [*] f-string without any placeholders
   --> scripts/label_production_messages.py:356:15
    |
354 |             "next_idx": idx,
355 |         })
356 |         print(f"âœ“ Progress saved. Resume with: --resume flag")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
357 |         sys.exit(0)
    |
help: Remove extraneous `f` prefix

F401 [*] `os` imported but unused
  --> scripts/label_soc_categories.py:27:8
   |
25 | import json
26 | import logging
27 | import os
   |        ^^
28 | import re
29 | import sys
   |
help: Remove unused import: `os`

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/label_soc_categories.py:42:1
   |
40 | logger = logging.getLogger(__name__)
41 |
42 | from evals.judge_config import JUDGE_BASE_URL, JUDGE_MODEL, get_judge_api_key  # noqa: E402
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
43 | MAX_WORKERS = 10  # Concurrent API requests
   |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/labeling_functions.py:22:1
   |
20 |   """
21 |
22 | / from __future__ import annotations
23 | |
24 | | import re
25 | | from dataclasses import dataclass
26 | | from typing import Callable
27 | |
28 | | # Import existing heuristics
29 | | from jarvis.classifiers.response_mobilization import classify_response_pressure
30 | | from jarvis.text_normalizer import (
31 | |     EMOJI_PATTERN,
32 | |     extract_temporal_refs,
33 | |     is_acknowledgment_only,
34 | |     is_emoji_only,
35 | |     is_question,
36 | |     is_reaction,
37 | |     is_spam_message,
38 | |     trigger_expects_content,
39 | | )
40 | | from jarvis.classifiers.category_classifier import (
41 | |     DEICTIC_PATTERN,
42 | |     EMOTIONAL_PATTERN,
43 | | )
   | |_^
44 |
45 |   # Constants
   |
help: Organize imports

UP035 [*] Import from `collections.abc` instead: `Callable`
  --> scripts/labeling_functions.py:26:1
   |
24 | import re
25 | from dataclasses import dataclass
26 | from typing import Callable
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^
27 |
28 | # Import existing heuristics
   |
help: Import from `collections.abc`

F401 [*] `jarvis.text_normalizer.EMOJI_PATTERN` imported but unused
  --> scripts/labeling_functions.py:31:5
   |
29 | from jarvis.classifiers.response_mobilization import classify_response_pressure
30 | from jarvis.text_normalizer import (
31 |     EMOJI_PATTERN,
   |     ^^^^^^^^^^^^^
32 |     extract_temporal_refs,
33 |     is_acknowledgment_only,
   |
help: Remove unused import: `jarvis.text_normalizer.EMOJI_PATTERN`

E501 Line too long (102 > 100)
   --> scripts/labeling_functions.py:123:101
    |
122 | @_registry.register("lf_acknowledgment", weight=1.5)
123 | def lf_acknowledgment(text: str, context: list[str], last_message: str, metadata: dict | None) -> str:
    |                                                                                                     ^^
124 |     """Generic acknowledgments -> ack."""
125 |     return "ack" if is_acknowledgment_only(text) else ABSTAIN
    |

E501 Line too long (101 > 100)
   --> scripts/labeling_functions.py:164:101
    |
163 | @_registry.register("lf_temporal_refs", weight=0.9)
164 | def lf_temporal_refs(text: str, context: list[str], last_message: str, metadata: dict | None) -> str:
    |                                                                                                     ^
165 |     """Messages with temporal references -> info (likely scheduling/logistics)."""
166 |     refs = extract_temporal_refs(text)
    |

E501 Line too long (103 > 100)
   --> scripts/labeling_functions.py:171:101
    |
170 | @_registry.register("lf_expects_content", weight=1.0)
171 | def lf_expects_content(text: str, context: list[str], last_message: str, metadata: dict | None) -> str:
    |                                                                                                     ^^^
172 |     """Last message expects content-rich response -> info."""
173 |     if not last_message:
    |

E501 Line too long (103 > 100)
   --> scripts/labeling_functions.py:182:101
    |
181 | @_registry.register("lf_mob_high_answer", weight=1.3)
182 | def lf_mob_high_answer(text: str, context: list[str], last_message: str, metadata: dict | None) -> str:
    |                                                                                                     ^^^
183 |     """HIGH pressure + ANSWER type -> info."""
184 |     result = classify_response_pressure(text)
    |

E501 Line too long (107 > 100)
   --> scripts/labeling_functions.py:191:101
    |
190 | @_registry.register("lf_mob_high_commitment", weight=1.3)
191 | def lf_mob_high_commitment(text: str, context: list[str], last_message: str, metadata: dict | None) -> str:
    |                                                                                                     ^^^^^^^
192 |     """HIGH pressure + COMMITMENT type -> info."""
193 |     result = classify_response_pressure(text)
    |

E501 Line too long (108 > 100)
   --> scripts/labeling_functions.py:200:101
    |
199 | @_registry.register("lf_mob_medium_emotional", weight=1.2)
200 | def lf_mob_medium_emotional(text: str, context: list[str], last_message: str, metadata: dict | None) -> str:
    |                                                                                                     ^^^^^^^^
201 |     """MEDIUM pressure + EMOTIONAL type -> emotional."""
202 |     result = classify_response_pressure(text)
    |

E501 Line too long (104 > 100)
   --> scripts/labeling_functions.py:209:101
    |
208 | @_registry.register("lf_mob_none_closing", weight=1.0)
209 | def lf_mob_none_closing(text: str, context: list[str], last_message: str, metadata: dict | None) -> str:
    |                                                                                                     ^^^^
210 |     """NONE pressure + CLOSING type -> ack."""
211 |     result = classify_response_pressure(text)
    |

E501 Line too long (103 > 100)
   --> scripts/labeling_functions.py:218:101
    |
217 | @_registry.register("lf_mob_backchannel", weight=1.0)
218 | def lf_mob_backchannel(text: str, context: list[str], last_message: str, metadata: dict | None) -> str:
    |                                                                                                     ^^^
219 |     """Backchannel feature from mobilization -> ack."""
220 |     result = classify_response_pressure(text)
    |

E501 Line too long (102 > 100)
   --> scripts/labeling_functions.py:246:101
    |
245 | @_registry.register("lf_mob_imperative", weight=1.0)
246 | def lf_mob_imperative(text: str, context: list[str], last_message: str, metadata: dict | None) -> str:
    |                                                                                                     ^^
247 |     """Imperative feature from mobilization -> info."""
248 |     result = classify_response_pressure(text)
    |

E501 Line too long (104 > 100)
   --> scripts/labeling_functions.py:270:101
    |
269 | @_registry.register("lf_bare_punctuation", weight=1.5)
270 | def lf_bare_punctuation(text: str, context: list[str], last_message: str, metadata: dict | None) -> str:
    |                                                                                                     ^^^^
271 |     """Bare punctuation (?, !!, ...) -> clarify."""
272 |     if re.match(r"^[?!.]{1,3}$", text.strip()):
    |

E501 Line too long (108 > 100)
   --> scripts/labeling_functions.py:284:101
    |
283 | @_registry.register("lf_deictic_thin_context", weight=1.0)
284 | def lf_deictic_thin_context(text: str, context: list[str], last_message: str, metadata: dict | None) -> str:
    |                                                                                                     ^^^^^^^^
285 |     """Deictic pronouns + thin context -> clarify."""
286 |     if DEICTIC_PATTERN.search(text) and len(context) < 2:
    |

E501 Line too long (103 > 100)
   --> scripts/labeling_functions.py:292:101
    |
291 | @_registry.register("lf_clarify_signals", weight=1.1)
292 | def lf_clarify_signals(text: str, context: list[str], last_message: str, metadata: dict | None) -> str:
    |                                                                                                     ^^^
293 |     """Multiple clarify signals -> clarify."""
294 |     # Import the detect function from category_classifier
    |

E501 Line too long (106 > 100)
   --> scripts/labeling_functions.py:305:101
    |
304 | @_registry.register("lf_emotional_distress", weight=1.4)
305 | def lf_emotional_distress(text: str, context: list[str], last_message: str, metadata: dict | None) -> str:
    |                                                                                                     ^^^^^^
306 |     """Emotional distress patterns -> emotional."""
307 |     return "emotional" if EMOTIONAL_PATTERN.search(text) else ABSTAIN
    |

E501 Line too long (108 > 100)
   --> scripts/labeling_functions.py:311:101
    |
310 | @_registry.register("lf_multiple_exclamation", weight=0.9)
311 | def lf_multiple_exclamation(text: str, context: list[str], last_message: str, metadata: dict | None) -> str:
    |                                                                                                     ^^^^^^^^
312 |     """3+ exclamation marks -> emotional."""
313 |     return "emotional" if text.count("!") >= 3 else ABSTAIN
    |

E501 Line too long (101 > 100)
   --> scripts/labeling_functions.py:328:101
    |
327 | @_registry.register("lf_dd_commissive", weight=0.7)
328 | def lf_dd_commissive(text: str, context: list[str], last_message: str, metadata: dict | None) -> str:
    |                                                                                                     ^
329 |     """DailyDialog act=4 (commissive) -> info."""
330 |     if metadata and metadata.get("act") == 4:
    |

E501 Line too long (107 > 100)
   --> scripts/labeling_functions.py:336:101
    |
335 | @_registry.register("lf_dd_negative_emotion", weight=0.6)
336 | def lf_dd_negative_emotion(text: str, context: list[str], last_message: str, metadata: dict | None) -> str:
    |                                                                                                     ^^^^^^^
337 |     """DailyDialog negative emotions (anger=1, disgust=2, fear=3, sadness=5) -> emotional."""
338 |     if metadata and metadata.get("emotion") in {1, 2, 3, 5}:
    |

E501 Line too long (107 > 100)
   --> scripts/labeling_functions.py:344:101
    |
343 | @_registry.register("lf_dd_positive_emotion", weight=0.6)
344 | def lf_dd_positive_emotion(text: str, context: list[str], last_message: str, metadata: dict | None) -> str:
    |                                                                                                     ^^^^^^^
345 |     """DailyDialog positive emotions (happiness=4, surprise=6) -> emotional."""
346 |     if metadata and metadata.get("emotion") in {4, 6}:
    |

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/llm_category_labeler.py:15:1
   |
13 |   """
14 |
15 | / from __future__ import annotations
16 | |
17 | | import argparse
18 | | import json
19 | | import sys
20 | | import time
21 | | from pathlib import Path
22 | | from collections import Counter
   | |_______________________________^
23 |
24 |   PROJECT_ROOT = Path(__file__).parent.parent
   |
help: Organize imports

E402 Module level import not at top of file
  --> scripts/llm_category_labeler.py:27:1
   |
25 | sys.path.insert(0, str(PROJECT_ROOT))
26 |
27 | from evals.judge_config import get_judge_client, JUDGE_BASE_URL, JUDGE_MODEL
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
28 |
29 | VALID_CATEGORIES = ["closing", "acknowledge", "question", "request", "emotion", "statement"]
   |

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/llm_category_labeler.py:27:1
   |
25 | sys.path.insert(0, str(PROJECT_ROOT))
26 |
27 | from evals.judge_config import get_judge_client, JUDGE_BASE_URL, JUDGE_MODEL
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
28 |
29 | VALID_CATEGORIES = ["closing", "acknowledge", "question", "request", "emotion", "statement"]
   |
help: Organize imports

E501 Line too long (104 > 100)
  --> scripts/llm_category_labeler.py:34:101
   |
33 | 1. closing: Says goodbye ("bye", "ttyl", "see you later", "gotta go")
34 | 2. acknowledge: â‰¤5 words AND expresses agreement/thanks ("ok", "yeah", "thanks", "sure", "here you are")
   |                                                                                                     ^^^^
35 | 3. request: Asks for action ("can you", "could you", "would you", "please" + verb, "let's", "I'd like")
36 | 4. question: Has "?" OR starts with question word ("what", "when", "where", "who", "why", "how", "is", "are", "do")
   |

E501 Line too long (103 > 100)
  --> scripts/llm_category_labeler.py:35:101
   |
33 | 1. closing: Says goodbye ("bye", "ttyl", "see you later", "gotta go")
34 | 2. acknowledge: â‰¤5 words AND expresses agreement/thanks ("ok", "yeah", "thanks", "sure", "here you are")
35 | 3. request: Asks for action ("can you", "could you", "would you", "please" + verb, "let's", "I'd like")
   |                                                                                                     ^^^
36 | 4. question: Has "?" OR starts with question word ("what", "when", "where", "who", "why", "how", "is", "are", "do")
37 | 5. emotion: Expressing feelings - has emotion words ("happy", "sad", "love", "hate", "excited", "stressed", "wow") OR "!!" OR ALLCAPS â€¦
   |

E501 Line too long (115 > 100)
  --> scripts/llm_category_labeler.py:36:101
   |
34 | 2. acknowledge: â‰¤5 words AND expresses agreement/thanks ("ok", "yeah", "thanks", "sure", "here you are")
35 | 3. request: Asks for action ("can you", "could you", "would you", "please" + verb, "let's", "I'd like")
36 | 4. question: Has "?" OR starts with question word ("what", "when", "where", "who", "why", "how", "is", "are", "do")
   |                                                                                                     ^^^^^^^^^^^^^^^
37 | 5. emotion: Expressing feelings - has emotion words ("happy", "sad", "love", "hate", "excited", "stressed", "wow") OR "!!" OR ALLCAPS â€¦
38 |    Note: "happy birthday" and "happy new year" are greetings (statement), not genuine emotions
   |

E501 Line too long (148 > 100)
  --> scripts/llm_category_labeler.py:37:101
   |
35 | â€¦d you", "please" + verb, "let's", "I'd like")
36 | â€¦ "when", "where", "who", "why", "how", "is", "are", "do")
37 | â€¦y", "sad", "love", "hate", "excited", "stressed", "wow") OR "!!" OR ALLCAPS words OR ðŸ˜‚ðŸ˜­â¤ï¸
   |                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
38 | â€¦gs (statement), not genuine emotions
39 | â€¦ault - includes "I'm happy to hear that")
   |

E501 Line too long (112 > 100)
  --> scripts/llm_category_labeler.py:67:101
   |
66 |     # request
67 |     request_patterns = ["can you", "could you", "would you", "please", "i suggest", "let's", "lets", "i'd like"]
   |                                                                                                     ^^^^^^^^^^^^
68 |     if any(p in text_lower for p in request_patterns):
69 |         return "request"
   |

F541 [*] f-string without any placeholders
   --> scripts/llm_category_labeler.py:211:11
    |
210 |     dist = Counter(ex["heuristic_label"] for ex in ambiguous)
211 |     print(f"  Heuristic label distribution (pre-screening):", flush=True)
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
212 |     for cat, count in sorted(dist.items(), key=lambda x: -x[1]):
213 |         pct = count / len(ambiguous) * 100
    |
help: Remove extraneous `f` prefix

E501 Line too long (110 > 100)
   --> scripts/llm_category_labeler.py:244:101
    |
242 |         messages_block = "\n\n".join([
243 |             f"Message {j + 1}:\n"
244 |             f"Previous: \"{ex['last_message'][:100] if ex['last_message'] else '(start of conversation)'}\"\n"
    |                                                                                                     ^^^^^^^^^^
245 |             f"Current: \"{ex['text']}\""
246 |             for j, ex in enumerate(batch)
    |

E501 Line too long (123 > 100)
   --> scripts/llm_category_labeler.py:259:101
    |
257 |                 model=model,  # Use model from args (llama-3.3-70b)
258 |                 messages=[
259 |                     {"role": "system", "content": "You are a text classifier. Output only category labels, nothing else."},
    |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^
260 |                     {"role": "user", "content": prompt}
261 |                 ],
    |

F541 [*] f-string without any placeholders
   --> scripts/llm_category_labeler.py:268:23
    |
266 |             # Check if response is valid
267 |             if not response or not response.choices:
268 |                 print(f"    ERROR: Empty response from API", flush=True)
    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
269 |                 predictions.extend([ex["heuristic_label"] for ex in batch])
270 |                 continue
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/llm_category_labeler.py:276:23
    |
274 |             content = message.content or message.reasoning
275 |             if content is None:
276 |                 print(f"    ERROR: API returned None", flush=True)
    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
277 |                 predictions.extend([ex["heuristic_label"] for ex in batch])
278 |                 continue
    |
help: Remove extraneous `f` prefix

E501 Line too long (146 > 100)
   --> scripts/llm_category_labeler.py:296:101
    |
294 | â€¦.split("\n") if line.strip()]
295 | â€¦
296 | â€¦|Message\s+\d+[\.:]\s*|\*\*Result:?\s*|-\s*Category:\s*)', '', line, flags=re.IGNORECASE)
    |                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
297 | â€¦('*').strip().lower()
298 | â€¦
    |

F541 [*] f-string without any placeholders
   --> scripts/llm_category_labeler.py:397:11
    |
396 |     print("=" * 60)
397 |     print(f"LLM Category Labeling")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^
398 |     print("=" * 60)
399 |     print(f"Model: {args.model}")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/llm_category_labeler.py:445:11
    |
443 |     # Show distribution
444 |     dist = Counter(predictions)
445 |     print(f"\nLLM label distribution:")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
446 |     for cat, count in sorted(dist.items(), key=lambda x: -x[1]):
447 |         pct = count / len(predictions) * 100
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/llm_category_labeler.py:451:11
    |
450 |     # Compare to heuristic pre-screening
451 |     print(f"\nComparison to heuristic pre-screening:")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
452 |     agreement = sum(1 for ex, pred in zip(examples, predictions) if ex["heuristic_label"] == pred)
453 |     agreement_pct = agreement / len(examples) * 100
    |
help: Remove extraneous `f` prefix

E402 Module level import not at top of file
  --> scripts/manual_label_test.py:17:1
   |
15 | sys.path.insert(0, str(PROJECT_ROOT))
16 |
17 | from datasets import load_dataset
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |

F541 [*] f-string without any placeholders
  --> scripts/manual_label_test.py:22:11
   |
20 | def sample_samsum(n: int = 100, seed: int = 42) -> list[dict]:
21 |     """Sample n turns from SAMSum conversations."""
22 |     print(f"Loading SAMSum...")
   |           ^^^^^^^^^^^^^^^^^^^^
23 |     ds = load_dataset("knkarthick/samsum", split="train")
   |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
  --> scripts/manual_label_test.py:57:11
   |
55 | def sample_dailydialog(n: int = 50, seed: int = 42) -> list[dict]:
56 |     """Sample n utterances from DailyDialog."""
57 |     print(f"Loading DailyDialog...")
   |           ^^^^^^^^^^^^^^^^^^^^^^^^^
58 |     ds = load_dataset("OpenRL/daily_dialog", split="train")
   |
help: Remove extraneous `f` prefix

E501 Line too long (105 > 100)
   --> scripts/manual_label_test.py:101:101
    |
100 | 1. needs_answer - Expects factual info (has "?" + wh-word: what/when/where/who/how/why)
101 | 2. needs_confirmation - Expects yes/no/acknowledgment (imperative OR "can you/could you/will you/please")
    |                                                                                                     ^^^^^
102 | 3. needs_empathy - Needs emotional support (negative: sad/stressed/died/failed OR very positive: promoted/excited OR emojis ðŸ˜­ðŸ˜ŠðŸŽ‰)
103 | 4. conversational - Casual engagement (default, everything else)
    |

E501 Line too long (131 > 100)
   --> scripts/manual_label_test.py:102:101
    |
100 | â€¦wh-word: what/when/where/who/how/why)
101 | â€¦ent (imperative OR "can you/could you/will you/please")
102 | â€¦ve: sad/stressed/died/failed OR very positive: promoted/excited OR emojis ðŸ˜­ðŸ˜ŠðŸŽ‰)
    |                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
103 | â€¦erything else)
    |

E501 Line too long (109 > 100)
  --> scripts/manual_review_all_150.py:33:101
   |
32 |     # needs_answer: asking for factual info
33 |     if "?" in text and any(w in text_lower for w in ["what", "when", "where", "who", "how", "why", "which"]):
   |                                                                                                     ^^^^^^^^^
34 |         correct_label = "needs_answer"
35 |     # needs_confirmation: yes/no questions or requests
   |

E501 Line too long (106 > 100)
  --> scripts/manual_review_all_150.py:36:101
   |
34 |         correct_label = "needs_answer"
35 |     # needs_confirmation: yes/no questions or requests
36 |     elif "?" in text and not any(w in text_lower for w in ["what", "when", "where", "who", "how", "why"]):
   |                                                                                                     ^^^^^^
37 |         correct_label = "needs_confirmation"
38 |     # Explicit requests
   |

E501 Line too long (111 > 100)
  --> scripts/manual_review_all_150.py:39:101
   |
37 |         correct_label = "needs_confirmation"
38 |     # Explicit requests
39 |     elif any(p in text_lower for p in ["can you", "could you", "will you", "would you", "shall we", "please"]):
   |                                                                                                     ^^^^^^^^^^^
40 |         correct_label = "needs_confirmation"
41 |     # needs_empathy: emotional content
   |

E501 Line too long (114 > 100)
  --> scripts/manual_review_all_150.py:42:101
   |
40 |         correct_label = "needs_confirmation"
41 |     # needs_empathy: emotional content
42 |     elif any(w in text_lower for w in ["sorry", "hope", "congrat", "excited", "sad", "hate", "love", ":(", ":)"]):
   |                                                                                                     ^^^^^^^^^^^^^^
43 |         # Check context
44 |         if "sorry about" in text_lower or "hope" in text_lower:
   |

E501 Line too long (142 > 100)
   --> scripts/manual_review_all_150.py:126:101
    |
124 | â€¦ == "correct")
125 | â€¦
126 | â€¦heuristic_reviews)} ({heuristic_correct / max(len(heuristic_reviews), 1) * 100:.1f}%)")
    |                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
127 | â€¦({llm_correct / max(len(llm_reviews), 1) * 100:.1f}%)")
    |

E501 Line too long (110 > 100)
   --> scripts/manual_review_all_150.py:127:101
    |
126 | print(f"\nHeuristic accuracy: {heuristic_correct}/{len(heuristic_reviews)} ({heuristic_correct / max(len(heuristic_reviews), 1) * 100â€¦
127 | print(f"LLM accuracy: {llm_correct}/{len(llm_reviews)} ({llm_correct / max(len(llm_reviews), 1) * 100:.1f}%)")
    |                                                                                                     ^^^^^^^^^^
128 |
129 | # Show errors
    |

F401 [*] `joblib` imported but unused
  --> scripts/model_comparison.py:16:8
   |
14 | from pathlib import Path
15 |
16 | import joblib
   |        ^^^^^^
17 | import numpy as np
18 | from sklearn.compose import ColumnTransformer
   |
help: Remove unused import: `joblib`

E501 Line too long (101 > 100)
  --> scripts/model_comparison.py:59:101
   |
57 |     print(f"âœ“ Test:  {len(X_test):,} samples", flush=True)
58 |     print(f"âœ“ Features: {X_train.shape[1]} ({metadata['embedding_dims']} emb + "
59 |           f"{metadata['hand_crafted_dims']} hc + {metadata.get('spacy_dims', 0)} spacy)", flush=True)
   |                                                                                                     ^
60 |     print(f"âœ“ Classes: {list(label_encoder.classes_)} â†’ {list(range(len(label_encoder.classes_)))}", flush=True)
61 |     print(flush=True)
   |

E501 Line too long (112 > 100)
  --> scripts/model_comparison.py:60:101
   |
58 |     print(f"âœ“ Features: {X_train.shape[1]} ({metadata['embedding_dims']} emb + "
59 |           f"{metadata['hand_crafted_dims']} hc + {metadata.get('spacy_dims', 0)} spacy)", flush=True)
60 |     print(f"âœ“ Classes: {list(label_encoder.classes_)} â†’ {list(range(len(label_encoder.classes_)))}", flush=True)
   |                                                                                                     ^^^^^^^^^^^^
61 |     print(flush=True)
   |

E402 Module level import not at top of file
  --> scripts/monitor_training.py:13:1
   |
11 | sys.path.insert(0, str(PROJECT_ROOT))
12 |
13 | from jarvis.utils.memory import get_memory_info, get_swap_info
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |

F401 [*] `jarvis.utils.memory.get_memory_info` imported but unused
  --> scripts/monitor_training.py:13:33
   |
11 | sys.path.insert(0, str(PROJECT_ROOT))
12 |
13 | from jarvis.utils.memory import get_memory_info, get_swap_info
   |                                 ^^^^^^^^^^^^^^^
   |
help: Remove unused import: `jarvis.utils.memory.get_memory_info`

E501 Line too long (109 > 100)
  --> scripts/monitor_training.py:50:101
   |
49 |             print(
50 |                 f"{int(elapsed):>5d}s  {cpu:>5.1f}%  {ram_mb:>8.1f}  {swap['used_mb']:>9.1f}  {status:>10s}",
   |                                                                                                     ^^^^^^^^^
51 |                 flush=True,
52 |             )
   |

E501 Line too long (102 > 100)
   --> scripts/ner_server.py:108:101
    |
107 |     Features:
108 |     - Directive indicators (5): imperative, you+modal, request verbs, starts_modal, directive_question
    |                                                                                                     ^^
109 |     - Commissive indicators (4): i_will, promise_verb, first_person_count, agreement
110 |     - General syntactic (5): modal_count, verb_count, second_person_count, has_negation, is_interrogative
    |

E501 Line too long (105 > 100)
   --> scripts/ner_server.py:110:101
    |
108 |     - Directive indicators (5): imperative, you+modal, request verbs, starts_modal, directive_question
109 |     - Commissive indicators (4): i_will, promise_verb, first_person_count, agreement
110 |     - General syntactic (5): modal_count, verb_count, second_person_count, has_negation, is_interrogative
    |                                                                                                     ^^^^^
111 |
112 |     Returns:
    |

E501 Line too long (106 > 100)
   --> scripts/ner_server.py:128:101
    |
127 |     # 2. "you/u" + modal pattern
128 |     features.append(1.0 if re.search(r"\b(you|u)\s+(can|could|should|will|would)\b", text, re.I) else 0.0)
    |                                                                                                     ^^^^^^
129 |
130 |     # 3. Request verbs
    |

E501 Line too long (111 > 100)
   --> scripts/ner_server.py:139:101
    |
138 |     # 5. Directive question (ends with ? and has you/your)
139 |     features.append(1.0 if text.endswith("?") and any(t.text.lower() in {"you", "your"} for t in doc) else 0.0)
    |                                                                                                     ^^^^^^^^^^^
140 |
141 |     # Commissive indicators (4)
    |

E501 Line too long (110 > 100)
   --> scripts/ner_server.py:202:101
    |
200 |         # Directive indicators (5)
201 |         features.append(1.0 if doc and doc[0].tag_ == "VB" else 0.0)
202 |         features.append(1.0 if re.search(r"\b(you|u)\s+(can|could|should|will|would)\b", text, re.I) else 0.0)
    |                                                                                                     ^^^^^^^^^^
203 |         features.append(1.0 if any(tok.lemma_ in request_verbs for tok in doc) else 0.0)
204 |         features.append(1.0 if doc and doc[0].lemma_ in modal_verbs else 0.0)
    |

E501 Line too long (115 > 100)
   --> scripts/ner_server.py:205:101
    |
203 |         features.append(1.0 if any(tok.lemma_ in request_verbs for tok in doc) else 0.0)
204 |         features.append(1.0 if doc and doc[0].lemma_ in modal_verbs else 0.0)
205 |         features.append(1.0 if text.endswith("?") and any(t.text.lower() in {"you", "your"} for t in doc) else 0.0)
    |                                                                                                     ^^^^^^^^^^^^^^^
206 |
207 |         # Commissive indicators (4)
    |

E402 Module level import not at top of file
  --> scripts/optuna_search.py:41:1
   |
39 | sys.path.insert(0, str(PROJECT_ROOT))
40 |
41 | from jarvis.utils.memory import get_memory_info, get_swap_info
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
42 |
43 | # Setup logging
   |

F401 [*] `jarvis.utils.memory.get_memory_info` imported but unused
  --> scripts/optuna_search.py:41:33
   |
39 | sys.path.insert(0, str(PROJECT_ROOT))
40 |
41 | from jarvis.utils.memory import get_memory_info, get_swap_info
   |                                 ^^^^^^^^^^^^^^^
42 |
43 | # Setup logging
   |
help: Remove unused import: `jarvis.utils.memory.get_memory_info`

N803 Argument name `X` should be lowercase
  --> scripts/optuna_search.py:56:22
   |
56 | def apply_resampling(X, y, resample_map, seed=42):
   |                      ^
57 |     """Resample classes: oversample (factor>1) or downsample (factor<1)."""
58 |     if not resample_map:
   |

N806 Variable `X_list` in function should be lowercase
  --> scripts/optuna_search.py:61:5
   |
59 |         return X, y
60 |
61 |     X_list = []
   |     ^^^^^^
62 |     y_list = []
   |

N806 Variable `label_X` in function should be lowercase
  --> scripts/optuna_search.py:66:9
   |
64 |     for label in sorted(set(y)):
65 |         mask = y == label
66 |         label_X = X[mask]
   |         ^^^^^^^
67 |         label_y = y[mask]
68 |         n_original = len(label_y)
   |

N806 Variable `X_resampled` in function should be lowercase
  --> scripts/optuna_search.py:93:5
   |
91 |             y_list.append(label_y)
92 |
93 |     X_resampled = np.vstack(X_list)
   |     ^^^^^^^^^^^
94 |     y_resampled = np.concatenate(y_list)
   |

N803 Argument name `X_train` should be lowercase
   --> scripts/optuna_search.py:106:24
    |
104 |     """Optuna objective function for SVM hyperparameter optimization."""
105 |
106 |     def __init__(self, X_train, y_train, metadata, seed=42, n_jobs=1):
    |                        ^^^^^^^
107 |         self.X_train = X_train
108 |         self.y_train = y_train
    |

N806 Variable `C` in function should be lowercase
   --> scripts/optuna_search.py:156:9
    |
154 |         # Sample hyperparameters
155 |         # SVM hyperparameters
156 |         C = trial.suggest_float("C", 0.1, 100.0, log=True)
    |         ^
157 |
158 |         # Gamma: either 'scale' or numeric value (constrained to reasonable range)
    |

F541 [*] f-string without any placeholders
   --> scripts/optuna_search.py:184:21
    |
182 |         logger.info(f"  C: {C:.4f}")
183 |         logger.info(f"  gamma: {gamma if gamma_type == 'scale' else f'{gamma:.6f}'}")
184 |         logger.info(f"  Resampling:")
    |                     ^^^^^^^^^^^^^^^^
185 |         for label, ratio in sorted(resample_map.items()):
186 |             logger.info(f"    {label:12s}: {ratio:.2f}x")
    |
help: Remove extraneous `f` prefix

N806 Variable `X_resampled` in function should be lowercase
   --> scripts/optuna_search.py:189:9
    |
188 |         # Apply resampling to training data
189 |         X_resampled, y_resampled = apply_resampling(
    |         ^^^^^^^^^^^
190 |             self.X_train, self.y_train, resample_map, self.seed
191 |         )
    |

N806 Variable `X_train` in function should be lowercase
   --> scripts/optuna_search.py:290:5
    |
288 |     metadata = json.loads((args.data_dir / "metadata.json").read_text())
289 |
290 |     X_train, y_train = train_data["X"], train_data["y"]
    |     ^^^^^^^
291 |     X_test, y_test = test_data["X"], test_data["y"]
    |

N806 Variable `X_test` in function should be lowercase
   --> scripts/optuna_search.py:291:5
    |
290 |     X_train, y_train = train_data["X"], train_data["y"]
291 |     X_test, y_test = test_data["X"], test_data["y"]
    |     ^^^^^^
292 |
293 |     print(f"Train: {len(X_train):,} samples")
    |

F541 [*] f-string without any placeholders
   --> scripts/optuna_search.py:342:11
    |
340 |     print(f"Trial number: {best_trial.number}")
341 |     print(f"F1 score: {best_trial.value:.4f}")
342 |     print(f"\nBest hyperparameters:")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^
343 |     for key, value in best_trial.params.items():
344 |         if key == "gamma_value" and best_trial.params.get("gamma_type") != "numeric":
    |
help: Remove extraneous `f` prefix

N806 Variable `best_C` in function should be lowercase
   --> scripts/optuna_search.py:354:5
    |
353 |     # Extract best params
354 |     best_C = best_trial.params["C"]
    |     ^^^^^^
355 |     if best_trial.params["gamma_type"] == "scale":
356 |         best_gamma = "scale"
    |

F541 [*] f-string without any placeholders
   --> scripts/optuna_search.py:367:11
    |
365 |     print(f"C: {best_C:.4f}")
366 |     print(f"gamma: {best_gamma if isinstance(best_gamma, str) else f'{best_gamma:.6f}'}")
367 |     print(f"Resampling:")
    |           ^^^^^^^^^^^^^^
368 |     for label, ratio in sorted(best_resample.items()):
369 |         print(f"  {label:12s}: {ratio:.2f}x")
    |
help: Remove extraneous `f` prefix

N806 Variable `X_train_resampled` in function should be lowercase
   --> scripts/optuna_search.py:372:5
    |
371 |     # Apply resampling
372 |     X_train_resampled, y_train_resampled = apply_resampling(
    |     ^^^^^^^^^^^^^^^^^
373 |         X_train, y_train, best_resample, args.seed
374 |     )
    |

invalid-syntax: Cannot use an escape sequence (backslash) in f-strings on Python 3.11 (syntax was added in Python 3.12)
   --> scripts/optuna_search.py:441:20
    |
440 |     # Print header
441 |     print(f"{'True \\ Pred':>15s}", end="")
    |                    ^
442 |     for label in labels:
443 |         print(f"{label[:10]:>12s}", end="")
    |

invalid-syntax: Cannot use an escape sequence (backslash) in f-strings on Python 3.11 (syntax was added in Python 3.12)
   --> scripts/optuna_search.py:441:21
    |
440 |     # Print header
441 |     print(f"{'True \\ Pred':>15s}", end="")
    |                     ^
442 |     for label in labels:
443 |         print(f"{label[:10]:>12s}", end="")
    |

E501 Line too long (109 > 100)
  --> scripts/prepare_category_data.py:22:101
   |
21 | Usage:
22 |     uv run python scripts/prepare_category_data.py --dry-run                           # preview distribution
   |                                                                                                     ^^^^^^^^^
23 |     uv run python scripts/prepare_category_data.py                                      # full run (majority vote)
24 |     uv run python scripts/prepare_category_data.py --method dawid_skene                # use Dawid-Skene EM
   |

E501 Line too long (114 > 100)
  --> scripts/prepare_category_data.py:23:101
   |
21 | Usage:
22 |     uv run python scripts/prepare_category_data.py --dry-run                           # preview distribution
23 |     uv run python scripts/prepare_category_data.py                                      # full run (majority vote)
   |                                                                                                     ^^^^^^^^^^^^^^
24 |     uv run python scripts/prepare_category_data.py --method dawid_skene                # use Dawid-Skene EM
25 |     uv run python scripts/prepare_category_data.py --min-confidence 0.5                # filter low-confidence
   |

E501 Line too long (107 > 100)
  --> scripts/prepare_category_data.py:24:101
   |
22 |     uv run python scripts/prepare_category_data.py --dry-run                           # preview distribution
23 |     uv run python scripts/prepare_category_data.py                                      # full run (majority vote)
24 |     uv run python scripts/prepare_category_data.py --method dawid_skene                # use Dawid-Skene EM
   |                                                                                                     ^^^^^^^
25 |     uv run python scripts/prepare_category_data.py --min-confidence 0.5                # filter low-confidence
26 |     uv run python scripts/prepare_category_data.py --llm-labels labels.jsonl          # override with LLM labels
   |

E501 Line too long (110 > 100)
  --> scripts/prepare_category_data.py:25:101
   |
23 |     uv run python scripts/prepare_category_data.py                                      # full run (majority vote)
24 |     uv run python scripts/prepare_category_data.py --method dawid_skene                # use Dawid-Skene EM
25 |     uv run python scripts/prepare_category_data.py --min-confidence 0.5                # filter low-confidence
   |                                                                                                     ^^^^^^^^^^
26 |     uv run python scripts/prepare_category_data.py --llm-labels labels.jsonl          # override with LLM labels
27 | """
   |

E501 Line too long (112 > 100)
  --> scripts/prepare_category_data.py:26:101
   |
24 |     uv run python scripts/prepare_category_data.py --method dawid_skene                # use Dawid-Skene EM
25 |     uv run python scripts/prepare_category_data.py --min-confidence 0.5                # filter low-confidence
26 |     uv run python scripts/prepare_category_data.py --llm-labels labels.jsonl          # override with LLM labels
   |                                                                                                     ^^^^^^^^^^^^
27 | """
   |

I001 [*] Import block is un-sorted or un-formatted
   --> scripts/prepare_category_data.py:272:5
    |
270 |       Returns dict with stats.
271 |       """
272 | /     from scripts.label_aggregation import aggregate_labels
273 | |     from scripts.labeling_functions import get_registry
274 | |
275 | |     from jarvis.classifiers.response_mobilization import classify_response_pressure
276 | |     from jarvis.embedding_adapter import get_embedder
    | |_____________________________________________________^
277 |
278 |       output_dir = OUTPUT_DIR
    |
help: Organize imports

E501 Line too long (104 > 100)
   --> scripts/prepare_category_data.py:288:101
    |
286 |     all_examples = dd_examples + samsum_examples
287 |
288 |     # Step 1b: Add synthetic examples for ack only (clarify/social synthetic removed - caused confusion)
    |                                                                                                     ^^^^
289 |     if add_synthetic:
290 |         from scripts.generate_synthetic_examples import generate_ack_examples
    |

E501 Line too long (101 > 100)
   --> scripts/prepare_category_data.py:305:101
    |
304 |     # Separate synthetic from real examples
305 |     synthetic_examples = [ex for ex in all_examples if ex.get("source", "").startswith("synthetic_")]
    |                                                                                                     ^
306 |     real_examples = [ex for ex in all_examples if not ex.get("source", "").startswith("synthetic_")]
    |

E501 Line too long (102 > 100)
   --> scripts/prepare_category_data.py:373:101
    |
372 |     # Per-source breakdown
373 |     for source in ("dailydialog", "samsum", "synthetic_ack", "synthetic_clarify", "synthetic_social"):
    |                                                                                                     ^^
374 |         src_examples = [ex for ex in all_examples if ex["source"] == source]
375 |         if not src_examples:
    |

N806 Variable `X` in function should be lowercase
   --> scripts/prepare_category_data.py:464:5
    |
463 |     # Step 6: Build feature matrix and save
464 |     X = np.hstack([embeddings, hc_matrix])
    |     ^
465 |     y = np.array([ex["label"] for ex in balanced])
    |

N806 Variable `X_train` in function should be lowercase
   --> scripts/prepare_category_data.py:471:5
    |
469 |     from sklearn.model_selection import train_test_split
470 |
471 |     X_train, X_test, y_train, y_test = train_test_split(
    |     ^^^^^^^
472 |         X, y, test_size=0.2, random_state=seed, stratify=y,
473 |     )
    |

N806 Variable `X_test` in function should be lowercase
   --> scripts/prepare_category_data.py:471:14
    |
469 |     from sklearn.model_selection import train_test_split
470 |
471 |     X_train, X_test, y_train, y_test = train_test_split(
    |              ^^^^^^
472 |         X, y, test_size=0.2, random_state=seed, stratify=y,
473 |     )
    |

F811 [*] Redefinition of unused `get_embedder` from line 222
   --> scripts/prepare_dailydialog_data.py:371:42
    |
369 |     log_memory("BEFORE get_embedder()")
370 |
371 |     from jarvis.embedding_adapter import get_embedder
    |                                          ^^^^^^^^^^^^ `get_embedder` redefined here
372 |     embedder = get_embedder()
    |
   ::: scripts/prepare_dailydialog_data.py:222:42
    |
220 |     """
221 |     from jarvis.classifiers.response_mobilization import classify_response_pressure
222 |     from jarvis.embedding_adapter import get_embedder
    |                                          ------------ previous definition of `get_embedder` here
223 |
224 |     output_dir = OUTPUT_DIR
    |
help: Remove definition: `get_embedder`

F541 [*] f-string without any placeholders
   --> scripts/prepare_dailydialog_data.py:452:11
    |
451 |     # Combine features (384 + 19 + 14 = 417)
452 |     print(f"\nCombining features:")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^
453 |     print(f"  Embeddings: {embeddings.shape}")
454 |     print(f"  Hand-crafted: {hc_matrix.shape}")
    |
help: Remove extraneous `f` prefix

N806 Variable `X` in function should be lowercase
   --> scripts/prepare_dailydialog_data.py:456:5
    |
454 |     print(f"  Hand-crafted: {hc_matrix.shape}")
455 |     print(f"  SpaCy: {spacy_matrix.shape}")
456 |     X = np.hstack([embeddings, hc_matrix, spacy_matrix])
    |     ^
457 |     print(f"  Total: {X.shape}")
458 |     log_memory("After np.hstack (TRIPLE MEMORY USAGE)")
    |

N806 Variable `X_train` in function should be lowercase
   --> scripts/prepare_dailydialog_data.py:468:5
    |
466 |     from sklearn.model_selection import train_test_split
467 |
468 |     X_train, X_test, y_train, y_test = train_test_split(
    |     ^^^^^^^
469 |         X, y, test_size=0.2, random_state=seed, stratify=y,
470 |     )
    |

N806 Variable `X_test` in function should be lowercase
   --> scripts/prepare_dailydialog_data.py:468:14
    |
466 |     from sklearn.model_selection import train_test_split
467 |
468 |     X_train, X_test, y_train, y_test = train_test_split(
    |              ^^^^^^
469 |         X, y, test_size=0.2, random_state=seed, stratify=y,
470 |     )
    |

N806 Variable `X` in function should be lowercase
   --> scripts/prepare_dailydialog_data_cpu.py:270:5
    |
268 |     feature_dims = embedding_dims + hand_crafted_dims
269 |
270 |     X = np.hstack([embeddings, hc_matrix])
    |     ^
271 |     del embeddings, hc_matrix
272 |     log_memory("After combining features")
    |

N806 Variable `X_train` in function should be lowercase
   --> scripts/prepare_dailydialog_data_cpu.py:278:5
    |
276 |     from sklearn.model_selection import train_test_split
277 |
278 |     X_train, X_test, y_train, y_test = train_test_split(
    |     ^^^^^^^
279 |         X, y, test_size=0.2, random_state=seed, stratify=y,
280 |     )
    |

N806 Variable `X_test` in function should be lowercase
   --> scripts/prepare_dailydialog_data_cpu.py:278:14
    |
276 |     from sklearn.model_selection import train_test_split
277 |
278 |     X_train, X_test, y_train, y_test = train_test_split(
    |              ^^^^^^
279 |         X, y, test_size=0.2, random_state=seed, stratify=y,
280 |     )
    |

E402 Module level import not at top of file
  --> scripts/prepare_for_manual_labeling.py:11:1
   |
 9 | sys.path.insert(0, str(PROJECT_ROOT))
10 |
11 | from jarvis.text_normalizer import normalize_text
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 |
13 | # Load samples
   |

E501 Line too long (102 > 100)
  --> scripts/prepare_for_manual_labeling.py:22:101
   |
20 |     msg["text_original"] = msg["text"]
21 |     msg["text_normalized"] = normalize_text(msg["text"], expand_slang=True, spell_check=False)
22 |     msg["previous_normalized"] = normalize_text(msg["previous"], expand_slang=True, spell_check=False)
   |                                                                                                     ^^
23 |
24 | # Save for manual labeling
   |

F401 [*] `sklearn.metrics.classification_report` imported but unused
  --> scripts/quick_c_search.py:20:29
   |
18 | import numpy as np
19 | from sklearn.compose import ColumnTransformer
20 | from sklearn.metrics import classification_report
   |                             ^^^^^^^^^^^^^^^^^^^^^
21 | from sklearn.model_selection import StratifiedKFold, cross_val_score
22 | from sklearn.pipeline import Pipeline
   |
help: Remove unused import: `sklearn.metrics.classification_report`

E402 Module level import not at top of file
  --> scripts/quick_c_search.py:29:1
   |
27 | sys.path.insert(0, str(PROJECT_ROOT))
28 |
29 | from jarvis.utils.memory import get_swap_info, get_memory_info
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
30 |
31 | import psutil
   |

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/quick_c_search.py:29:1
   |
27 |   sys.path.insert(0, str(PROJECT_ROOT))
28 |
29 | / from jarvis.utils.memory import get_swap_info, get_memory_info
30 | |
31 | | import psutil
   | |_____________^
   |
help: Organize imports

E402 Module level import not at top of file
  --> scripts/quick_c_search.py:31:1
   |
29 | from jarvis.utils.memory import get_swap_info, get_memory_info
30 |
31 | import psutil
   | ^^^^^^^^^^^^^
   |

F541 [*] f-string without any placeholders
  --> scripts/quick_c_search.py:76:11
   |
74 |     args = parser.parse_args()
75 |
76 |     print(f"Quick C value search")
   |           ^^^^^^^^^^^^^^^^^^^^^^^
77 |     print(f"Subset size: {args.subset_size:,} samples")
78 |     print(f"C values to test: {args.c_values}")
   |
help: Remove extraneous `f` prefix

N806 Variable `X_full` in function should be lowercase
  --> scripts/quick_c_search.py:90:5
   |
88 |     metadata = json.loads((args.data_dir / "metadata.json").read_text())
89 |
90 |     X_full, y_full = train_data["X"], train_data["y"]
   |     ^^^^^^
91 |     embedding_dims = metadata["embedding_dims"]
92 |     hand_crafted_dims = metadata["hand_crafted_dims"]
   |

N806 Variable `X_subset` in function should be lowercase
   --> scripts/quick_c_search.py:100:5
    |
 98 |     from sklearn.model_selection import train_test_split
 99 |
100 |     X_subset, _, y_subset, _ = train_test_split(
    |     ^^^^^^^^
101 |         X_full,
102 |         y_full,
    |

N806 Variable `directive_X` in function should be lowercase
   --> scripts/quick_c_search.py:112:9
    |
110 |         print(f"\nOversampling directive by {args.oversample_directive}x...")
111 |         directive_mask = y_subset == "directive"
112 |         directive_X = X_subset[directive_mask]
    |         ^^^^^^^^^^^
113 |         directive_y = y_subset[directive_mask]
    |

N806 Variable `X_subset` in function should be lowercase
   --> scripts/quick_c_search.py:120:9
    |
118 |         # Randomly sample with replacement
119 |         indices = np.random.choice(len(directive_y), size=n_to_add, replace=True)
120 |         X_subset = np.vstack([X_subset, directive_X[indices]])
    |         ^^^^^^^^
121 |         y_subset = np.concatenate([y_subset, directive_y[indices]])
    |

E501 Line too long (109 > 100)
   --> scripts/quick_c_search.py:123:101
    |
121 |         y_subset = np.concatenate([y_subset, directive_y[indices]])
122 |
123 |         print(f"Added {n_to_add} directive samples ({n_directive_original} â†’ {len(directive_y) + n_to_add})")
    |                                                                                                     ^^^^^^^^^
124 |
125 |     # Show class distribution
    |

F541 [*] f-string without any placeholders
   --> scripts/quick_c_search.py:126:11
    |
125 |     # Show class distribution
126 |     print(f"\nSubset class distribution:")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
127 |     for label in sorted(set(y_subset)):
128 |         count = (y_subset == label).sum()
    |
help: Remove extraneous `f` prefix

E501 Line too long (108 > 100)
   --> scripts/quick_c_search.py:193:101
    |
191 |                 f"Auto-detected n_jobs={n_jobs} "
192 |                 f"(available: {available_gb:.1f}GB, ~{memory_per_worker_mb:.0f}MB/worker, "
193 |                 f"pressure: {mem_info.macos_pressure.pressure_level if mem_info.macos_pressure else 'N/A'})"
    |                                                                                                     ^^^^^^^^
194 |             )
    |

E501 Line too long (104 > 100)
   --> scripts/quick_c_search.py:207:101
    |
205 |         pipeline = Pipeline([
206 |             ("preprocessor", preprocessor),
207 |             ("svm", LinearSVC(C=c_val, class_weight="balanced", max_iter=5000, random_state=args.seed)),
    |                                                                                                     ^^^^
208 |         ])
    |

E501 Line too long (104 > 100)
   --> scripts/quick_c_search.py:211:101
    |
210 |         start = time.time()
211 |         scores = cross_val_score(pipeline, X_subset, y_subset, cv=cv, scoring="f1_macro", n_jobs=n_jobs)
    |                                                                                                     ^^^^
212 |         elapsed = time.time() - start
    |

F541 [*] f-string without any placeholders
   --> scripts/quick_c_search.py:255:11
    |
254 |     # Recommendations
255 |     print(f"\nRecommendations:")
    |           ^^^^^^^^^^^^^^^^^^^^^
256 |     if best_result["C"] == max(args.c_values):
257 |         print(f"  âš ï¸  Best C is at the upper bound ({best_result['C']})")
    |
help: Remove extraneous `f` prefix

E501 Line too long (104 > 100)
   --> scripts/quick_c_search.py:260:101
    |
258 |         higher_vals = [int(c * 2) for c in args.c_values[-2:]]
259 |         print(f"  â†’ Test higher values: {higher_vals}")
260 |         print(f"  â†’ Run: python scripts/quick_c_search.py --c-values {' '.join(map(str, higher_vals))}")
    |                                                                                                     ^^^^
261 |     elif best_result["C"] == min(args.c_values):
262 |         print(f"  âš ï¸  Best C is at the lower bound ({best_result['C']})")
    |

E501 Line too long (103 > 100)
   --> scripts/quick_c_search.py:265:101
    |
263 |         lower_vals = [c / 2 for c in args.c_values[:2]]
264 |         print(f"  â†’ Test lower values: {lower_vals}")
265 |         print(f"  â†’ Run: python scripts/quick_c_search.py --c-values {' '.join(map(str, lower_vals))}")
    |                                                                                                     ^^^
266 |     else:
267 |         # Find neighbors - check if there's a big gap
    |

E501 Line too long (110 > 100)
   --> scripts/quick_c_search.py:283:102
    |
281 |                     int(next_c),
282 |                 ]
283 |                 print(f"  âš ï¸  Large gap between C={best_result['C']} and C={next_c} (ratio: {gap_ratio:.1f}x)")
    |                                                                                                     ^^^^^^^^^^
284 |                 print(f"  â†’ Refine search in this range: {refined_range}")
285 |                 print(f"  â†’ Run: python scripts/quick_c_search.py --c-values {' '.join(map(str, refined_range))}")
    |

E501 Line too long (114 > 100)
   --> scripts/quick_c_search.py:285:101
    |
283 |                 print(f"  âš ï¸  Large gap between C={best_result['C']} and C={next_c} (ratio: {gap_ratio:.1f}x)")
284 |                 print(f"  â†’ Refine search in this range: {refined_range}")
285 |                 print(f"  â†’ Run: python scripts/quick_c_search.py --c-values {' '.join(map(str, refined_range))}")
    |                                                                                                     ^^^^^^^^^^^^^^
286 |             else:
287 |                 neighbors = []
    |

F541 [*] f-string without any placeholders
   --> scripts/quick_c_search.py:293:23
    |
291 |                 neighbors.append(results[c_idx + 1]["C"])
292 |
293 |                 print(f"  âœ“ Best C is in the middle of tested range")
    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
294 |                 print(f"  â†’ Run full dataset with C={neighbors} for final tuning")
295 |         else:
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/quick_c_search.py:296:19
    |
294 |                 print(f"  â†’ Run full dataset with C={neighbors} for final tuning")
295 |         else:
296 |             print(f"  âœ“ Best C found")
    |                   ^^^^^^^^^^^^^^^^^^^
297 |             print(f"  â†’ Run full dataset with C={best_result['C']}")
    |
help: Remove extraneous `f` prefix

E402 Module level import not at top of file
  --> scripts/rbf_search.py:36:1
   |
34 | sys.path.insert(0, str(PROJECT_ROOT))
35 |
36 | from jarvis.utils.memory import MemoryMonitor, get_swap_info, get_memory_info, get_top_memory_processes
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
37 |
38 | import psutil
   |

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/rbf_search.py:36:1
   |
34 |   sys.path.insert(0, str(PROJECT_ROOT))
35 |
36 | / from jarvis.utils.memory import MemoryMonitor, get_swap_info, get_memory_info, get_top_memory_processes
37 | |
38 | | import psutil
   | |_____________^
39 |
40 |   # Setup logging to file for real-time progress tracking
   |
help: Organize imports

E501 Line too long (103 > 100)
  --> scripts/rbf_search.py:36:101
   |
34 | sys.path.insert(0, str(PROJECT_ROOT))
35 |
36 | from jarvis.utils.memory import MemoryMonitor, get_swap_info, get_memory_info, get_top_memory_processes
   |                                                                                                     ^^^
37 |
38 | import psutil
   |

E402 Module level import not at top of file
  --> scripts/rbf_search.py:38:1
   |
36 | from jarvis.utils.memory import MemoryMonitor, get_swap_info, get_memory_info, get_top_memory_processes
37 |
38 | import psutil
   | ^^^^^^^^^^^^^
39 |
40 | # Setup logging to file for real-time progress tracking
   |

N803 Argument name `X` should be lowercase
  --> scripts/rbf_search.py:65:22
   |
65 | def apply_resampling(X, y, resample_map, seed=42):
   |                      ^
66 |     """Resample classes: oversample (factor>1) or downsample (factor<1)."""
67 |     if not resample_map:
   |

F541 [*] f-string without any placeholders
  --> scripts/rbf_search.py:70:11
   |
68 |         return X, y
69 |
70 |     print(f"\nApplying resampling:")
   |           ^^^^^^^^^^^^^^^^^^^^^^^^^
71 |     X_list = []
72 |     y_list = []
   |
help: Remove extraneous `f` prefix

N806 Variable `X_list` in function should be lowercase
  --> scripts/rbf_search.py:71:5
   |
70 |     print(f"\nApplying resampling:")
71 |     X_list = []
   |     ^^^^^^
72 |     y_list = []
   |

N806 Variable `label_X` in function should be lowercase
  --> scripts/rbf_search.py:76:9
   |
74 |     for label in sorted(set(y)):
75 |         mask = y == label
76 |         label_X = X[mask]
   |         ^^^^^^^
77 |         label_y = y[mask]
78 |         n_original = len(label_y)
   |

E501 Line too long (108 > 100)
  --> scripts/rbf_search.py:91:101
   |
89 |             y_list.append(label_y)
90 |             y_list.append(label_y[indices])
91 |             print(f"  {label:12s}: {n_original:5d} â†’ {n_original + n_to_add:5d} ({factor:.2f}x oversample)")
   |                                                                                                     ^^^^^^^^
92 |
93 |         elif factor < 1.0:
   |

N806 Variable `X_resampled` in function should be lowercase
   --> scripts/rbf_search.py:108:5
    |
106 |             print(f"  {label:12s}: {n_original:5d} (unchanged)")
107 |
108 |     X_resampled = np.vstack(X_list)
    |     ^^^^^^^^^^^
109 |     y_resampled = np.concatenate(y_list)
    |

E501 Line too long (106 > 100)
   --> scripts/rbf_search.py:151:101
    |
149 |         nargs="+",
150 |         default=["commissive=2.6", "directive=3.0"],
151 |         help="Resample classes: >1.0=oversample, <1.0=downsample (default: commissive=2.6 directive=3.0)",
    |                                                                                                     ^^^^^^
152 |     )
153 |     parser.add_argument(
    |

N806 Variable `X_train` in function should be lowercase
   --> scripts/rbf_search.py:184:5
    |
182 |     metadata = json.loads((args.data_dir / "metadata.json").read_text())
183 |
184 |     X_train, y_train = train_data["X"], train_data["y"]
    |     ^^^^^^^
185 |     X_test, y_test = test_data["X"], test_data["y"]
    |

N806 Variable `X_test` in function should be lowercase
   --> scripts/rbf_search.py:185:5
    |
184 |     X_train, y_train = train_data["X"], train_data["y"]
185 |     X_test, y_test = test_data["X"], test_data["y"]
    |     ^^^^^^
186 |
187 |     embedding_dims = metadata["embedding_dims"]
    |

N806 Variable `X_train` in function should be lowercase
   --> scripts/rbf_search.py:198:9
    |
196 |         from sklearn.model_selection import train_test_split
197 |         print(f"\nUsing subset: {args.subset_size:,} samples (for testing)")
198 |         X_train, _, y_train, _ = train_test_split(
    |         ^^^^^^^
199 |             X_train,
200 |             y_train,
    |

N806 Variable `X_train_resampled` in function should be lowercase
   --> scripts/rbf_search.py:208:5
    |
206 |     # Apply resampling ONLY to training data
207 |     resample_map = parse_resample_args(args.resample)
208 |     X_train_resampled, y_train_resampled = apply_resampling(
    |     ^^^^^^^^^^^^^^^^^
209 |         X_train, y_train, resample_map, args.seed
210 |     )
    |

F541 [*] f-string without any placeholders
   --> scripts/rbf_search.py:215:11
    |
214 |     # Show class distribution (training data after resampling)
215 |     print(f"\nTraining class distribution (after resampling):")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
216 |     for label in sorted(set(y_train_resampled)):
217 |         count = (y_train_resampled == label).sum()
    |
help: Remove extraneous `f` prefix

E501 Line too long (117 > 100)
   --> scripts/rbf_search.py:229:101
    |
227 |     spacy_dims = metadata.get("spacy_dims", 0)
228 |     if spacy_dims > 0:
229 |         spacy_cols = list(range(embedding_dims + hand_crafted_dims, embedding_dims + hand_crafted_dims + spacy_dims))
    |                                                                                                     ^^^^^^^^^^^^^^^^^
230 |         print(f"\nFeature columns:")
231 |         print(f"  Embeddings: {len(embedding_cols)} features (normalized, passthrough)")
    |

F541 [*] f-string without any placeholders
   --> scripts/rbf_search.py:230:15
    |
228 |     if spacy_dims > 0:
229 |         spacy_cols = list(range(embedding_dims + hand_crafted_dims, embedding_dims + hand_crafted_dims + spacy_dims))
230 |         print(f"\nFeature columns:")
    |               ^^^^^^^^^^^^^^^^^^^^^
231 |         print(f"  Embeddings: {len(embedding_cols)} features (normalized, passthrough)")
232 |         print(f"  Hand-crafted: {len(hc_cols)} features (scaled)")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/rbf_search.py:243:15
    |
241 |         )
242 |     else:
243 |         print(f"\nFeature columns:")
    |               ^^^^^^^^^^^^^^^^^^^^^
244 |         print(f"  Embeddings: {len(embedding_cols)} features (normalized, passthrough)")
245 |         print(f"  Hand-crafted: {len(hc_cols)} features (scaled)")
    |
help: Remove extraneous `f` prefix

E501 Line too long (139 > 100)
   --> scripts/rbf_search.py:302:101
    |
300 | â€¦dom_state=args.seed)
301 | â€¦
302 | â€¦{len(gamma_values)} gamma values = {len(args.c_values) * len(gamma_values)} configs")
    |                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
303 | â€¦s...\n")
    |

F841 Local variable `final_info` is assigned to but never used
   --> scripts/rbf_search.py:339:9
    |
337 |         stop_monitoring.set()
338 |         checker_thread.join(timeout=2.0)
339 |         final_info = monitor.stop()
    |         ^^^^^^^^^^
340 |
341 |         elapsed = time.time() - start_time
    |
help: Remove assignment to unused variable `final_info`

E501 Line too long (121 > 100)
   --> scripts/rbf_search.py:368:101
    |
366 |         std_f1 = search.cv_results_['std_test_score'][i]
367 |
368 |         gamma_str = str(params['svm__gamma']) if isinstance(params['svm__gamma'], str) else f"{params['svm__gamma']:.4f}"
    |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^
369 |         print(f"{params['svm__C']:8.1f}  {gamma_str:>8s}  {mean_f1:8.4f}  {std_f1:6.4f}")
    |

invalid-syntax: Cannot use an escape sequence (backslash) in f-strings on Python 3.11 (syntax was added in Python 3.12)
   --> scripts/rbf_search.py:417:20
    |
416 |     # Print header
417 |     print(f"{'True \\ Pred':>15s}", end="")
    |                    ^
418 |     for label in labels:
419 |         print(f"{label[:10]:>12s}", end="")
    |

invalid-syntax: Cannot use an escape sequence (backslash) in f-strings on Python 3.11 (syntax was added in Python 3.12)
   --> scripts/rbf_search.py:417:21
    |
416 |     # Print header
417 |     print(f"{'True \\ Pred':>15s}", end="")
    |                     ^
418 |     for label in labels:
419 |         print(f"{label[:10]:>12s}", end="")
    |

F541 [*] f-string without any placeholders
   --> scripts/rbf_search.py:469:11
    |
468 |     # Recommendations
469 |     print(f"\nNext steps:")
    |           ^^^^^^^^^^^^^^^^
470 |     if best_result['mean_f1'] < 0.65:
471 |         print(f"  âš ï¸  F1 is still low ({best_result['mean_f1']:.4f})")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/rbf_search.py:472:15
    |
470 |     if best_result['mean_f1'] < 0.65:
471 |         print(f"  âš ï¸  F1 is still low ({best_result['mean_f1']:.4f})")
472 |         print(f"  â†’ Try different oversampling rates")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
473 |     elif best_result['mean_f1'] > 0.75:
474 |         print(f"  âœ… Good F1 score ({best_result['mean_f1']:.4f})!")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/rbf_search.py:475:15
    |
473 |     elif best_result['mean_f1'] > 0.75:
474 |         print(f"  âœ… Good F1 score ({best_result['mean_f1']:.4f})!")
475 |         print(f"  â†’ Evaluate on test set with show_per_class_scores.py")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
476 |     else:
477 |         print(f"  â†’ Moderate F1 ({best_result['mean_f1']:.4f})")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/rbf_search.py:478:15
    |
476 |     else:
477 |         print(f"  â†’ Moderate F1 ({best_result['mean_f1']:.4f})")
478 |         print(f"  â†’ Try adjusting class balance")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
479 |
480 |     # Check final memory
    |
help: Remove extraneous `f` prefix

E402 Module level import not at top of file
  --> scripts/revalidate_with_ner_server.py:18:1
   |
16 | sys.path.insert(0, str(PROJECT_ROOT))
17 |
18 | from jarvis.text_normalizer import normalize_text
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
19 | from jarvis.nlp.ner_client import get_syntactic_features_batch, is_service_running
   |

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/revalidate_with_ner_server.py:18:1
   |
16 |   sys.path.insert(0, str(PROJECT_ROOT))
17 |
18 | / from jarvis.text_normalizer import normalize_text
19 | | from jarvis.nlp.ner_client import get_syntactic_features_batch, is_service_running
   | |__________________________________________________________________________________^
20 |
21 |   print("=" * 70)
   |
help: Organize imports

E402 Module level import not at top of file
  --> scripts/revalidate_with_ner_server.py:19:1
   |
18 | from jarvis.text_normalizer import normalize_text
19 | from jarvis.nlp.ner_client import get_syntactic_features_batch, is_service_running
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
20 |
21 | print("=" * 70)
   |

E402 Module level import not at top of file
  --> scripts/revalidate_with_ner_server.py:65:1
   |
64 | # 2. Get embeddings
65 | from models.bert_embedder import get_in_process_embedder
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
66 |
67 | print("   Loading BERT model...")
   |

E402 Module level import not at top of file
  --> scripts/revalidate_with_ner_server.py:73:1
   |
72 | # 3. Hand-crafted features (copy from prepare script)
73 | import re
   | ^^^^^^^^^
74 |
75 | EMOJI_RE = re.compile(
   |

E402 Module level import not at top of file
   --> scripts/revalidate_with_ner_server.py:157:1
    |
156 | # Distribution
157 | from collections import Counter
    | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
158 | dist = Counter(lgbm_preds)
159 | print("New LightGBM Distribution:")
    |

I001 [*] Import block is un-sorted or un-formatted
   --> scripts/revalidate_with_ner_server.py:157:1
    |
156 | # Distribution
157 | from collections import Counter
    | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
158 | dist = Counter(lgbm_preds)
159 | print("New LightGBM Distribution:")
    |
help: Organize imports

E501 Line too long (111 > 100)
   --> scripts/revalidate_with_ner_server.py:184:101
    |
182 | print("Per-Class Performance")
183 | print("=" * 70)
184 | print(classification_report(claude, lgbm_preds, labels=labels, target_names=labels, digits=3, zero_division=0))
    |                                                                                                     ^^^^^^^^^^^
185 |
186 | # Verdict
    |

E402 Module level import not at top of file
  --> scripts/revalidate_with_proper_features.py:23:1
   |
21 | sys.path.insert(0, str(PROJECT_ROOT))
22 |
23 | from jarvis.text_normalizer import normalize_text
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
24 |
25 | print("=" * 70)
   |

E402 Module level import not at top of file
  --> scripts/revalidate_with_proper_features.py:68:1
   |
67 | # 2. Get embeddings
68 | from models.bert_embedder import get_in_process_embedder
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
69 |
70 | print("   Loading BERT model...")
   |

E402 Module level import not at top of file
  --> scripts/revalidate_with_proper_features.py:76:1
   |
75 | # 3. Hand-crafted features (19 features)
76 | import re
   | ^^^^^^^^^
77 |
78 | EMOJI_RE = re.compile(
   |

E501 Line too long (125 > 100)
   --> scripts/revalidate_with_proper_features.py:141:101
    |
139 |     # 2. you_modal: "can you", "could you", "would you", "will you"
140 |     text_lower = text.lower()
141 |     you_modal = 1.0 if any(p in text_lower for p in ["can you", "could you", "would you", "will you", "should you"]) else 0.0
    |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^
142 |     features.append(you_modal)
    |

E501 Line too long (105 > 100)
   --> scripts/revalidate_with_proper_features.py:160:101
    |
159 |     # 6. i_will: "I'll", "I will", "I'm gonna"
160 |     i_will = 1.0 if any(p in text_lower for p in ["i'll", "i will", "i'm gonna", "ima", "imma"]) else 0.0
    |                                                                                                     ^^^^^
161 |     features.append(i_will)
    |

E501 Line too long (103 > 100)
   --> scripts/revalidate_with_proper_features.py:169:101
    |
168 |     # 8. first_person_count
169 |     first_person = sum(1 for token in doc if token.text.lower() in ("i", "me", "my", "mine", "myself"))
    |                                                                                                     ^^^
170 |     features.append(float(first_person))
    |

E501 Line too long (101 > 100)
   --> scripts/revalidate_with_proper_features.py:173:101
    |
172 |     # 9. agreement: Agreement words
173 |     agreement_words = {"sure", "okay", "ok", "yes", "yeah", "yep", "yup", "sounds good", "bet", "fs"}
    |                                                                                                     ^
174 |     has_agreement = 1.0 if any(word in text_lower for word in agreement_words) else 0.0
175 |     features.append(has_agreement)
    |

E501 Line too long (105 > 100)
   --> scripts/revalidate_with_proper_features.py:186:101
    |
185 |     # 12. second_person_count
186 |     second_person = sum(1 for token in doc if token.text.lower() in ("you", "your", "yours", "yourself"))
    |                                                                                                     ^^^^^
187 |     features.append(float(second_person))
    |

E501 Line too long (112 > 100)
   --> scripts/revalidate_with_proper_features.py:194:101
    |
193 |     # 14. is_interrogative: Question indicators
194 |     is_question = 1.0 if "?" in text or any(token.tag_ in ("WDT", "WP", "WP$", "WRB") for token in doc) else 0.0
    |                                                                                                     ^^^^^^^^^^^^
195 |     features.append(is_question)
    |

F541 [*] f-string without any placeholders
   --> scripts/revalidate_with_proper_features.py:220:7
    |
218 |     lgbm_preds_new = list(lgbm_pred_raw)
219 |
220 | print(f"âœ“ New LightGBM predictions complete")
    |       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
221 | print()
    |
help: Remove extraneous `f` prefix

E402 Module level import not at top of file
   --> scripts/revalidate_with_proper_features.py:233:1
    |
232 | # Agreement rates (Claude as ground truth)
233 | from collections import Counter
    | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
234 |
235 | lgbm_old_agreement = sum(1 for c, l in zip(claude_preds, lgbm_preds_old) if c == l)
    |

E501 Line too long (121 > 100)
   --> scripts/revalidate_with_proper_features.py:285:101
    |
283 | print("New LightGBM Per-Class (Claude as ground truth)")
284 | print("=" * 70)
285 | print(classification_report(claude_preds, lgbm_preds_new, labels=labels, target_names=labels, digits=3, zero_division=0))
    |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^
286 |
287 | # Winner
    |

F541 [*] f-string without any placeholders
   --> scripts/revalidate_with_proper_features.py:295:11
    |
293 | if lgbm_new_rate > llm_rate + 0.05:
294 |     print(f"âœ… LightGBM (FIXED) WINS ({lgbm_new_rate:.1%} vs {llm_rate:.1%})")
295 |     print(f"   â†’ Use LightGBM for production")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
296 | elif llm_rate > lgbm_new_rate + 0.05:
297 |     print(f"âœ… LLM STILL WINS ({llm_rate:.1%} vs {lgbm_new_rate:.1%})")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/revalidate_with_proper_features.py:298:11
    |
296 | elif llm_rate > lgbm_new_rate + 0.05:
297 |     print(f"âœ… LLM STILL WINS ({llm_rate:.1%} vs {lgbm_new_rate:.1%})")
298 |     print(f"   â†’ Need more improvements to LightGBM")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
299 | else:
300 |     print(f"ðŸ¤ TIE ({lgbm_new_rate:.1%} vs {llm_rate:.1%})")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/revalidate_with_proper_features.py:301:11
    |
299 | else:
300 |     print(f"ðŸ¤ TIE ({lgbm_new_rate:.1%} vs {llm_rate:.1%})")
301 |     print(f"   â†’ Either model is acceptable")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
302 |
303 | print()
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
  --> scripts/show_distribution.py:38:7
   |
36 | print("10K STRATIFIED SUBSET (before oversampling)")
37 | print("=" * 70)
38 | print(f"Total samples: 10,000\n")
   |       ^^^^^^^^^^^^^^^^^^^^^^^^^^
39 |
40 | X_subset, _, y_subset, _ = train_test_split(
   |
help: Remove extraneous `f` prefix

N816 Variable `label_X` in global scope should not be mixedCase
  --> scripts/show_distribution.py:63:5
   |
61 | for label, factor in oversample_map.items():
62 |     mask = y_subset == label
63 |     label_X = X_subset[mask]
   |     ^^^^^^^
64 |     label_y = y_subset[mask]
   |

E501 Line too long (105 > 100)
   --> scripts/show_distribution.py:102:101
    |
100 |           f"{resampled_counts[label]:>5,} ({resampled_pct:5.1f}%)")
101 |
102 | print(f"\n{'TOTAL':<15s} {len(y_full):>6,}          {len(y_subset):>5,}          {len(y_resampled):>5,}")
    |                                                                                                     ^^^^^
103 |
104 | # Recommendations
    |

N816 Variable `label_X` in global scope should not be mixedCase
  --> scripts/show_per_class_scores.py:44:5
   |
42 | for label, factor in oversample_map.items():
43 |     mask = y_subset == label
44 |     label_X = X_subset[mask]
   |     ^^^^^^^
45 |     label_y = y_subset[mask]
   |

invalid-syntax: Cannot use an escape sequence (backslash) in f-strings on Python 3.11 (syntax was added in Python 3.12)
   --> scripts/show_per_class_scores.py:115:16
    |
114 | # Print header
115 | print(f"{'True \\ Pred':>15s}", end="")
    |                ^
116 | for label in labels:
117 |     print(f"{label[:10]:>12s}", end="")
    |

invalid-syntax: Cannot use an escape sequence (backslash) in f-strings on Python 3.11 (syntax was added in Python 3.12)
   --> scripts/show_per_class_scores.py:115:17
    |
114 | # Print header
115 | print(f"{'True \\ Pred':>15s}", end="")
    |                 ^
116 | for label in labels:
117 |     print(f"{label[:10]:>12s}", end="")
    |

N816 Variable `label_X` in global scope should not be mixedCase
  --> scripts/show_per_class_scores_3x.py:41:5
   |
39 | for label, factor in oversample_map.items():
40 |     mask = y_subset == label
41 |     label_X = X_subset[mask]
   |     ^^^^^^^
42 |     label_y = y_subset[mask]
   |

invalid-syntax: Cannot use an escape sequence (backslash) in f-strings on Python 3.11 (syntax was added in Python 3.12)
   --> scripts/show_per_class_scores_3x.py:137:16
    |
136 | # Print header
137 | print(f"{'True \\ Pred':>15s}", end="")
    |                ^
138 | for label in labels:
139 |     print(f"{label[:10]:>12s}", end="")
    |

invalid-syntax: Cannot use an escape sequence (backslash) in f-strings on Python 3.11 (syntax was added in Python 3.12)
   --> scripts/show_per_class_scores_3x.py:137:17
    |
136 | # Print header
137 | print(f"{'True \\ Pred':>15s}", end="")
    |                 ^
138 | for label in labels:
139 |     print(f"{label[:10]:>12s}", end="")
    |

E501 Line too long (115 > 100)
  --> scripts/test_ack_labeling.py:10:101
   |
 8 | ack_examples = [
 9 |     {"text": "ok", "context": [], "last_message": "See you at 5", "metadata": None},
10 |     {"text": "lol", "context": ["That was so funny"], "last_message": "Did you see that video?", "metadata": None},
   |                                                                                                     ^^^^^^^^^^^^^^^
11 |     {"text": "thanks", "context": [], "last_message": "Here's the file", "metadata": None},
12 |     {"text": "Liked \"hey there\"", "context": [], "last_message": "hey there", "metadata": None},
   |

E402 Module level import not at top of file
  --> scripts/test_improved_prompt.py:23:1
   |
21 |             os.environ.setdefault(_k.strip(), _v.strip())
22 |
23 | from openai import OpenAI
   | ^^^^^^^^^^^^^^^^^^^^^^^^^
24 |
25 | from evals.judge_config import JUDGE_API_KEY_ENV, JUDGE_BASE_URL
   |

E402 Module level import not at top of file
  --> scripts/test_improved_prompt.py:25:1
   |
23 | from openai import OpenAI
24 |
25 | from evals.judge_config import JUDGE_API_KEY_ENV, JUDGE_BASE_URL
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |

E501 Line too long (113 > 100)
   --> scripts/test_improved_prompt.py:127:101
    |
127 | def label_with_improved_prompt(messages: list[dict], model: str = "qwen-3-235b-a22b-instruct-2507") -> list[str]:
    |                                                                                                     ^^^^^^^^^^^^^
128 |     """Label messages ONE AT A TIME with improved prompt including context."""
129 |     api_key = os.environ.get(JUDGE_API_KEY_ENV, "")
    |

E501 Line too long (116 > 100)
   --> scripts/test_improved_prompt.py:204:101
    |
203 |     print(
204 |         f"  Heuristic labeled: {heuristic_labeled}/{len(messages)} ({heuristic_labeled / len(messages) * 100:.1f}%)"
    |                                                                                                     ^^^^^^^^^^^^^^^^
205 |     )
206 |     print(
    |

E501 Line too long (103 > 100)
   --> scripts/test_improved_prompt.py:207:101
    |
205 |     )
206 |     print(
207 |         f"  Need LLM: {len(llm_needed)}/{len(messages)} ({len(llm_needed) / len(messages) * 100:.1f}%)"
    |                                                                                                     ^^^
208 |     )
    |

F541 [*] f-string without any placeholders
   --> scripts/test_improved_prompt.py:259:11
    |
257 |     llm_errors = [e for e in errors if e["method"] == "llm"]
258 |
259 |     print(f"\nBreakdown:")
    |           ^^^^^^^^^^^^^^^
260 |     print(
261 |         f"  Heuristic errors: {len(heuristic_errors)}/{heuristic_labeled} ({len(heuristic_errors) / max(heuristic_labeled, 1) * 100:.â€¦
    |
help: Remove extraneous `f` prefix

E501 Line too long (150 > 100)
   --> scripts/test_improved_prompt.py:261:101
    |
259 | â€¦
260 | â€¦
261 | â€¦istic_labeled} ({len(heuristic_errors) / max(heuristic_labeled, 1) * 100:.1f}% error rate)"
    |                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
262 | â€¦
263 | â€¦
    |

E501 Line too long (128 > 100)
   --> scripts/test_improved_prompt.py:264:101
    |
262 |     )
263 |     print(
264 |         f"  LLM errors: {len(llm_errors)}/{len(llm_needed)} ({len(llm_errors) / max(len(llm_needed), 1) * 100:.1f}% error rate)"
    |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
265 |     )
    |

F541 [*] f-string without any placeholders
   --> scripts/test_improved_prompt.py:268:11
    |
267 |     # Show errors
268 |     print(f"\nFirst 15 errors:")
    |           ^^^^^^^^^^^^^^^^^^^^^
269 |     for i, e in enumerate(errors[:15], 1):
270 |         print(f'{i:2d}. [{e["method"]:10s}] Gold: {e["gold"]:20s} Got: {e["improved"]:20s}')
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/test_improved_prompt.py:304:11
    |
302 |     print("=" * 80)
303 |     print(f"Improved Prompt:  {accuracy * 100:.1f}%")
304 |     print(f"Original Qwen:    44.0%")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^
305 |     print(f"Original Llama:   50.0%")
306 |     print(f"Improvement:      {(accuracy - 0.44) * 100:+.1f} points vs Qwen")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/test_improved_prompt.py:305:11
    |
303 |     print(f"Improved Prompt:  {accuracy * 100:.1f}%")
304 |     print(f"Original Qwen:    44.0%")
305 |     print(f"Original Llama:   50.0%")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^
306 |     print(f"Improvement:      {(accuracy - 0.44) * 100:+.1f} points vs Qwen")
    |
help: Remove extraneous `f` prefix

E402 Module level import not at top of file
  --> scripts/test_linear_oversample.py:28:1
   |
27 | # Create 10k subset
28 | from sklearn.model_selection import train_test_split
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
29 | X_subset, _, y_subset, _ = train_test_split(
30 |     X_full, y_full, train_size=10000, stratify=y_full, random_state=42
   |

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/test_linear_oversample.py:28:1
   |
27 | # Create 10k subset
28 | from sklearn.model_selection import train_test_split
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
29 | X_subset, _, y_subset, _ = train_test_split(
30 |     X_full, y_full, train_size=10000, stratify=y_full, random_state=42
   |
help: Organize imports

N816 Variable `label_X` in global scope should not be mixedCase
  --> scripts/test_linear_oversample.py:47:5
   |
45 | for label, factor in oversample_map.items():
46 |     mask = y_subset == label
47 |     label_X = X_subset[mask]
   |     ^^^^^^^
48 |     label_y = y_subset[mask]
   |

F541 [*] f-string without any placeholders
   --> scripts/test_linear_oversample.py:100:7
    |
 99 | print(f"\n{'=' * 50}")
100 | print(f"Linear SVM + Oversample")
    |       ^^^^^^^^^^^^^^^^^^^^^^^^^^
101 | print(f"F1: {scores.mean():.4f} Â± {scores.std():.4f}")
102 | print(f"{'=' * 50}")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/test_linear_oversample.py:105:7
    |
104 | print("\nComparison:")
105 | print(f"  Linear, no oversample:  0.6099")
    |       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
106 | print(f"  RBF, no oversample:     0.6295 (+3.2%)")
107 | print(f"  RBF + oversample:       0.7150 (+17.2%)")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/test_linear_oversample.py:106:7
    |
104 | print("\nComparison:")
105 | print(f"  Linear, no oversample:  0.6099")
106 | print(f"  RBF, no oversample:     0.6295 (+3.2%)")
    |       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
107 | print(f"  RBF + oversample:       0.7150 (+17.2%)")
108 | print(f"  Linear + oversample:    {scores.mean():.4f} (+{100*(scores.mean()-0.6099)/0.6099:.1f}%)")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/test_linear_oversample.py:107:7
    |
105 | print(f"  Linear, no oversample:  0.6099")
106 | print(f"  RBF, no oversample:     0.6295 (+3.2%)")
107 | print(f"  RBF + oversample:       0.7150 (+17.2%)")
    |       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
108 | print(f"  Linear + oversample:    {scores.mean():.4f} (+{100*(scores.mean()-0.6099)/0.6099:.1f}%)")
    |
help: Remove extraneous `f` prefix

E402 Module level import not at top of file
  --> scripts/test_llm_labeling.py:27:1
   |
25 |             os.environ.setdefault(_k.strip(), _v.strip())
26 |
27 | from datasets import load_dataset
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
28 | from openai import OpenAI
   |

E402 Module level import not at top of file
  --> scripts/test_llm_labeling.py:28:1
   |
27 | from datasets import load_dataset
28 | from openai import OpenAI
   | ^^^^^^^^^^^^^^^^^^^^^^^^^
29 |
30 | from evals.judge_config import JUDGE_API_KEY_ENV, JUDGE_BASE_URL
   |

E402 Module level import not at top of file
  --> scripts/test_llm_labeling.py:30:1
   |
28 | from openai import OpenAI
29 |
30 | from evals.judge_config import JUDGE_API_KEY_ENV, JUDGE_BASE_URL
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
31 |
32 | # Label mapping (correct one)
   |

F541 [*] f-string without any placeholders
  --> scripts/test_llm_labeling.py:59:11
   |
57 | def sample_dailydialog(n: int = 50, seed: int = 42) -> list[dict]:
58 |     """Sample n messages from DailyDialog with ground truth labels."""
59 |     print(f"Loading DailyDialog...")
   |           ^^^^^^^^^^^^^^^^^^^^^^^^^
60 |     dd = load_dataset("OpenRL/daily_dialog", split="train")
   |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/test_llm_labeling.py:220:15
    |
218 |     if accuracy >= 0.90:
219 |         print(f"âœ… {args.model} is GOOD ENOUGH (â‰¥90% accuracy)")
220 |         print(f"   Safe to use for labeling SAMSum (138k messages)")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
221 |     elif accuracy >= 0.80:
222 |         print(f"âš ï¸  {args.model} is OKAY (80-90% accuracy)")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/test_llm_labeling.py:223:15
    |
221 |     elif accuracy >= 0.80:
222 |         print(f"âš ï¸  {args.model} is OKAY (80-90% accuracy)")
223 |         print(f"   Consider using a larger model or heuristics")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
224 |     else:
225 |         print(f"âŒ {args.model} is TOO INACCURATE (<80%)")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/test_llm_labeling.py:226:15
    |
224 |     else:
225 |         print(f"âŒ {args.model} is TOO INACCURATE (<80%)")
226 |         print(f"   Use a larger model or different approach")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
227 |     print("=" * 80)
    |
help: Remove extraneous `f` prefix

E402 Module level import not at top of file
  --> scripts/test_llm_with_heuristics.py:30:1
   |
28 |             os.environ.setdefault(_k.strip(), _v.strip())
29 |
30 | from datasets import load_dataset
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
31 | from openai import OpenAI
   |

E402 Module level import not at top of file
  --> scripts/test_llm_with_heuristics.py:31:1
   |
30 | from datasets import load_dataset
31 | from openai import OpenAI
   | ^^^^^^^^^^^^^^^^^^^^^^^^^
32 |
33 | from evals.judge_config import JUDGE_API_KEY_ENV, JUDGE_BASE_URL
   |

E402 Module level import not at top of file
  --> scripts/test_llm_with_heuristics.py:33:1
   |
31 | from openai import OpenAI
32 |
33 | from evals.judge_config import JUDGE_API_KEY_ENV, JUDGE_BASE_URL
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |

E501 Line too long (114 > 100)
  --> scripts/test_llm_with_heuristics.py:47:101
   |
45 |     if text.startswith(("What ", "When ", "Where ", "Who ", "How ", "Why ", "Which ")):
46 |         return "needs_answer", 0.99
47 |     if "?" in text and any(f" {w} " in f" {text_lower} " for w in ["what", "when", "where", "who", "how", "why"]):
   |                                                                                                     ^^^^^^^^^^^^^^
48 |         return "needs_answer", 0.95
   |

E501 Line too long (117 > 100)
  --> scripts/test_llm_with_heuristics.py:58:101
   |
56 |     # needs_empathy: ONLY explicit emotions
57 |     # Strong negative
58 |     if any(p in text_lower for p in ["i'm so sad", "i'm so stressed", "i hate ", "this sucks", "i'm sorry to hear"]):
   |                                                                                                     ^^^^^^^^^^^^^^^^^
59 |         return "needs_empathy", 0.95
60 |     # Strong positive
   |

E501 Line too long (113 > 100)
  --> scripts/test_llm_with_heuristics.py:61:101
   |
59 |         return "needs_empathy", 0.95
60 |     # Strong positive
61 |     if any(p in text_lower for p in ["congrat", "i'm so excited", "i'm so happy", "that's amazing", "so proud"]):
   |                                                                                                     ^^^^^^^^^^^^^
62 |         return "needs_empathy", 0.95
63 |     # Clear emotion emojis
   |

E501 Line too long (102 > 100)
   --> scripts/test_llm_with_heuristics.py:135:101
    |
133 |     client = OpenAI(base_url=JUDGE_BASE_URL, api_key=api_key)
134 |
135 |     prompt_template = """Classify each message into ONE category based on what kind of REPLY it needs:
    |                                                                                                     ^^
136 |
137 | 1. needs_answer - Expects factual information (questions with what/when/where/who/how/why)
    |

E501 Line too long (113 > 100)
   --> scripts/test_llm_with_heuristics.py:168:101
    |
167 |         # Parse labels
168 |         label_map = {"1": "needs_answer", "2": "needs_confirmation", "3": "needs_empathy", "4": "conversational"}
    |                                                                                                     ^^^^^^^^^^^^^
169 |         labels = []
170 |         for line in response_text.strip().splitlines():
    |

E501 Line too long (119 > 100)
   --> scripts/test_llm_with_heuristics.py:208:101
    |
206 |             llm_needed.append(msg)
207 |
208 |     print(f"  Heuristic labeled: {heuristic_labeled}/{len(messages)} ({heuristic_labeled / len(messages) * 100:.1f}%)")
    |                                                                                                     ^^^^^^^^^^^^^^^^^^^
209 |     print(f"  Need LLM: {len(llm_needed)}/{len(messages)} ({len(llm_needed) / len(messages) * 100:.1f}%)")
    |

E501 Line too long (106 > 100)
   --> scripts/test_llm_with_heuristics.py:209:101
    |
208 |     print(f"  Heuristic labeled: {heuristic_labeled}/{len(messages)} ({heuristic_labeled / len(messages) * 100:.1f}%)")
209 |     print(f"  Need LLM: {len(llm_needed)}/{len(messages)} ({len(llm_needed) / len(messages) * 100:.1f}%)")
    |                                                                                                     ^^^^^^
210 |
211 |     # Label remaining with LLM
    |

F401 [*] `numpy` imported but unused
  --> scripts/test_mlx_memory.py:10:17
   |
 8 | from pathlib import Path
 9 |
10 | import numpy as np
   |                 ^^
11 | import psutil
   |
help: Remove unused import: `numpy`

F541 [*] f-string without any placeholders
  --> scripts/test_mlx_memory.py:49:11
   |
47 |     mem_after_encode = log_memory("After encoding")
48 |
49 |     print(f"\n4. Results:")
   |           ^^^^^^^^^^^^^^^^
50 |     print(f"   Embeddings shape: {embeddings.shape}")
51 |     print(f"   Expected: ({n_texts}, 384)")
   |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
  --> scripts/test_mlx_memory.py:54:11
   |
53 |     # Memory deltas
54 |     print(f"\n5. Memory deltas:")
   |           ^^^^^^^^^^^^^^^^^^^^^^
55 |     print(f"   Embedder load: +{mem_after_load - mem_start:.1f} MB")
56 |     print(f"   Text creation: +{mem_after_texts - mem_after_load:.1f} MB")
   |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
  --> scripts/test_mlx_memory.py:65:11
   |
64 |     # Verdict
65 |     print(f"\n7. Verdict:")
   |           ^^^^^^^^^^^^^^^^
66 |     total_increase = mem_after_encode - mem_start
67 |     if total_increase < 800:
   |
help: Remove extraneous `f` prefix

E402 Module level import not at top of file
  --> scripts/test_qwen_labeling.py:23:1
   |
21 |             os.environ.setdefault(_k.strip(), _v.strip())
22 |
23 | from openai import OpenAI
   | ^^^^^^^^^^^^^^^^^^^^^^^^^
24 |
25 | from evals.judge_config import JUDGE_API_KEY_ENV, JUDGE_BASE_URL
   |

E402 Module level import not at top of file
  --> scripts/test_qwen_labeling.py:25:1
   |
23 | from openai import OpenAI
24 |
25 | from evals.judge_config import JUDGE_API_KEY_ENV, JUDGE_BASE_URL
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |

E501 Line too long (102 > 100)
  --> scripts/test_qwen_labeling.py:85:101
   |
83 |     client = OpenAI(base_url=JUDGE_BASE_URL, api_key=api_key)
84 |
85 |     prompt_template = """Classify each message into ONE category based on what kind of REPLY it needs:
   |                                                                                                     ^^
86 |
87 | 1. needs_answer - Expects factual information (questions with what/when/where/who/how/why)
   |

E501 Line too long (116 > 100)
   --> scripts/test_qwen_labeling.py:171:101
    |
170 |     print(
171 |         f"  Heuristic labeled: {heuristic_labeled}/{len(messages)} ({heuristic_labeled / len(messages) * 100:.1f}%)"
    |                                                                                                     ^^^^^^^^^^^^^^^^
172 |     )
173 |     print(
    |

E501 Line too long (106 > 100)
   --> scripts/test_qwen_labeling.py:174:101
    |
172 |     )
173 |     print(
174 |         f"  Need Qwen: {len(qwen_needed)}/{len(messages)} ({len(qwen_needed) / len(messages) * 100:.1f}%)"
    |                                                                                                     ^^^^^^
175 |     )
    |

F541 [*] f-string without any placeholders
   --> scripts/test_qwen_labeling.py:225:11
    |
223 |     qwen_errors = [e for e in errors if e["method"] == "llm"]
224 |
225 |     print(f"\nBreakdown:")
    |           ^^^^^^^^^^^^^^^
226 |     print(
227 |         f"  Heuristic errors: {len(heuristic_errors)}/{heuristic_labeled} ({len(heuristic_errors) / max(heuristic_labeled, 1) * 100:.â€¦
    |
help: Remove extraneous `f` prefix

E501 Line too long (150 > 100)
   --> scripts/test_qwen_labeling.py:227:101
    |
225 | â€¦
226 | â€¦
227 | â€¦istic_labeled} ({len(heuristic_errors) / max(heuristic_labeled, 1) * 100:.1f}% error rate)"
    |                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
228 | â€¦
229 | â€¦
    |

E501 Line too long (133 > 100)
   --> scripts/test_qwen_labeling.py:230:101
    |
228 |     )
229 |     print(
230 |         f"  Qwen errors: {len(qwen_errors)}/{len(qwen_needed)} ({len(qwen_errors) / max(len(qwen_needed), 1) * 100:.1f}% error rate)"
    |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
231 |     )
    |

F541 [*] f-string without any placeholders
   --> scripts/test_qwen_labeling.py:234:11
    |
233 |     # Show errors
234 |     print(f"\nFirst 15 errors:")
    |           ^^^^^^^^^^^^^^^^^^^^^
235 |     for i, e in enumerate(errors[:15], 1):
236 |         print(f'{i:2d}. [{e["method"]:10s}] Gold: {e["gold"]:20s} Qwen: {e["qwen"]:20s}')
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/test_qwen_labeling.py:269:11
    |
267 |     print("=" * 80)
268 |     print(f"Qwen3-235B:     {accuracy * 100:.1f}%")
269 |     print(f"Llama 3.3 70B:  50.0%")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^
270 |     print(f"Improvement:    {(accuracy - 0.5) * 100:+.1f} percentage points")
    |
help: Remove extraneous `f` prefix

E402 Module level import not at top of file
  --> scripts/test_simple_prompt.py:23:1
   |
21 |             os.environ.setdefault(_k.strip(), _v.strip())
22 |
23 | from openai import OpenAI
   | ^^^^^^^^^^^^^^^^^^^^^^^^^
24 |
25 | from evals.judge_config import JUDGE_API_KEY_ENV, JUDGE_BASE_URL
   |

E402 Module level import not at top of file
  --> scripts/test_simple_prompt.py:25:1
   |
23 | from openai import OpenAI
24 |
25 | from evals.judge_config import JUDGE_API_KEY_ENV, JUDGE_BASE_URL
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |

E501 Line too long (110 > 100)
   --> scripts/test_simple_prompt.py:128:101
    |
126 |             batch_prompts.append(f"Message {i}:\n{prompt}\n")
127 |
128 |         full_prompt = "\n".join(batch_prompts) + f"\nReply with {len(batch)} letters (A/B/C/D), one per line:"
    |                                                                                                     ^^^^^^^^^^
129 |
130 |         if (start // batch_size + 1) % 5 == 0:
    |

E501 Line too long (118 > 100)
   --> scripts/test_simple_prompt.py:131:101
    |
130 |         if (start // batch_size + 1) % 5 == 0:
131 |             print(f"    Batch {start // batch_size + 1}/{(len(messages) + batch_size - 1) // batch_size}", flush=True)
    |                                                                                                     ^^^^^^^^^^^^^^^^^^
132 |
133 |         resp = client.chat.completions.create(
    |

E501 Line too long (114 > 100)
   --> scripts/test_simple_prompt.py:230:101
    |
229 |     print(f"\nHeuristic errors: {len(h_errors)}/{heuristic_labeled}")
230 |     print(f"LLM errors: {len(l_errors)}/{len(llm_needed)} ({len(l_errors) / max(len(llm_needed), 1) * 100:.1f}%)")
    |                                                                                                     ^^^^^^^^^^^^^^
231 |
232 |     print(f"\nFirst 10 errors:")
    |

F541 [*] f-string without any placeholders
   --> scripts/test_simple_prompt.py:232:11
    |
230 |     print(f"LLM errors: {len(l_errors)}/{len(llm_needed)} ({len(l_errors) / max(len(llm_needed), 1) * 100:.1f}%)")
231 |
232 |     print(f"\nFirst 10 errors:")
    |           ^^^^^^^^^^^^^^^^^^^^^
233 |     for i, e in enumerate(errors[:10], 1):
234 |         print(f'{i:2d}. Gold: {e["gold"]:20s} Got: {e["got"]:20s}')
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/test_simple_prompt.py:242:11
    |
240 |     print("=" * 80)
241 |     print(f"Simple Prompt (A/B/C/D):  {accuracy * 100:.1f}%")
242 |     print(f"Original Qwen:            44.0%")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
243 |     print(f"Original Llama:           50.0%")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/test_simple_prompt.py:243:11
    |
241 |     print(f"Simple Prompt (A/B/C/D):  {accuracy * 100:.1f}%")
242 |     print(f"Original Qwen:            44.0%")
243 |     print(f"Original Llama:           50.0%")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
244 |
245 |     if accuracy >= 0.75:
    |
help: Remove extraneous `f` prefix

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/train_category_classifier_v2.py:17:1
   |
15 |   """
16 |
17 | / import json
18 | | import re
19 | | import time
20 | | from pathlib import Path
21 | |
22 | | import joblib
23 | | import lightgbm as lgb
24 | | import numpy as np
25 | | import spacy
26 | | from sklearn.metrics import classification_report
27 | | from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit
28 | | from sklearn.svm import LinearSVC
29 | | from tqdm import tqdm
30 | |
31 | | # Add project root to path
32 | | import sys
   | |__________^
33 |   sys.path.insert(0, str(Path(__file__).parent.parent))
   |
help: Organize imports

E501 Line too long (125 > 100)
  --> scripts/train_category_classifier_v2.py:79:101
   |
77 |     # 2. you_modal: "can you", "could you", "would you", "will you"
78 |     text_lower = text.lower()
79 |     you_modal = 1.0 if any(p in text_lower for p in ["can you", "could you", "would you", "will you", "should you"]) else 0.0
   |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^
80 |     features.append(you_modal)
   |

E501 Line too long (105 > 100)
  --> scripts/train_category_classifier_v2.py:98:101
   |
97 |     # 6. i_will: "I'll", "I will", "I'm gonna"
98 |     i_will = 1.0 if any(p in text_lower for p in ["i'll", "i will", "i'm gonna", "ima", "imma"]) else 0.0
   |                                                                                                     ^^^^^
99 |     features.append(i_will)
   |

E501 Line too long (103 > 100)
   --> scripts/train_category_classifier_v2.py:107:101
    |
106 |     # 8. first_person_count
107 |     first_person = sum(1 for token in doc if token.text.lower() in ("i", "me", "my", "mine", "myself"))
    |                                                                                                     ^^^
108 |     features.append(float(first_person))
    |

E501 Line too long (101 > 100)
   --> scripts/train_category_classifier_v2.py:111:101
    |
110 |     # 9. agreement: Agreement words
111 |     agreement_words = {"sure", "okay", "ok", "yes", "yeah", "yep", "yup", "sounds good", "bet", "fs"}
    |                                                                                                     ^
112 |     has_agreement = 1.0 if any(word in text_lower for word in agreement_words) else 0.0
113 |     features.append(has_agreement)
    |

E501 Line too long (105 > 100)
   --> scripts/train_category_classifier_v2.py:124:101
    |
123 |     # 12. second_person_count
124 |     second_person = sum(1 for token in doc if token.text.lower() in ("you", "your", "yours", "yourself"))
    |                                                                                                     ^^^^^
125 |     features.append(float(second_person))
    |

E501 Line too long (112 > 100)
   --> scripts/train_category_classifier_v2.py:132:101
    |
131 |     # 14. is_interrogative: Question indicators
132 |     is_question = 1.0 if "?" in text or any(token.tag_ in ("WDT", "WP", "WP$", "WRB") for token in doc) else 0.0
    |                                                                                                     ^^^^^^^^^^^^
133 |     features.append(is_question)
    |

E501 Line too long (104 > 100)
   --> scripts/train_category_classifier_v2.py:241:101
    |
239 |     # 4. Concatenate all features
240 |     X = np.hstack([bert_embeds, hand_crafted, spacy_feats])
241 |     print(f"Final feature matrix: {X.shape} (384 BERT + 19 hand + 14 spaCy = {X.shape[1]})", flush=True)
    |                                                                                                     ^^^^
242 |
243 |     return X
    |

F841 Local variable `y_test_encoded` is assigned to but never used
   --> scripts/train_category_classifier_v2.py:300:5
    |
298 |     le = LabelEncoder()
299 |     y_train_encoded = le.fit_transform(y_train)
300 |     y_test_encoded = le.transform(y_test)
    |     ^^^^^^^^^^^^^^
301 |
302 |     param_grid = {
    |
help: Remove assignment to unused variable `y_test_encoded`

E402 Module level import not at top of file
  --> scripts/train_category_svm.py:35:1
   |
33 | sys.path.insert(0, str(PROJECT_ROOT))
34 |
35 | from jarvis.utils.memory import MemoryMonitor, get_swap_info, get_top_memory_processes
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
36 |
37 | # Setup logging to file for real-time progress tracking
   |

F541 [*] f-string without any placeholders
  --> scripts/train_category_svm.py:92:15
   |
90 |         y_train = np.array([label_mapping.get(label, label) for label in y_train])
91 |         y_test = np.array([label_mapping.get(label, label) for label in y_test])
92 |         print(f"Applied 3-class label mapping (directive+commissive â†’ action)")
   |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
93 |
94 |     embedding_dims = metadata["embedding_dims"]
   |
help: Remove extraneous `f` prefix

F841 Local variable `final_info` is assigned to but never used
   --> scripts/train_category_svm.py:174:9
    |
172 |         stop_monitoring.set()
173 |         checker_thread.join(timeout=2.0)
174 |         final_info = monitor.stop()
    |         ^^^^^^^^^^
175 |
176 |         print(f"\nTraining completed at {time.strftime('%H:%M:%S')}")
    |
help: Remove assignment to unused variable `final_info`

F541 [*] f-string without any placeholders
  --> scripts/train_final_lightgbm.py:50:7
   |
49 | print(f"ðŸ† Using Trial #{best_trial['trial']} (F1={best_trial['f1_mean']:.4f})")
50 | print(f"   Simpler model: 30 leaves (vs trial 7's 54 leaves for only +0.09% gain)")
   |       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
51 | print(f"   n_estimators: {best_params['n_estimators']}")
52 | print(f"   num_leaves: {best_params['num_leaves']}")
   |
help: Remove extraneous `f` prefix

E501 Line too long (103 > 100)
  --> scripts/tune_lightgbm.py:42:101
   |
40 | # Ensure immediate flush (critical for tail -f monitoring)
41 | for handler in logger.handlers:
42 |     handler.stream.reconfigure(line_buffering=True) if hasattr(handler.stream, 'reconfigure') else None
   |                                                                                                     ^^^
43 |
44 | PROJECT_ROOT = Path(__file__).parent.parent
   |

E402 Module level import not at top of file
  --> scripts/validate_llm_categories.py:24:1
   |
22 | sys.path.insert(0, str(PROJECT_ROOT))
23 |
24 | from evals.judge_config import get_judge_client, JUDGE_BASE_URL
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
25 |
26 | VALID_CATEGORIES = ["closing", "acknowledge", "question", "request", "emotion", "statement"]
   |

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/validate_llm_categories.py:24:1
   |
22 | sys.path.insert(0, str(PROJECT_ROOT))
23 |
24 | from evals.judge_config import get_judge_client, JUDGE_BASE_URL
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
25 |
26 | VALID_CATEGORIES = ["closing", "acknowledge", "question", "request", "emotion", "statement"]
   |
help: Organize imports

E501 Line too long (104 > 100)
  --> scripts/validate_llm_categories.py:31:101
   |
30 | 1. closing: Says goodbye ("bye", "ttyl", "see you later", "gotta go")
31 | 2. acknowledge: â‰¤5 words AND expresses agreement/thanks ("ok", "yeah", "thanks", "sure", "here you are")
   |                                                                                                     ^^^^
32 | 3. request: Asks for action ("can you", "could you", "would you", "please" + verb, "let's", "I'd like")
33 | 4. question: Has "?" OR starts with question word ("what", "when", "where", "who", "why", "how", "is", "are", "do")
   |

E501 Line too long (103 > 100)
  --> scripts/validate_llm_categories.py:32:101
   |
30 | 1. closing: Says goodbye ("bye", "ttyl", "see you later", "gotta go")
31 | 2. acknowledge: â‰¤5 words AND expresses agreement/thanks ("ok", "yeah", "thanks", "sure", "here you are")
32 | 3. request: Asks for action ("can you", "could you", "would you", "please" + verb, "let's", "I'd like")
   |                                                                                                     ^^^
33 | 4. question: Has "?" OR starts with question word ("what", "when", "where", "who", "why", "how", "is", "are", "do")
34 | 5. emotion: Expressing feelings - has emotion words ("happy", "sad", "love", "hate", "excited", "stressed", "wow") OR "!!" OR ALLCAPS â€¦
   |

E501 Line too long (115 > 100)
  --> scripts/validate_llm_categories.py:33:101
   |
31 | 2. acknowledge: â‰¤5 words AND expresses agreement/thanks ("ok", "yeah", "thanks", "sure", "here you are")
32 | 3. request: Asks for action ("can you", "could you", "would you", "please" + verb, "let's", "I'd like")
33 | 4. question: Has "?" OR starts with question word ("what", "when", "where", "who", "why", "how", "is", "are", "do")
   |                                                                                                     ^^^^^^^^^^^^^^^
34 | 5. emotion: Expressing feelings - has emotion words ("happy", "sad", "love", "hate", "excited", "stressed", "wow") OR "!!" OR ALLCAPS â€¦
35 |    Note: "happy birthday" and "happy new year" are greetings (statement), not genuine emotions
   |

E501 Line too long (148 > 100)
  --> scripts/validate_llm_categories.py:34:101
   |
32 | â€¦d you", "please" + verb, "let's", "I'd like")
33 | â€¦ "when", "where", "who", "why", "how", "is", "are", "do")
34 | â€¦y", "sad", "love", "hate", "excited", "stressed", "wow") OR "!!" OR ALLCAPS words OR ðŸ˜‚ðŸ˜­â¤ï¸
   |                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
35 | â€¦gs (statement), not genuine emotions
36 | â€¦ault - includes "I'm happy to hear that")
   |

E501 Line too long (101 > 100)
   --> scripts/validate_llm_categories.py:109:101
    |
107 |     from datasets import load_dataset
108 |
109 |     print(f"Loading datasets for stratified sampling ({n_per_category} per category)...", flush=True)
    |                                                                                                     ^
110 |
111 |     # Load both datasets
    |

E501 Line too long (120 > 100)
   --> scripts/validate_llm_categories.py:201:101
    |
199 |         else:
200 |             # If not enough, take all we have
201 |             print(f"    Warning: Only {len(cat_examples)} examples for {category}, needed {n_per_category}", flush=True)
    |                                                                                                     ^^^^^^^^^^^^^^^^^^^^
202 |             stratified.extend(cat_examples)
    |

E501 Line too long (110 > 100)
   --> scripts/validate_llm_categories.py:239:101
    |
237 |         messages_block = "\n\n".join([
238 |             f"Message {j + 1}:\n"
239 |             f"Previous: \"{ex['last_message'][:100] if ex['last_message'] else '(start of conversation)'}\"\n"
    |                                                                                                     ^^^^^^^^^^
240 |             f"Current: \"{ex['text']}\""
241 |             for j, ex in enumerate(batch)
    |

E501 Line too long (123 > 100)
   --> scripts/validate_llm_categories.py:254:101
    |
252 |                 model=model,  # Use model from args
253 |                 messages=[
254 |                     {"role": "system", "content": "You are a text classifier. Output only category labels, nothing else."},
    |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^
255 |                     {"role": "user", "content": prompt}
256 |                 ],
    |

F541 [*] f-string without any placeholders
   --> scripts/validate_llm_categories.py:263:23
    |
261 |             # Check if response is valid
262 |             if not response or not response.choices:
263 |                 print(f"    ERROR: Empty response from API", flush=True)
    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
264 |                 predictions.extend(["statement"] * len(batch))
265 |                 continue
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/validate_llm_categories.py:271:23
    |
269 |             content = message.content or message.reasoning
270 |             if content is None:
271 |                 print(f"    ERROR: API returned None for both content and reasoning", flush=True)
    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
272 |                 print(f"    Response: {response}", flush=True)
273 |                 predictions.extend(["statement"] * len(batch))
    |
help: Remove extraneous `f` prefix

E501 Line too long (146 > 100)
   --> scripts/validate_llm_categories.py:300:101
    |
298 | â€¦
299 | â€¦
300 | â€¦|Message\s+\d+[\.:]\s*|\*\*Result:?\s*|-\s*Category:\s*)', '', line, flags=re.IGNORECASE)
    |                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
301 | â€¦('*').strip().lower()
    |

I001 [*] Import block is un-sorted or un-formatted
   --> scripts/validate_llm_categories.py:335:5
    |
333 |           Dict with accuracy, per-class accuracy, confusion matrix.
334 |       """
335 | /     from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
336 | |     import numpy as np
    | |______________________^
337 |
338 |       y_true = [ex["heuristic_label"] for ex in examples]
    |
help: Organize imports

F401 [*] `numpy` imported but unused
   --> scripts/validate_llm_categories.py:336:21
    |
334 |     """
335 |     from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
336 |     import numpy as np
    |                     ^^
337 |
338 |     y_true = [ex["heuristic_label"] for ex in examples]
    |
help: Remove unused import: `numpy`

F541 [*] f-string without any placeholders
   --> scripts/validate_llm_categories.py:345:11
    |
343 |     print("\n=== Results ===")
344 |     print(f"Overall accuracy: {accuracy:.1%}")
345 |     print(f"\nClassification report:")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
346 |     print(classification_report(y_true, y_pred, labels=VALID_CATEGORIES, zero_division=0))
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/validate_llm_categories.py:378:11
    |
376 |     args = parser.parse_args()
377 |
378 |     print(f"=== LLM Category Labeling Pilot ===")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
379 |     print(f"Model: {args.model}")
380 |     print(f"API: {JUDGE_BASE_URL}")
    |
help: Remove extraneous `f` prefix

N806 Variable `DATA_DIR` in function should be lowercase
  --> scripts/validate_llm_on_test.py:36:5
   |
34 | def sample_test_data(n_samples: int = 200) -> tuple[list[str], list[str]]:
35 |     """Sample N examples from test set with ground truth labels."""
36 |     DATA_DIR = PROJECT_ROOT / "data" / "dailydialog_native"
   |     ^^^^^^^^
37 |
38 |     print(f"ðŸ“‚ Loading test data from {DATA_DIR}...")
   |

F841 Local variable `y_test` is assigned to but never used
  --> scripts/validate_llm_on_test.py:42:5
   |
40 |     # Load test data
41 |     test_data = np.load(DATA_DIR / "test.npz", allow_pickle=True)
42 |     y_test = test_data["y"]
   |     ^^^^^^
43 |
44 |     # Load metadata for label mapping
   |
help: Remove assignment to unused variable `y_test`

F541 [*] f-string without any placeholders
  --> scripts/validate_llm_on_test.py:52:11
   |
51 |     # Load DailyDialog dataset directly
52 |     print(f"ðŸ“¥ Downloading DailyDialog dataset...")
   |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
53 |     from datasets import load_dataset
54 |     dataset = load_dataset("OpenRL/daily_dialog", trust_remote_code=True)
   |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/validate_llm_on_test.py:123:21
    |
122 |       # Construct prompt
123 |       system_prompt = f"""You are a text classification expert. Classify each message into one of these categories:
    |  _____________________^
124 | | - commissive: promises, commitments ("I'll do it", "sure, I can help")
125 | | - directive: requests, commands ("can you help?", "send me the file")
126 | | - inform: statements, facts ("I'm at the store", "the meeting is at 3pm")
127 | | - question: asking for information ("where are you?", "what time?")
128 | |
129 | | Respond with ONLY the category name, nothing else."""
    | |_____________________________________________________^
130 |
131 |       predictions = []
    |
help: Remove extraneous `f` prefix

E501 Line too long (113 > 100)
   --> scripts/validate_llm_on_test.py:123:101
    |
122 |     # Construct prompt
123 |     system_prompt = f"""You are a text classification expert. Classify each message into one of these categories:
    |                                                                                                     ^^^^^^^^^^^^^
124 | - commissive: promises, commitments ("I'll do it", "sure, I can help")
125 | - directive: requests, commands ("can you help?", "send me the file")
    |

F541 [*] f-string without any placeholders
   --> scripts/validate_llm_on_test.py:132:11
    |
131 |     predictions = []
132 |     print(f"ðŸ¤– Getting LLM labels (Qwen 235B)...")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
133 |     print(f"   Cost: ~${len(texts) * 0.0001:.4f}")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/validate_llm_on_test.py:169:11
    |
167 |             predictions.append("inform")  # Default
168 |
169 |     print(f"âœ“ LLM labeling complete")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^
170 |     print()
171 |     return predictions
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/validate_on_production.py:157:15
    |
155 |     # Lazy load embedder (singleton)
156 |     if not hasattr(get_embeddings, '_embedder'):
157 |         print(f"   Loading BERT model...")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
158 |         get_embeddings._embedder = get_in_process_embedder(model_name="bge-small")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/validate_on_production.py:171:11
    |
169 |     texts = [msg["text"] for msg in messages]
170 |
171 |     print(f"ðŸ”® Extracting embeddings (BERT)...")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
172 |     embeddings = get_embeddings(texts)
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/validate_on_production.py:174:11
    |
172 |     embeddings = get_embeddings(texts)
173 |
174 |     print(f"ðŸ”§ Extracting hand-crafted features...")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
175 |     hand_crafted = np.array([extract_hand_crafted_features(text) for text in texts])
    |
help: Remove extraneous `f` prefix

F841 Local variable `labels_desc` is assigned to but never used
   --> scripts/validate_on_production.py:206:5
    |
205 |     # Construct prompt
206 |     labels_desc = ", ".join(labels)
    |     ^^^^^^^^^^^
207 |     system_prompt = f"""You are a text classification expert. Classify each message into one of these categories:
208 | - commissive: promises, commitments ("I'll do it", "sure, I can help")
    |
help: Remove assignment to unused variable `labels_desc`

F541 [*] f-string without any placeholders
   --> scripts/validate_on_production.py:207:21
    |
205 |       # Construct prompt
206 |       labels_desc = ", ".join(labels)
207 |       system_prompt = f"""You are a text classification expert. Classify each message into one of these categories:
    |  _____________________^
208 | | - commissive: promises, commitments ("I'll do it", "sure, I can help")
209 | | - directive: requests, commands ("can you help?", "send me the file")
210 | | - inform: statements, facts ("I'm at the store", "the meeting is at 3pm")
211 | | - question: asking for information ("where are you?", "what time?")
212 | |
213 | | Respond with ONLY the category name, nothing else."""
    | |_____________________________________________________^
214 |
215 |       predictions = []
    |
help: Remove extraneous `f` prefix

E501 Line too long (113 > 100)
   --> scripts/validate_on_production.py:207:101
    |
205 |     # Construct prompt
206 |     labels_desc = ", ".join(labels)
207 |     system_prompt = f"""You are a text classification expert. Classify each message into one of these categories:
    |                                                                                                     ^^^^^^^^^^^^^
208 | - commissive: promises, commitments ("I'll do it", "sure, I can help")
209 | - directive: requests, commands ("can you help?", "send me the file")
    |

F541 [*] f-string without any placeholders
   --> scripts/validate_on_production.py:216:11
    |
215 |     predictions = []
216 |     print(f"ðŸ¤– Getting LLM labels (Qwen 235B)...")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
217 |     print(f"   Cost: ~${len(messages) * 0.0001:.4f}")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/validate_on_production.py:247:11
    |
245 |             predictions.append("inform")  # Default
246 |
247 |     print(f"âœ“ LLM labeling complete")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^
248 |     return predictions
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/validate_on_production.py:337:15
    |
335 |     if not model_path.exists():
336 |         print(f"âŒ Model not found: {model_path}")
337 |         print(f"   Run: uv run python scripts/train_final_lightgbm.py")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
338 |         sys.exit(1)
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/validate_on_production.py:358:11
    |
357 |     # Get LightGBM predictions
358 |     print(f"ðŸ”® Running LightGBM classifier...")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
359 |     lgbm_pred_raw = model.predict(features)
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> scripts/validate_on_production.py:367:11
    |
365 |         lgbm_preds = list(lgbm_pred_raw)
366 |
367 |     print(f"âœ“ LightGBM predictions complete")
    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
368 |     print()
    |
help: Remove extraneous `f` prefix

E501 Line too long (102 > 100)
   --> scripts/validate_on_production.py:403:101
    |
401 |         results = {
402 |             "n_samples": len(messages),
403 |             "agreement_rate": sum(1 for l, g in zip(llm_preds, lgbm_preds) if l == g) / len(messages),
    |                                                                                                     ^^
404 |             "messages": [
405 |                 {
    |

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/validate_on_real_messages.py:8:1
   |
 6 |   Reports agreement rate and disagreement patterns.
 7 |   """
 8 | / import json
 9 | | import os
10 | | import random
11 | | import re
12 | | import sqlite3
13 | | import time
14 | | from pathlib import Path
15 | |
16 | | import joblib
17 | | import numpy as np
18 | | import spacy
19 | | from groq import Groq
20 | | from tqdm import tqdm
21 | |
22 | | # Add project root to path
23 | | import sys
   | |__________^
24 |   sys.path.insert(0, str(Path(__file__).parent.parent))
   |
help: Organize imports

F401 [*] `random` imported but unused
  --> scripts/validate_on_real_messages.py:10:8
   |
 8 | import json
 9 | import os
10 | import random
   |        ^^^^^^
11 | import re
12 | import sqlite3
   |
help: Remove unused import: `random`

E501 Line too long (125 > 100)
  --> scripts/validate_on_real_messages.py:66:101
   |
64 |     # 2. you_modal
65 |     text_lower = text.lower()
66 |     you_modal = 1.0 if any(p in text_lower for p in ["can you", "could you", "would you", "will you", "should you"]) else 0.0
   |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^
67 |     features.append(you_modal)
   |

E501 Line too long (105 > 100)
  --> scripts/validate_on_real_messages.py:85:101
   |
84 |     # 6. i_will
85 |     i_will = 1.0 if any(p in text_lower for p in ["i'll", "i will", "i'm gonna", "ima", "imma"]) else 0.0
   |                                                                                                     ^^^^^
86 |     features.append(i_will)
   |

E501 Line too long (103 > 100)
  --> scripts/validate_on_real_messages.py:94:101
   |
93 |     # 8. first_person_count
94 |     first_person = sum(1 for token in doc if token.text.lower() in ("i", "me", "my", "mine", "myself"))
   |                                                                                                     ^^^
95 |     features.append(float(first_person))
   |

E501 Line too long (101 > 100)
   --> scripts/validate_on_real_messages.py:98:101
    |
 97 |     # 9. agreement
 98 |     agreement_words = {"sure", "okay", "ok", "yes", "yeah", "yep", "yup", "sounds good", "bet", "fs"}
    |                                                                                                     ^
 99 |     has_agreement = 1.0 if any(word in text_lower for word in agreement_words) else 0.0
100 |     features.append(has_agreement)
    |

E501 Line too long (105 > 100)
   --> scripts/validate_on_real_messages.py:111:101
    |
110 |     # 12. second_person_count
111 |     second_person = sum(1 for token in doc if token.text.lower() in ("you", "your", "yours", "yourself"))
    |                                                                                                     ^^^^^
112 |     features.append(float(second_person))
    |

E501 Line too long (112 > 100)
   --> scripts/validate_on_real_messages.py:119:101
    |
118 |     # 14. is_interrogative
119 |     is_question = 1.0 if "?" in text or any(token.tag_ in ("WDT", "WP", "WP$", "WRB") for token in doc) else 0.0
    |                                                                                                     ^^^^^^^^^^^^
120 |     features.append(is_question)
    |

E501 Line too long (138 > 100)
   --> scripts/validate_on_real_messages.py:164:101
    |
163 | â€¦
164 | â€¦fier for text messages. Classify each message into exactly ONE of these 6 categories:
    |                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
165 | â€¦
166 | â€¦
    |

E501 Line too long (185 > 100)
   --> scripts/validate_on_real_messages.py:168:101
    |
166 | â€¦
167 | â€¦e care)
168 | â€¦ks, sounds good, cool, yeah, sure, makes sense, np, understood, will do, right, yep, agreed, noted, perfect)
    |                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
169 | â€¦arification)
170 | â€¦ease", direct asks for someone to DO something)
    |

E501 Line too long (124 > 100)
   --> scripts/validate_on_real_messages.py:170:101
    |
168 | 2. **acknowledge**: Simple acknowledgments with no new info (ok, got it, thanks, sounds good, cool, yeah, sure, makes sense, np, undeâ€¦
169 | 3. **question**: Requests for information (uses ?, asks for details, seeks clarification)
170 | 4. **request**: Action requests or commands (imperative verbs, "can you", "please", direct asks for someone to DO something)
    |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^
171 | 5. **emotion**: Emotional reactions or greetings (hi, hey, congrats, haha, lol, love it, sorry, wow, omg, excited)
172 | 6. **statement**: Informational statements, updates, or observations (declarative sentences providing info, reporting status)
    |

E501 Line too long (114 > 100)
   --> scripts/validate_on_real_messages.py:171:101
    |
169 | 3. **question**: Requests for information (uses ?, asks for details, seeks clarification)
170 | 4. **request**: Action requests or commands (imperative verbs, "can you", "please", direct asks for someone to DO something)
171 | 5. **emotion**: Emotional reactions or greetings (hi, hey, congrats, haha, lol, love it, sorry, wow, omg, excited)
    |                                                                                                     ^^^^^^^^^^^^^^
172 | 6. **statement**: Informational statements, updates, or observations (declarative sentences providing info, reporting status)
    |

E501 Line too long (125 > 100)
   --> scripts/validate_on_real_messages.py:172:101
    |
170 | 4. **request**: Action requests or commands (imperative verbs, "can you", "please", direct asks for someone to DO something)
171 | 5. **emotion**: Emotional reactions or greetings (hi, hey, congrats, haha, lol, love it, sorry, wow, omg, excited)
172 | 6. **statement**: Informational statements, updates, or observations (declarative sentences providing info, reporting status)
    |                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^
173 |
174 | **Rules:**
    |

E501 Line too long (102 > 100)
   --> scripts/validate_on_real_messages.py:185:101
    |
185 | def load_real_messages(db_path: str, sample_size: int = 100, exclude_texts: set = None) -> list[dict]:
    |                                                                                                     ^^
186 |     """Load random real messages from chat.db that are NOT in training set."""
187 |     conn = sqlite3.connect(f"file:{db_path}?mode=ro", uri=True)
    |

N806 Variable `X` in function should be lowercase
   --> scripts/validate_on_real_messages.py:289:5
    |
288 |     # Combine features
289 |     X = np.hstack([bert_embeds, hand_features, spacy_features])
    |     ^
290 |
291 |     # Predict
    |

F401 [*] `collections.Counter` imported but unused
   --> scripts/validate_on_real_messages.py:388:29
    |
387 |     # Disagreements by category
388 |     from collections import Counter, defaultdict
    |                             ^^^^^^^
389 |
390 |     disagreements = defaultdict(list)
    |
help: Remove unused import: `collections.Counter`

I001 [*] Import block is un-sorted or un-formatted
  --> tests/integration/test_latency_gate.py:7:1
   |
 5 |   """
 6 |
 7 | / import pytest
 8 | | import time
 9 | | from models.generator import MLXGenerator
10 | | from jarvis.embedding_adapter import get_embedder
   | |_________________________________________________^
11 |
12 |   # Latency Budgets (ms)
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> tests/integration/test_latency_gate.py:21:1
   |
19 |     embedder = get_embedder()
20 |     text = "This is a sample sentence for latency benchmarking."
21 |     
   | ^^^^
22 |     # Warm up
23 |     embedder.encode(text)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/integration/test_latency_gate.py:24:1
   |
22 |     # Warm up
23 |     embedder.encode(text)
24 |     
   | ^^^^
25 |     start = time.perf_counter()
26 |     embedder.encode(text)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/integration/test_latency_gate.py:28:1
   |
26 |     embedder.encode(text)
27 |     latency = (time.perf_counter() - start) * 1000
28 |     
   | ^^^^
29 |     print(f"Embedding Latency: {latency:.2f}ms")
30 |     assert latency < EMBEDDING_LATENCY_THRESHOLD_MS, f"Embedding too slow: {latency:.2f}ms"
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tests/integration/test_latency_gate.py:35:5
   |
33 |   def test_generation_latency_gate():
34 |       """Gate: Generation (warm start) must be under budget."""
35 | /     from models.loader import MLXModelLoader
36 | |     from contracts.models import GenerationRequest
   | |__________________________________________________^
37 |       
38 |       loader = MLXModelLoader()
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> tests/integration/test_latency_gate.py:37:1
   |
35 |     from models.loader import MLXModelLoader
36 |     from contracts.models import GenerationRequest
37 |     
   | ^^^^
38 |     loader = MLXModelLoader()
39 |     if not loader.is_loaded():
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/integration/test_latency_gate.py:41:1
   |
39 |     if not loader.is_loaded():
40 |         loader.load()
41 |         
   | ^^^^^^^^
42 |     generator = MLXGenerator(loader=loader, skip_templates=True)
43 |     request = GenerationRequest(prompt="Hi", max_tokens=5)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/integration/test_latency_gate.py:44:1
   |
42 |     generator = MLXGenerator(loader=loader, skip_templates=True)
43 |     request = GenerationRequest(prompt="Hi", max_tokens=5)
44 |     
   | ^^^^
45 |     # Warm up the model/cache
46 |     generator.generate(request)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/integration/test_latency_gate.py:47:1
   |
45 |     # Warm up the model/cache
46 |     generator.generate(request)
47 |     
   | ^^^^
48 |     start = time.perf_counter()
49 |     generator.generate(request)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/integration/test_latency_gate.py:51:1
   |
49 |     generator.generate(request)
50 |     latency = (time.perf_counter() - start) * 1000
51 |     
   | ^^^^
52 |     print(f"Generation Latency (Warm): {latency:.2f}ms")
53 |     # This might be tight on some hardware, but it's a gate for a reason!
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tests/integration/test_model_pipeline.py:9:1
   |
 7 |   """
 8 |
 9 | / import pytest
10 | | from unittest.mock import MagicMock, patch
11 | | from jarvis.generation import can_use_llm
12 | | from contracts.models import GenerationRequest
13 | | from models.generator import MLXGenerator
14 | | from models.loader import MLXModelLoader, GenerationResult, ModelConfig
   | |_______________________________________________________________________^
15 |
16 |   # =============================================================================
   |
help: Organize imports

F401 [*] `unittest.mock.patch` imported but unused
  --> tests/integration/test_model_pipeline.py:10:38
   |
 9 | import pytest
10 | from unittest.mock import MagicMock, patch
   |                                      ^^^^^
11 | from jarvis.generation import can_use_llm
12 | from contracts.models import GenerationRequest
   |
help: Remove unused import: `unittest.mock.patch`

F401 [*] `jarvis.generation.can_use_llm` imported but unused
  --> tests/integration/test_model_pipeline.py:11:31
   |
 9 | import pytest
10 | from unittest.mock import MagicMock, patch
11 | from jarvis.generation import can_use_llm
   |                               ^^^^^^^^^^^
12 | from contracts.models import GenerationRequest
13 | from models.generator import MLXGenerator
   |
help: Remove unused import: `jarvis.generation.can_use_llm`

F401 [*] `models.loader.ModelConfig` imported but unused
  --> tests/integration/test_model_pipeline.py:14:61
   |
12 | from contracts.models import GenerationRequest
13 | from models.generator import MLXGenerator
14 | from models.loader import MLXModelLoader, GenerationResult, ModelConfig
   |                                                             ^^^^^^^^^^^
15 |
16 | # =============================================================================
   |
help: Remove unused import: `models.loader.ModelConfig`

W293 [*] Blank line contains whitespace
  --> tests/integration/test_model_pipeline.py:35:1
   |
33 |     # Initialize generator with mock loader
34 |     generator = MLXGenerator(loader=mock_loader, skip_templates=True)
35 |     
   | ^^^^
36 |     # Create request
37 |     request = GenerationRequest(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/integration/test_model_pipeline.py:43:1
   |
41 |         max_tokens=50
42 |     )
43 |     
   | ^^^^
44 |     # Execute
45 |     response = generator.generate(request)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/integration/test_model_pipeline.py:46:1
   |
44 |     # Execute
45 |     response = generator.generate(request)
46 |     
   | ^^^^
47 |     # Verify
48 |     assert response.text == "Mocked response"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/integration/test_model_pipeline.py:50:1
   |
48 |     assert response.text == "Mocked response"
49 |     assert response.finish_reason == "stop"
50 |     
   | ^^^^
51 |     # Verify loader call
52 |     mock_loader.generate_sync.assert_called_once()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/integration/test_model_pipeline.py:54:1
   |
52 |     mock_loader.generate_sync.assert_called_once()
53 |     call_kwargs = mock_loader.generate_sync.call_args.kwargs
54 |     
   | ^^^^
55 |     # Check that the prompt passed to generate_sync contains our input
56 |     # (The actual prompt will be formatted by PromptBuilder, so we check for containment)
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> tests/integration/test_model_pipeline.py:67:1
   |
65 | def test_real_model_loading_and_generation():
66 |     """Smoke test for the actual MLX model.
67 |     
   | ^^^^
68 |     This test attempts to:
69 |     1. Load the model (verifies path and memory)
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> tests/integration/test_model_pipeline.py:71:1
   |
69 |     1. Load the model (verifies path and memory)
70 |     2. Run a simple generation (verifies tokenizer and tensor operations)
71 |     
   | ^^^^
72 |     Run with: pytest -v -m real_model
73 |     """
   |
help: Remove whitespace from blank line

F401 `mlx.core` imported but unused; consider using `importlib.util.find_spec` to test for availability
  --> tests/integration/test_model_pipeline.py:75:28
   |
73 |     """
74 |     try:
75 |         import mlx.core as mx
   |                            ^^
76 |     except ImportError:
77 |         pytest.skip("MLX not installed")
   |
help: Remove unused import: `mlx.core`

W293 [*] Blank line contains whitespace
  --> tests/integration/test_model_pipeline.py:83:1
   |
81 |     # If not, this test will fail, which is intended for the smoke test.
82 |     loader = MLXModelLoader()
83 |     
   | ^^^^
84 |     try:
85 |         # Load
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/integration/test_model_pipeline.py:89:1
   |
87 |         loader.load()
88 |         assert loader.is_loaded()
89 |         
   | ^^^^^^^^
90 |         # Generate
91 |         prompt = "Say hello."
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/integration/test_model_pipeline.py:98:1
    |
 96 |             temperature=0.1
 97 |         )
 98 |         
    | ^^^^^^^^
 99 |         # print("Generated: " + result.text)
100 |         assert result.text
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/integration/test_model_pipeline.py:102:1
    |
100 |         assert result.text
101 |         assert len(result.text) > 0
102 |         
    | ^^^^^^^^
103 |     except Exception as e:
104 |         pytest.fail("Real model test failed: " + str(e))
    |
help: Remove whitespace from blank line

W292 [*] No newline at end of file
   --> tests/integration/test_model_pipeline.py:106:24
    |
104 |         pytest.fail("Real model test failed: " + str(e))
105 |     finally:
106 |         loader.unload()
    |                        ^
    |
help: Add trailing newline

I001 [*] Import block is un-sorted or un-formatted
  --> tests/unit/test_behavioral_mobilization.py:9:1
   |
 7 |   """
 8 |
 9 | / import pytest
10 | | from jarvis.classifiers.response_mobilization import classify_response_pressure, ResponsePressure
   | |_________________________________________________________________________________________________^
11 |
12 |   class TestMobilizationBehavior:
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> tests/unit/test_behavioral_mobilization.py:24:1
   |
22 |         text1 = f"Hey {name1}, are you free for lunch?"
23 |         text2 = f"Hey {name2}, are you free for lunch?"
24 |         
   | ^^^^^^^^
25 |         res1 = classify_response_pressure(text1)
26 |         res2 = classify_response_pressure(text2)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/unit/test_behavioral_mobilization.py:27:1
   |
25 |         res1 = classify_response_pressure(text1)
26 |         res2 = classify_response_pressure(text2)
27 |         
   | ^^^^^^^^
28 |         assert res1.pressure == res2.pressure
29 |         assert abs(res1.confidence - res2.confidence) < 0.01
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/unit/test_behavioral_mobilization.py:35:1
   |
33 |         base_text = "Can you review that document?"
34 |         urgent_text = "Can you review that document ASAP? It's urgent."
35 |         
   | ^^^^^^^^
36 |         base_res = classify_response_pressure(base_text)
37 |         urgent_res = classify_response_pressure(urgent_text)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/unit/test_behavioral_mobilization.py:38:1
   |
36 |         base_res = classify_response_pressure(base_text)
37 |         urgent_res = classify_response_pressure(urgent_text)
38 |         
   | ^^^^^^^^
39 |         # Pressure should be equal or higher (HIGH >= LOW)
40 |         # Note: We assume HIGH > LOW in the enum ordering or logic
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/unit/test_behavioral_mobilization.py:43:1
   |
41 |         if base_res.pressure == ResponsePressure.LOW:
42 |             assert urgent_res.pressure in [ResponsePressure.HIGH, ResponsePressure.LOW]
43 |         
   | ^^^^^^^^
44 |         # If both are same category, confidence in "HIGH" or similar should stay high
45 |         # This depends on the internal implementation of the classifier
   |
help: Remove whitespace from blank line

E501 Line too long (107 > 100)
   --> tests/unit/test_category_classifier.py:147:101
    |
145 |         assert result is not None
146 |         # Should be one of valid categories
147 |         assert result.category in {"closing", "acknowledge", "question", "request", "emotion", "statement"}
    |                                                                                                     ^^^^^^^
    |

I001 [*] Import block is un-sorted or un-formatted
  --> tests/unit/test_prompt_snapshots.py:7:1
   |
 5 |   """
 6 |
 7 | / from jarvis.prompts import build_reply_prompt, build_threaded_reply_prompt
 8 | | from jarvis.threading import ThreadContext, ThreadTopic, ThreadState, UserRole
 9 | | from contracts.imessage import Message
   | |______________________________________^
10 |
11 |   def test_reply_prompt_snapshot(snapshot):
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> tests/unit/test_prompt_snapshots.py:15:1
   |
13 |     context = "[10:00] John: Hello\\n[10:01] Me: Hi"
14 |     last_message = "How are you?"
15 |     
   | ^^^^
16 |     prompt = build_reply_prompt(
17 |         context=context,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/unit/test_prompt_snapshots.py:22:1
   |
20 |         tone="casual"
21 |     )
22 |     
   | ^^^^
23 |     assert prompt == snapshot
   |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
  --> tests/unit/test_prompt_snapshots.py:44:30
   |
42 |         user_role=UserRole.RESPONDER,
43 |         confidence=1.0,
44 |         relevant_messages=[], 
   |                              ^
45 |         action_items=[],
46 |         participants_count=2
   |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
  --> tests/unit/test_prompt_snapshots.py:48:1
   |
46 |         participants_count=2
47 |     )
48 |     
   | ^^^^
49 |     # Mock config
50 |     class MockConfig:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/unit/test_prompt_snapshots.py:55:1
   |
53 |         include_action_items = False
54 |         suggest_follow_up = False
55 |     
   | ^^^^
56 |     prompt = build_threaded_reply_prompt(
57 |         thread_context=thread_context,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/unit/test_prompt_snapshots.py:62:1
   |
60 |         tone="casual"
61 |     )
62 |     
   | ^^^^
63 |     assert prompt == snapshot
   |
help: Remove whitespace from blank line

W292 [*] No newline at end of file
  --> tests/unit/test_prompt_snapshots.py:63:30
   |
61 |     )
62 |     
63 |     assert prompt == snapshot
   |                              ^
   |
help: Add trailing newline

I001 [*] Import block is un-sorted or un-formatted
 --> tests/unit/test_text_normalizer_pbt.py:3:1
  |
1 |   """Property-based tests for text normalizer."""
2 |
3 | / import pytest
4 | | from hypothesis import given, strategies as st
5 | | from jarvis.text_normalizer import normalize_text, is_reaction
  | |______________________________________________________________^
6 |
7 |   @given(st.text())
  |
help: Organize imports

Found 475 errors.
[*] 182 fixable with the `--fix` option (9 hidden fixes can be enabled with the `--unsafe-fixes` option).
make: *** [lint] Error 1
