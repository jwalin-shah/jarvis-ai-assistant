# LLM Category Labeling: Prompt Refinement Results

## Final Prompt Formula (WORKING!)

### Ultra-Explicit Checklist Format

```
For each message, check the rules IN ORDER and pick the FIRST match:

RULE 1: closing
- Check: Does it say "bye", "ttyl", "see you", "gotta go", or "talk soon"?
- If YES â†’ category = closing

RULE 2: acknowledge (ULTRA-CONSERVATIVE)
- Check: Is it EXACTLY one of these words (ignore punctuation):
  "ok", "okay", "yeah", "yep", "yup", "sure", "thanks", "thank you",
  "gotcha", "fine", "alright", "cool", "k", "kk", or just emoji like "ðŸ‘"?
- If YES â†’ category = acknowledge

RULE 3: request
- Check: Does it contain "can you", "could you", "would you", "please",
  OR "I suggest", OR "let's"?
- If YES â†’ category = request

RULE 4: question
- Check: Does it contain "?" OR start with "what", "when", "where", "who",
  "why", "how", "is", "are", "do", "does"?
- If YES â†’ category = question

RULE 5: emotion
- Check: Does it contain "happy", "sad", "angry", "stressed", "excited",
  "frustrated", "love", "hate", "amazing", "terrible" OR "!!"
  (2+ exclamation marks) OR ALLCAPS words?
- If YES â†’ category = emotion

RULE 6: statement
- If none of the above match â†’ category = statement
```

## Configuration

- **Model**: `zai-glm-4.7` (less verbose than gpt-oss-120b)
- **Max tokens**: 1500 per batch (allows verbose reasoning)
- **Batch size**: 10 messages per API call
- **Parsing**: Look for both `**Result:` and `- Category:` patterns
- **Temperature**: 0.0 (deterministic)

## Test Results

### Validation on Real Data (20 examples)

| Dataset | Accuracy | Notes |
|---------|----------|-------|
| **DailyDialog** | **100%** (10/10) | Formal conversations, perfect classification |
| **SAMSum** | **100%** (10/10) | Casual chat, fixed with ultra-conservative Rule 2 |

### Key Fix: Rule 2 (acknowledge)

**Problem**: `"Liberals as always."` (3 words) was classified as acknowledge

**Old Rule**: â‰¤3 words AND no "?" mark
- Too broad - catches short statements

**New Rule**: Explicit word list only
- Ultra-conservative - high precision
- Only matches known acknowledgment words
- Fixed the false positive

## Evolution

1. **Original 5-category schema** â†’ Overlapping boundaries (20% accuracy)
2. **6-category redesign** â†’ Clear hierarchy (60% accuracy)
3. **Checklist format** â†’ Explicit rules (80% accuracy on simple tests)
4. **Ultra-conservative Rule 2** â†’ **100% on both datasets**

## Next Steps

1. âœ… Achieved 100% on test cases (5+10+10 = 25 real examples)
2. Update production scripts:
   - `scripts/validate_llm_categories.py`
   - `scripts/llm_category_labeler.py`
   - `scripts/batch_review_llm.py`
3. Run pilot validation (200 examples)
4. Proceed with full labeling (15k examples)
5. Retrain classifier and measure F1 improvement

## Lessons Learned

1. **Explicit > Implicit**: Checklist format beats narrative descriptions
2. **Conservative for auto**: For minimal-response categories, be ultra-conservative
3. **Test on casual data**: Formal (DailyDialog) != Casual (SAMSum)
4. **Token budget matters**: Reasoning models need 1500+ tokens for 10 messages
5. **Parsing flexibility**: Match both `**Result:` and `- Category:` patterns
