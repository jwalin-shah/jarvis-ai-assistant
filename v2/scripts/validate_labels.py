#!/usr/bin/env python3
"""Validate LLM-generated labels against human judgment.

The relationship/intent labels were generated by lfm2.5-1.2b.
We need to verify they're accurate before trusting them for eval.

This script shows you samples and asks you to verify/correct the labels.

Usage:
    python scripts/validate_labels.py                # Interactive validation
    python scripts/validate_labels.py --results      # Show validation results
    python scripts/validate_labels.py --quick 20     # Quick 20-sample check
"""

import argparse
import json
import random
import sys
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent.parent))

LABELED_DATA = Path("results/test_set/model_results.jsonl")
VALIDATION_FILE = Path("results/validation/human_labels.json")


def load_labeled_data():
    """Load the LLM-labeled data."""
    if not LABELED_DATA.exists():
        print(f"Error: {LABELED_DATA} not found")
        sys.exit(1)

    samples = []
    with open(LABELED_DATA) as f:
        for line in f:
            samples.append(json.loads(line))
    return samples


def load_validation():
    """Load existing human validation."""
    if VALIDATION_FILE.exists():
        with open(VALIDATION_FILE) as f:
            return json.load(f)
    return {"validated": [], "stats": {}}


def save_validation(data):
    """Save validation results."""
    VALIDATION_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(VALIDATION_FILE, "w") as f:
        json.dump(data, f, indent=2)


def run_validation(n_samples: int = 30, quick: bool = False):
    """Interactive validation of LLM labels."""
    print("\n" + "=" * 70)
    print("LABEL VALIDATION")
    print("=" * 70)
    print("""
The relationship/intent labels were generated by an LLM (lfm2.5-1.2b).
We need to verify they're accurate before using them for evaluation.

For each sample, you'll see:
- The conversation context
- The LLM's guessed relationship and intent
- Your actual response

Then you'll say if the LLM labels are correct.
""")

    samples = load_labeled_data()
    validation = load_validation()

    # Skip already validated
    validated_ids = {v["id"] for v in validation["validated"]}
    unvalidated = [s for s in samples if s["id"] not in validated_ids]

    if not unvalidated:
        print("All samples already validated!")
        show_results()
        return

    random.shuffle(unvalidated)
    to_validate = unvalidated[:n_samples]

    print(f"\nValidating {len(to_validate)} samples...")
    print("Commands: y=correct, n=wrong, r=fix relationship, i=fix intent, s=skip, q=quit\n")

    relationships = ["family", "close_friend", "casual_friend", "work", "romantic", "acquaintance"]
    intents = ["statement", "question", "greeting", "thanks", "farewell", "logistics", "sharing", "reaction"]

    for idx, sample in enumerate(to_validate):
        print("\n" + "=" * 70)
        print(f"[{idx + 1}/{len(to_validate)}] Contact: {sample.get('contact', 'unknown')}")
        print("=" * 70)

        # Show conversation context (from prompt)
        prompt = sample.get("prompt", "")
        # Extract just the conversation part
        if "them:" in prompt or "me:" in prompt:
            lines = prompt.split("\n")
            conv_lines = [l for l in lines if l.strip().startswith(("them:", "me:"))]
            print("\nConversation:")
            for line in conv_lines[-8:]:  # Last 8 messages
                print(f"  {line}")

        print(f"\n  → YOUR ACTUAL RESPONSE: \"{sample.get('gold_response', '')}\"")

        print(f"\n  LLM Labels:")
        print(f"    Relationship: {sample.get('relationship', 'unknown')}")
        print(f"    Intent: {sample.get('intent', 'unknown')}")

        while True:
            choice = input("\n  Correct? (y/n/r=fix rel/i=fix intent/s=skip/q=quit): ").strip().lower()

            if choice == 'q':
                save_validation(validation)
                print(f"\nSaved {len(validation['validated'])} validations.")
                show_results()
                return

            if choice == 's':
                break

            if choice == 'y':
                validation["validated"].append({
                    "id": sample["id"],
                    "llm_relationship": sample.get("relationship"),
                    "llm_intent": sample.get("intent"),
                    "human_relationship": sample.get("relationship"),
                    "human_intent": sample.get("intent"),
                    "relationship_correct": True,
                    "intent_correct": True,
                    "timestamp": datetime.now().isoformat(),
                })
                break

            if choice == 'n':
                # Both wrong - ask for corrections
                print(f"\n  Available relationships: {', '.join(relationships)}")
                new_rel = input(f"  Correct relationship [{sample.get('relationship')}]: ").strip()
                if not new_rel:
                    new_rel = sample.get("relationship")

                print(f"\n  Available intents: {', '.join(intents)}")
                new_intent = input(f"  Correct intent [{sample.get('intent')}]: ").strip()
                if not new_intent:
                    new_intent = sample.get("intent")

                validation["validated"].append({
                    "id": sample["id"],
                    "llm_relationship": sample.get("relationship"),
                    "llm_intent": sample.get("intent"),
                    "human_relationship": new_rel,
                    "human_intent": new_intent,
                    "relationship_correct": new_rel == sample.get("relationship"),
                    "intent_correct": new_intent == sample.get("intent"),
                    "timestamp": datetime.now().isoformat(),
                })
                break

            if choice == 'r':
                print(f"\n  Available: {', '.join(relationships)}")
                new_rel = input(f"  Correct relationship: ").strip()
                if new_rel:
                    validation["validated"].append({
                        "id": sample["id"],
                        "llm_relationship": sample.get("relationship"),
                        "llm_intent": sample.get("intent"),
                        "human_relationship": new_rel,
                        "human_intent": sample.get("intent"),
                        "relationship_correct": False,
                        "intent_correct": True,
                        "timestamp": datetime.now().isoformat(),
                    })
                    break

            if choice == 'i':
                print(f"\n  Available: {', '.join(intents)}")
                new_intent = input(f"  Correct intent: ").strip()
                if new_intent:
                    validation["validated"].append({
                        "id": sample["id"],
                        "llm_relationship": sample.get("relationship"),
                        "llm_intent": sample.get("intent"),
                        "human_relationship": sample.get("relationship"),
                        "human_intent": new_intent,
                        "relationship_correct": True,
                        "intent_correct": False,
                        "timestamp": datetime.now().isoformat(),
                    })
                    break

            print("  Invalid choice. Try again.")

    save_validation(validation)
    print(f"\n✓ Saved {len(validation['validated'])} validations to {VALIDATION_FILE}")
    show_results()


def show_results():
    """Show validation results."""
    validation = load_validation()

    if not validation["validated"]:
        print("\nNo validations yet. Run: python scripts/validate_labels.py")
        return

    validated = validation["validated"]
    n = len(validated)

    rel_correct = sum(1 for v in validated if v.get("relationship_correct", False))
    intent_correct = sum(1 for v in validated if v.get("intent_correct", False))

    print("\n" + "=" * 70)
    print("LABEL VALIDATION RESULTS")
    print("=" * 70)
    print(f"\nTotal validated: {n}")
    print(f"\nLLM Label Accuracy:")
    print(f"  Relationship: {rel_correct}/{n} ({rel_correct/n*100:.0f}%)")
    print(f"  Intent:       {intent_correct}/{n} ({intent_correct/n*100:.0f}%)")

    # Show corrections
    corrections = [v for v in validated if not v.get("relationship_correct") or not v.get("intent_correct")]
    if corrections:
        print(f"\n--- CORRECTIONS MADE ({len(corrections)}) ---")
        for c in corrections[:10]:
            if not c.get("relationship_correct"):
                print(f"  Relationship: {c['llm_relationship']} → {c['human_relationship']}")
            if not c.get("intent_correct"):
                print(f"  Intent: {c['llm_intent']} → {c['human_intent']}")

    # Recommendation
    print("\n" + "=" * 70)
    print("RECOMMENDATION")
    print("=" * 70)

    if n < 30:
        print(f"\n⚠️  Need more validations. Run: python scripts/validate_labels.py")
        print(f"   Current: {n}/30 minimum recommended")
    elif rel_correct / n >= 0.8 and intent_correct / n >= 0.8:
        print(f"\n✓ LLM labels are reliable (>80% accurate)")
        print(f"  Can use LLM labels for evaluation")
    elif rel_correct / n >= 0.6:
        print(f"\n⚠️  LLM labels are somewhat reliable ({rel_correct/n*100:.0f}%)")
        print(f"  Consider: Use LLM labels but with caution")
        print(f"  Or: Manually label more samples for higher confidence")
    else:
        print(f"\n❌ LLM labels are unreliable (<60% accurate)")
        print(f"  Need to: Manually label the full dataset")
        print(f"  Or: Use a better LLM for labeling (GPT-4/Claude)")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--results", action="store_true", help="Show validation results")
    parser.add_argument("--quick", type=int, metavar="N", help="Quick N-sample validation")
    args = parser.parse_args()

    if args.results:
        show_results()
    elif args.quick:
        run_validation(n_samples=args.quick, quick=True)
    else:
        run_validation()


if __name__ == "__main__":
    main()
