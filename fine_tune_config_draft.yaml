# mlx-lm LoRA fine-tuning config for LFM 2.5 0.3B draft model
# Used as speculative decoding draft model
# Optimized for 8GB RAM Apple Silicon
#
# Usage:
#   mlx_lm.lora --config fine_tune_config_draft.yaml
#   make finetune-draft

model: "mlx-community/LFM2-350M-4bit"
data: "data/soc_sft"
train: true
adapter_path: "adapters/lfm-0.3b-soc-sft"

# QLoRA settings (smaller model = less capacity needed)
lora_layers: 8
lora_rank: 4
lora_alpha: 8
lora_dropout: 0.05

# Training hyperparameters (smaller model can use larger batch)
batch_size: 4
grad_checkpoint: true
learning_rate: 2.0e-4
iters: 2000
steps_per_eval: 100
save_every: 500
max_seq_length: 512
