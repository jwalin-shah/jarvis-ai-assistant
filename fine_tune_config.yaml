# mlx-lm LoRA fine-tuning config for LFM 2.5 1.2B
# Optimized for 8GB RAM Apple Silicon
#
# Usage:
#   mlx_lm.lora --config fine_tune_config.yaml
#   make finetune-sft

model: "LiquidAI/LFM2.5-1.2B-Instruct-MLX-4bit"
data: "data/soc_sft"
train: true
adapter_path: "adapters/lfm-1.2b-soc-sft"

# QLoRA settings (8GB RAM constraint)
lora_layers: 8
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05

# Training hyperparameters
batch_size: 2
grad_checkpoint: true
learning_rate: 2.0e-4
iters: 2000
steps_per_eval: 100
save_every: 500
max_seq_length: 512
